{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VR-aozjuNBkn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import optim, nn\n",
    "from modules.dataset import *\n",
    "from modules.network import *\n",
    "from modules.embeddings import *\n",
    "from torch import save, device, cuda\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Y86jEOjaNBku",
    "outputId": "52f708ff-1c3a-406b-bdab-1cd5b31788d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = device(\"cuda\") if cuda.is_available() else device(\"cpu\")\n",
    "print('Selected device:', device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Colab packages\n",
    "from google.colab import files, drive\n",
    "\n",
    "# Connect drive to colab\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories (Colab)\n",
    "!mkdir ./drive/My\\ Drive/Colab\\ Notebooks/images\n",
    "!mkdir ./drive/My\\ Drive/Colab\\ Notebooks/results\n",
    "!mkdir ./drive/My\\ Drive/Colab\\ Notebooks/results/losses\n",
    "\n",
    "# Set paths\n",
    "images_path = \"./drive/My Drive/Colab Notebooks/images\"\n",
    "res_path = \"./drive/My Drive/Colab Notebooks/results/losses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local paths\n",
    "images_path = \"./images\"\n",
    "res_path = \"./data/losses\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBpduFYANBkz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%Loading the dataset \n",
    "\n",
    "min_len = 10\n",
    "dataset = Mobydick('./data/mobydick.txt', min_len = min_len)\n",
    "\n",
    "with open('./data/clean_text', 'wb') as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['some', 'years', 'ago', 'never', 'mind', 'how', 'long', 'precisely', 'having', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', ',', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', ',', 'i', 'thought', 'i', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "# Given a min_len>2, it is not necessary to delete the chapters titles\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sentence length: 30.446351931330472\n",
      "Max sentence length: 127\n",
      "Min sentence length: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAGoCAYAAADVZM+hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzde5xkZXng8d8zMwyCchmGMVGGmYGAboSYrDPBcXWN94CgGJWIEkNUgiaam8kmEFdkiZvArlmziRMTghdCuAZjnAhGTUSzXgaZRhTQYCYTGoZB5dIggjLT08/+UaexKKq6T3VX1TlV9ft+Pv3prlPn8rznUqfefp/3PZGZSJIkSZKkai2pOgBJkiRJkmQFXZIkSZKkWrCCLkmSJElSDVhBlyRJkiSpBqygS5IkSZJUA1bQJUmSJEmqASvokgCIiFsj4kUVbHddRGRELBvAtjIijuj3diRJ6qVhvUdHxC9FxOebXn8vIg7vUWy/HxEX9CLONuteU8S6tBfrk7phBV3qICKeExFfjIj7I+LeiPhCRPx0D9b7qJvVuKnqS4YkaXR4j+6Pft+jM/MJmbl9nhieFxE7SqzrDzPztF7E1VruzLytiHVPL9YvdaPvLVbSMIqI/YGPA78CXAEsB/4r8HCVcUmSNO68RysilmXmdNVxSP1gC7rU3lMAMvPSzNyTmd/PzE9l5tdmZ4iIN0bENyJiKiI+GRFrm97LiHhLRPxb8f6maPhx4C+AZxWpU/cV8+8dEe+JiNsi4tsR8RcRsU/x3vMiYkdE/HZEfCci7oyINzRta5+I+OOImCxaEj7ftOzGooXhvoj4akQ8r0zhI2JJRJwREf8eEfdExBURcVDx3mwa2alFvHdHxDta4rmwKPc3IuJ3Z/8THhEXAWuAfyjK/7tNmz2l3fpa4toYEd9qTjmLiJ+LiK8Vfx8TEV8qyntnRLwvIpZ3WNdnI+K0ptetaXj/KSI+XbTM3BIRP9/03ksj4usR8UBE3BERv1Nmv0qSesJ7dA3v0cU6VkbE5oj4bkR8Gfixlvcf6WrW7l4aEY8HPgE8uYjhexHx5Ig4OyKujIi/iYjvAr9UTPublhDeGBE7i+Pw203b/XBEvLvp9SOt9O3KHS0p80UMm4vvBNsi4peb1nV2cQz+uijLzRGxYf4jKbVnBV1q75vAnuImdlxErGh+MyJeAfw+8EpgFfD/gEtb1nEC8NPATwI/D/xsZn4DeAvwpSJ16sBi3vNofOH4KeAI4BDgrKZ1/ShwQDH9TcCmppjeA6wH/gtwEPC7wExEHAJcBby7mP47wEciYlWJ8v868ArgZ4AnA1PAppZ5ngM8FXghcFbxxQbgXcA64HDgxcAvzC6Qma8HbgNeVpT/f5VYH03LbwEeBF7QNPl1wCXF33uA3wIOBp5VrOtXS5T3UYovCJ8u1vtE4LXAn0fEUcUsHwDenJn7AUcDn+l2G5KkBfMeXcN7dGET8APgScAbi59OHnMvzcwHgeOAnUUMT8jMncX8JwJXAgcCF3dY5/OBI4GXAGdEiXT9eco961JgB439/WrgDyPihU3vvxy4rIhtM/C++bYrdWIFXWojM79L42aUwF8BdxX/Of2RYpY3A3+Umd8oUqz+EPipaPoPPXBuZt6XmbcB19C4sT9GRATwy8BvZea9mflAsb6Tm2bbDZyTmbsz82rge8BTI2IJjZvfb2TmHUVLwhcz82EaN92rM/PqzJzJzE8DW4GXltgFbwbekZk7inWdDbw6Hj34yv8oWi2+CnyVxpccaHzR+cPMnMrMHcCfltjeXOtrdSmNCjMRsV9RnksBMnMiM7dk5nRm3gr8JY0vMN06Abg1Mz9UrOt64CM0bsrQOB5Pi4j9i3Jev4BtSJIWwHt0Pe/R0chuexVwVmY+mJk3ARfOsc5u76Vfysy/L/bX9+eI88HMvBH4EMX3hcWIiENpnG+/l5k/yMwbgAuA1zfN9vniWO4BLqLzdxhpXlbQpQ6KG/svZeZqGv/ZfTLwJ8Xba4H/W6Sl3QfcCwSN/57P+lbT3w8BT+iwqVXAvsBE0/r+sZg+656Wvlaz6zsYeBzw723WuxY4aXadxXqfQ+O/2vNZC3y0ablv0Gid/pGmeTqV78nA7U3vNf89l7L76xLglRGxN43WkeszcxIgIp4SER+PRhr8d2l8iTq45PabrQWe2bLvTqHRSgKNLyAvBSYj4nMR8awFbEOStEDeo2t5j15FY3yr5nVOzrHObu+lZWJt3faTSywznycDs/+caV73XOfT42IAT6fRaLKCLpWQmf8KfJjGlwBo3ADenJkHNv3sk5lfLLO6ltd3A98Hjmpa1wGZ2enLQuuyP6Clj1dTjBe1xPj4zDy3xHpvB45rWfZxmXlHiWXvBFY3vT605f3W8nclM79O48Z4HI9Obwd4P/CvwJGZuT+NFMfosKoHaXzpmvWjTX/fDnyupfxPyMxfKWK4LjNPpJH+/vc0BimSJFXAe3Rt7tF3AdMt61zTaeY57qWdYigTW+u2Z9Pj57rnz7funcBBRdZe87rL7G+pa1bQpTaiMUDYb0fE6uL1oTTSpLYUs/wFcOZsn+SIOCAiTiq5+m8Dq6MYvCwzZ2ik6L03Ip5YrO+QiPjZ+VZULPtB4P8UA5gsjYhnFa3LfwO8LCJ+tpj+uGJQlNVzr/WR8v3P2XTAiFgVESeWLN8VNPbNiqKP3dta3v82jb5vi3EJjT54zwX+tmn6fsB3ge9FxH+iMcJvJzfQaInfNxoD1ryp6b2PA0+JiNdHxF7Fz09HxI9HxPKIOCUiDsjM3cX2fAyLJA2I9+h63qOL9O6/A84u7q1PA05tN+8899JvAysj4oAFhPHOYttHAW8ALi+m3wC8NCIOiogfBX6zZbmO5c7M24EvAn9UHKen0/jO0KkfvLQoVtCl9h4AnglcGxEP0rjp3wT8NkBmfpTGoDGXFanUN9Fo0S3jM8DNwLci4u5i2u8B24Atxfr+icZgLGX8DnAjcB2NNL7zgCXFDeVEGq3Id9H4j/t/o9x1/39pDHLyqYh4gEb5n1kynnNoDKTyH0U5ruTRj775I+C/F6l5Cx39/FLgeTQGlLm7afrv0GhVf4DGF6rLH7voI94L7KJxU76Qphttkcb2Ehp9DHfSSF07D9i7mOX1wK3FsXoLTYPsSJL6znt0fe/Rb6OR/v4tGlkNH5pj3rb30iIj4lJgexFHN2nqn6NxrP4ZeE9mfqqYfhGNvvO3Ap/isd8P5iv3a2kMrrcT+CjwrmLcAKnnInNR2aaSNKeI+BXg5MxcyGBtkiSpT7xHS/VjC7qknoqIJ0XEs6PxnNan0mjR+GjVcUmSNO68R0v15+iCknptOY3Hmx0G3EfjuaB/XmlEkiQJvEdLtWeKuyRJkiRJNWCKuyRJkiRJNTDUKe4HH3xwrlu3ruowJEnqm4mJibszc1XVcQyS93dJ0qjrdH8f6gr6unXr2Lp1a9VhSJLUNxExWXUMg+b9XZI06jrd301xlyRJAETEsRFxS0Rsi4gz2ry/d0RcXrx/bUSsK6YfExE3FD9fjYifa1rm1oi4sXjPWrckSXMY6hZ0SZLUGxGxFNgEvBjYAVwXEZsz8+tNs70JmMrMIyLiZOA84DXATcCGzJyOiCcBX42If8jM6WK552fm3YMrjSRJw8kWdEmSBHAMsC0zt2fmLhqPXzqxZZ4TgQuLv68EXhgRkZkPNVXGHwf4iBhJkhbACrokSQI4BLi96fWOYlrbeYoK+f3ASoCIeGZE3AzcCLylqcKewKciYiIiTu+08Yg4PSK2RsTWu+66qycFkiRp2FhBlyRJANFmWmtLeMd5MvPazDwK+GngzIh4XPH+szPzGcBxwFsj4rntNp6Z52fmhszcsGrVWA1aL0nSI6ygS5IkaLSYH9r0ejWws9M8EbEMOAC4t3mGzPwG8CBwdPF6Z/H7O8BHaaTSS5KkNqygS5IkgOuAIyPisIhYDpwMbG6ZZzNwavH3q4HPZGYWyywDiIi1wFOBWyPi8RGxXzH98cBLaAwoJ0mS2nAUd0mSRDEC+9uATwJLgQ9m5s0RcQ6wNTM3Ax8ALoqIbTRazk8uFn8OcEZE7AZmgF/NzLsj4nDgoxEBje8cl2TmPw62ZJIkDQ8r6JIkCYDMvBq4umXaWU1//wA4qc1yFwEXtZm+HfjJ3kcqSdJoMsVdkiRJkqQasIIuSZIkSVINWEGXJEmSJKkGrKBLkiRJklQDVtDH0MTkFJuu2cbE5FTVoUiSJEmSCo7iPmYmJqc45YIt7JqeYfmyJVx82kbWr11RdViSJEmSNPZsQR8zW7bfw67pGWYSdk/PsGX7PVWHJEmSJEnCFvSxs/HwlSxftoTd0zPstWwJGw9fWXVIkiSNpHVnXFVqvlvPPb7PkUiShoUV9DGzfu0KLj5tI1u238PGw1ea3i5JkiRJNWEFfQytX7vCirkkSZIk1Yx90CVJkiRJqgEr6JIkSZIk1YAVdEmSJEmSasAKuiRJkiRJNWAFXZIkSZKkGrCCLkmSJElSDVhBlyRJkiSpBqygS5IkSZJUA1bQJUmSJEmqASvokiRJkiTVgBV0SZIkSZJqwAq6JEmSJEk10LcKekR8MCK+ExE3NU373xHxrxHxtYj4aEQc2PTemRGxLSJuiYif7VdckiRJkiTVUT9b0D8MHNsy7dPA0Zn5dOCbwJkAEfE04GTgqGKZP4+IpX2MTZIkSZKkWulbBT0z/wW4t2XapzJzuni5BVhd/H0icFlmPpyZ/wFsA47pV2ySJEmSJNVNlX3Q3wh8ovj7EOD2pvd2FNMkSZIkSRoLlVTQI+IdwDRw8eykNrNlh2VPj4itEbH1rrvu6leIkiRJkiQN1MAr6BFxKnACcEpmzlbCdwCHNs22GtjZbvnMPD8zN2TmhlWrVvU3WEmSJEmSBmSgFfSIOBb4PeDlmflQ01ubgZMjYu+IOAw4EvjyIGOTJEmSJKlKy/q14oi4FHgecHBE7ADeRWPU9r2BT0cEwJbMfEtm3hwRVwBfp5H6/tbM3NOv2CRJkiRJqpu+VdAz87VtJn9gjvn/J/A/+xWPJEmSJEl1VuUo7pIkSZIkqWAFXZIkSZKkGrCCLkmSJElSDVhBlyRJkiSpBqygS5IkSZJUA1bQJUmSJEmqgb49Zk2SJGkUrTvjqlpv99Zzj+9zJJKkfrEFXZIkSZKkGrCCLkmSJElSDVhBlyRJkiSpBqygS5IkSZJUA1bQJUkSABFxbETcEhHbIuKMNu/vHRGXF+9fGxHriunHRMQNxc9XI+Lnyq5TkiT9kBV0SZJERCwFNgHHAU8DXhsRT2uZ7U3AVGYeAbwXOK+YfhOwITN/CjgW+MuIWFZynZIkqeBj1iRJEsAxwLbM3A4QEZcBJwJfb5rnRODs4u8rgfdFRGTmQ03zPA7ILtY59qp6bJskqX5sQZckSQCHALc3vd5RTGs7T2ZOA/cDKwEi4pkRcTNwI/CW4v0y66RY/vSI2BoRW++6664eFEeSpOFjBV2SJAFEm2lZdp7MvDYzjwJ+GjgzIh5Xcp0Uy5+fmRsyc8OqVau6CFuSpNFhBV2SJEGjdfvQptergZ2d5omIZcABwL3NM2TmN4AHgaNLrlOSJBWsoEuSJIDrgCMj4rCIWA6cDGxumWczcGrx96uBz2RmFsssA4iItcBTgVtLrlOSJBUcJE6SJJGZ0xHxNuCTwFLgg5l5c0ScA2zNzM3AB4CLImIbjZbzk4vFnwOcERG7gRngVzPzboB26xxowSRJGiJW0CVJEgCZeTVwdcu0s5r+/gFwUpvlLgIuKrtOSZLUninukiRJkiTVgBV0SZIkSZJqwAq6JEmSJEk1YAVdkiRJkqQasIIuSZIkSVINWEGXJEmSJKkGrKBLkiRJklQDVtAlSV2ZmJxi0zXbmJicqjoUSZKkkbKs6gAkScNjYnKKUy7Ywq7pGZYvW8LFp21k/doVVYclzWndGVeVmu/Wc4/vcySSJM3NFnRJUmlbtt/DrukZZhJ2T8+wZfs9VYckSZI0MqygS5JK23j4SpYvW8LSgL2WLWHj4SurDkmSJGlkmOIuSSpt/doVXHzaRrZsv4eNh680vV2SJKmHrKBLkrqyfu0KK+aSJEl9YIq7JEmSJEk1YAVdkiRJkqQasIIuSZIkSVIN2AddkiSJ8s9LlySpX2xBlyRJkiSpBqygS5IkSZJUA1bQJUmSJEmqASvokiRJkiTVgBV0SZIkSZJqwAq6JEmSJEk14GPWJEmSRkjZx8Xdeu7xfY5EktQtW9AlSZIkSaoBK+iSJEmSJNWAFXRJkiRJkmrACrokSZIkSTXQtwp6RHwwIr4TETc1TTsoIj4dEf9W/F5RTI+I+NOI2BYRX4uIZ/QrLkmSJEmS6qifLegfBo5tmXYG8M+ZeSTwz8VrgOOAI4uf04H39zEuSZIkSZJqp28V9Mz8F+DelsknAhcWf18IvKJp+l9nwxbgwIh4Ur9ikyRJkiSpbgbdB/1HMvNOgOL3E4vphwC3N823o5j2GBFxekRsjYitd911V1+DlSRJkiRpUOoySFy0mZbtZszM8zNzQ2ZuWLVqVZ/DkiRJkiRpMAZdQf/2bOp68fs7xfQdwKFN860Gdg44NkmSJEmSKjPoCvpm4NTi71OBjzVN/8ViNPeNwP2zqfCSJEmSJI2DZf1acURcCjwPODgidgDvAs4FroiINwG3AScVs18NvBTYBjwEvKFfcUnSOJqYnGLL9nvYePhK1q9dUXU4kiRJaqNvFfTMfG2Ht17YZt4E3tqvWCRpnE1MTnHKBVvYNT3D8mVLuPi0jVbSJUmSaqgug8RJkvpky/Z72DU9w0zC7ukZtmy/p+qQJEmS1IYVdEkacRsPX8nyZUtYGrDXsiVsPHxl1SFJkiSpjb6luEuS6mH92hVcfNpG+6BLkiTVnBX0Fg6kJGkUrV+7ws80SZKkmrOC3sSBlCRJkiRJVbEPehMHUpIkjbOIODYibomIbRFxRpv3946Iy4v3r42IdcX0F0fERETcWPx+QdMyny3WeUPx88TBlUiSpOFiC3qT2YGUdk/POJCSJGmsRMRSYBPwYmAHcF1EbM7MrzfN9iZgKjOPiIiTgfOA1wB3Ay/LzJ0RcTTwSeCQpuVOycytAymIJElDzAp6EwdSkiSNsWOAbZm5HSAiLgNOBJor6CcCZxd/Xwm8LyIiM7/SNM/NwOMiYu/MfLj/YUuSNDqsoLdwICVJ0pg6BLi96fUO4Jmd5snM6Yi4H1hJowV91quAr7RUzj8UEXuAjwDvzsxs3XhEnA6cDrBmzZpFFkWSpOFkH3RJkgQQbaa1VqTnnCcijqKR9v7mpvdPycyfAP5r8fP6dhvPzPMzc0Nmbli1alVXgUuSNCqsoEuSJGi0mB/a9Ho1sLPTPBGxDDgAuLd4vRr4KPCLmfnvswtk5h3F7weAS2ik0kuSpDasoEuSJIDrgCMj4rCIWA6cDGxumWczcGrx96uBz2RmRsSBwFXAmZn5hdmZI2JZRBxc/L0XcAJwU5/LIUnS0LKCLkmSyMxp4G00RmD/BnBFZt4cEedExMuL2T4ArIyIbcDbgdlHsb0NOAJ4Z8vj1PYGPhkRXwNuAO4A/mpwpZIkabg4SJwkSQIgM68Grm6ZdlbT3z8ATmqz3LuBd3dY7fpexihJ0iizBV2SJEmSpBqwgi5JkiRJUg1YQZckSZIkqQbsgy5JkqSO1p1xVan5bj33+D5HIkmjzxZ0SRpRE5NTbLpmGxOTU1WHIkmSpBJsQZekETQxOcUpF2xh1/QMy5ct4eLTNrJ+7Yqqw5IkSdIcbEGXpBG0Zfs97JqeYSZh9/QMW7bfU3VIkiRJmocVdEkaQRsPX8nyZUtYGrDXsiVsPHxl1SFJkiRpHqa4S9IIWr92BReftpEt2+9h4+ErTW+XJEkaAlbQ1bWJySm/9EtDYP3aFV6jkiRJQ8QKurriwFOSJEmS1B/2QVdXHHhKkiRJkvrDCrq60jzw1NIlwc77vu8zliVJkiSpB6ygqyuzA0+95pg1EMGlX76NUy7YYiVdkiRJkhbJCrq6tn7tCg45cB+m95jqLkmSJEm9YgVdC+IzlrVQE5NTbLpmm1kXkiRJUgtHcdeC+IxlLYRPAZAkSZI6s4KuBfMZy+pWu6cAeA5JkiRJDaa4SxoYu0ZIkiRJndmCLmlg7BohSZIkdWYFXdJA2TVCkiRJas8Ud0mSJEmSasAKuiRJkiRJNWAFfQT4XOnqeQwkSZIkLZZ90Iecz5WunsdAkiRJUi/Ygj7k2j1XWoPlMZAkSZLUC1bQh5zPla6ex6AcuwGMH495NSLioKpjkCRJC2OK+5DzudLV8xjMz24A48djXqlrI+IG4EPAJzIzqw5IkiSVYwV9BPhc6ep5DObWrhuA+2u0ecwr9RTgRcAbgT+LiMuBD2fmN6sNS5IkzccUd0l9ZzeA8eMxr042fDozXwucBpwKfDkiPhcRz6o4PEmSNAdb0CX1nd0Axo/HvDoRsRL4BeD1wLeBXwM2Az8F/C1wWHXRSZKkucxbQY+IVwLnAU8EovjJzNy/z7FJGiF2Axg/HvPKfAm4CHhFZu5omr41Iv6iopgkSVIJZVLc/xfw8sw8IDP3z8z9rJxLkqrmKPEd/ffM/IPmynlEnASQmedVF5YkSZpPmRT3b2fmN3q50Yj4LRr94hK4EXgD8CTgMuAg4Hrg9Zm5q5fblSSNBkeJn9MZwBUt086kkd4uPWLdGVdVHYIkqUXHCnqR2g6NlLjLgb8HHp59PzP/biEbjIhDgF8HnpaZ34+IK4CTgZcC783My4oUvDcB71/INiRJo81R4h8rIo6jcS89JCL+tOmt/YHpaqKSJEndmKsF/WVNfz8EvKTpdQILqqA3bXefiNgN7AvcCbwAeF3x/oXA2VhBHysTk1MOKCWplNlR4ndPzzhK/A/tBLYCLwcmmqY/APxWJRFJkqSudKygZ+YbACLi2Zn5heb3IuLZC91gZt4REe8BbgO+D3yKxheJ+zJz9j/8O4BDFroNDR/TVSV1w1HiHyszvwp8NSIubrqfSpKkIVKmD/qfAc8oMa2UiFgBnEjjMS/30egTd1ybWbPD8qcDpwOsWbNmISGohkxXldQtR4l/tIi4IjN/HvhKRDzmHpqZT68gLEmS1IW5+qA/C/gvwKqIeHvTW/sDSxexzRcB/5GZdxXb+btiOwdGxLLiv/6raaTqPUZmng+cD7Bhw4a2lXgNH9NVJWnRfqP4fUKlUUiSpAWbqwV9OfCEYp79mqZ/F3j1IrZ5G7AxIvalkeL+Qhp95q4p1nsZcCrwsUVsQ0PGdFVJWpzMvLP4PVl1LJIkaWHm6oP+OeBzEfHhXt7sM/PaiLiSxqPUpoGv0GgRvwq4LCLeXUz7QK+2qeFguqokLVxEPED77mEBZGbuP+CQJElSl8r0QX9fm75s99No9f7LzPxBtxvNzHcB72qZvB04ptt1SZIkyMz95p9LkiTV2ZIS82wHvgf8VfHzXeDbwFOK15IkqWIRsX/x+6B2PyXXcWxE3BIR2yLijDbv7x0RlxfvXxsR64rpL46IiYi4sfj9gqZl1hfTt0XEn0ZE9KbEkiSNnjIt6P85M5/b9PofIuJfMvO5EXFzvwLTePDZ55LUM5fQGCBugkaqe3NFOIHD51o4IpYCm4AX03jc6XURsTkzv94025uAqcw8IiJOBs4DXgPcDbwsM3dGxNHAJ/nh41LfT+PpK1uAq4FjgU8spqCSJI2qMhX0VRGxJjNvA4iINcDBxXu7+haZRp7PPpek3snME4rfhy1wFccA2zJzO0BEXEbjsajNFfQTgbOLv6+k0Q0uMvMrTfPcDDwuIvYGDgL2z8wvFev8a+AVWEGXJKmtMhX03wY+HxH/TuO/8YcBvxoRjwcu7GdwGm0++1yS+iMiXgk8h0bL+f/LzL8vsdghwO1Nr3cAz+w0T2ZOR8T9wEoaLeizXgV8JTMfjohDivU0r/MQJElSW/NW0DPz6og4EvhPNCro/9o0MNyf9DM4jTaffS6NFrus1ENE/DlwBHBpMektEfHizHzrfIu2mdY6SOyc80TEUTTS3l/SxTpnlz2dRio8a9asmSdUSZJGU5kWdID1wLpi/qdHBJn5132LSmPBZ59Lo8MuK7XyM8DRmZkAEXEhcGOJ5XYAhza9Xg3s7DDPjohYBhwA3FtsZzXwUeAXM/Pfm+ZfPc86AcjM82k8dpUNGza0rcRLkjTq5q2gR8RFwI8BNwB7iskJWEHXovnsc2k02GWlVm4B1gCTxetDga+VWO464MiIOAy4AzgZeF3LPJuBU4EvAa8GPpOZGREHAlcBZ2bmF2Znzsw7I+KBiNgIXAv8IvBnCy6ZJEkjrkwL+gbgabP/iZckqZVdVqoXEf9A4x/oBwDfiIgvF6+fCXxxvuWLPuVvozEC+1Lgg5l5c0ScA2zNzM3AB4CLImIbjZbzk4vF30Yjrf6dEfHOYtpLMvM7wK8AHwb2oTE4nAPESZLUQZkK+k3AjwJ39jkWSdKQsstKLbxnsSvIzKtpPAqtedpZTX//ADipzXLvBt7dYZ1bgaMXG5skSeOgTAX9YODrxX/iH56dmJkv71tUkqShY5eVamXm56qOQZIkLU6ZCvrZ/Q5C9eAIzJI0/Ir+3n8G/DiwnEa6+oOZuX+lgUmSpHmVecza5yJiLXBkZv5TROxL42avEeIIzJI0Mt5Ho2/439IYR+YXgSMrjUiSJJWyZL4ZIuKXgSuBvywmHQL8fT+D0g9NTE6x6ZptTExO9XU77UZglqS5DOrzSd3LzG3A0szck5kfAp5XcUiSJKmEMinubwWOofF4FDLz3yLiiX2NSsBgW7UdgVlSN8y6qbWHImI5cENE/C8ag7w+vuKYJElSCfO2oAMPZ+au2RcRsYzGY1vUZ4Ns1Z4dgfntL3mqX7Qlzcusm1p7PY37+9uAB2k8B/1VlUYkSZJKKdOC/rmI+H1gn4h4MfCrwD/0NyzB4Fu1HYF5vDlIoLph1k19ZeZk0YK+Dvg74Jbmf7RLkqT6KlNBPwN4E3Aj8GYaz0e9oJ9BqcHnCmtQTFdWt/x8qq+IOB74C+DfgQAOi4g3Z+Ynqo1MkiTNpxcPpsoAACAASURBVMwo7jPAXxU/GjBbtTUI7dKVPe80Hz+fauuPgecXA8URET8GXAVYQZckqeY6VtAj4kbm6GuemU/vS0SSBm7U0pVN19eY+85s5bywHfhOVcFIkqTy5mpBP2FgUUiq1CilK5uur3EVEa8s/rw5Iq4GrqDxj/aTgOsqC0ySJJXWsYKemZODDERStUYlXdl0fY2xlzX9/W3gZ4q/7wK8CCRJGgJlBomTpKExaun6UlmZ+YaqY5AkSYtjBV3SSBmldH1pISJiNfBnwLNppLh/HviNzNxRaWCSJGlepSroEbEPsCYzb+lzPJK0aKOSri8t0IeAS2j0PQf4hWLaiyuLSJIklbJkvhki4mXADcA/Fq9/KiI29zsw9dbE5BSbrtnGxORU1aHUKhaNB885jZlVmfmhzJwufj4MrKo6KEmSNL8yLehnA8cAnwXIzBsiYl3fIlLP1WlU6zrFovHgOacxdHdE/AJwafH6tcA9FcYjSZJKmrcFHZjOzPv7Hon6pt2o1saiceE5pzH0RuDngW8BdwKvLqZJkqSaK9OCflNEvA5YGhFHAr8OfLG/YamX6jSqdZ1i0XjwnKuPickpB+/rs4hYCrwqM19edSySJKl7ZSrovwa8A3iYxqAznwTe3c+g1Ft1GtW6TrFoPHjO1YNdDQYjM/dExInAe6uORZIkdW/eCnpmPkSjgv6O/oejfqnTqNZ1ikXjwXOuGs0t5u26GnhM+uYLEfE+4HLgwdmJmXl9dSFJkqQy5q2gR8SngZMy877i9Qrgssz82X4HJ0kaTq0t5medcJRdDQbnvxS/z2malsALKohFkiR1oUyK+8GzlXOAzJyKiCf2MSZJ0pBrbTGfemiXXQ0GJDOfX3UMkiRpYcpU0GciYk1m3gYQEWtp/CdekjSk+j1gW7vB+Rbb1cBB5sqJiJXAu4Dn0Lhffx44JzN9hIEkSTVXpoL+DuDzEfG54vVzgdP7F5IkqZ8GMWBbrwfnc5C5rlwG/AvwquL1KTT6o7+osogkSVIpZQaJ+8eIeAawEQjgtzLz7r5HJknqi0EN2NbLwfkcZK4rB2XmHzS9fndEvKKyaCRJUmlLSs63N3AvcD/wtIh4bv9CkiT102z6+dJgaAZsG8aYK3RNRJwcEUuKn58Hrqo6KEmSNL8yo7ifB7wGuBmYKSYnjfQ5SdKQGcZnww9jzBV6M/B24KLi9VLgwYh4O5CZuX9lkUmSpDmV6YP+CuCpmflwv4ORJA3GMD4bfhhjrkJm7ld1DJIkaWHKpLhvB/bqdyCS2puYnGLTNduYmJyqOpSeGcUySZIkSYtVpgX9IeCGiPhn4JFW9Mz89b5FJQkYzZGrR7FMkiRJUi+UqaBvLn4kDdgojlw9imWSJEmSeqHMY9YujIh9gDWZecsAYpIWbGJyaqQGkZoduXr39MzIjFw9imWS6iYilgI/QtN9PjNvqy4iSZJURplR3F8GvAdYDhwWET8FnJOZL+93cFI3RjF1ehRHrh7FMkl1EhG/BrwL+DaPfvrK0ysLSpIklVImxf1s4BjgswCZeUNEHNbHmKQFGdXU6VEcuXoUyyTVyG/QePrKPVUHIi3GujOuKj3vrece38dIJGlwyoziPp2Z97dMy34EIy3GbOr00sDUaUnj7Hag9b4tSZKGQJkW9Jsi4nXA0og4Evh14Iv9DUvqnqnTksZZRLy9+HM78NmIuIpHP33l/1QSmCRJKq1MBf3XgHfQuMlfAnwS+IN+BiUtlKnTksbYfsXv24qf5cUPmPkmSdJQKFNBPz4z30Gjkg5ARJwE/G3fopIkSV3JzP8BjXt0Zj7qHl3ctyVJUs2V6YN+ZslppUXEgRFxZUT8a0R8IyKeFREHRcSnI+Lfit82g0qS1L2e37clSdJgdGxBj4jjgJcCh0TEnza9tT8wvcjt/l/gHzPz1RGxHNgX+H3gnzPz3Ig4AzgD+L1FbmfojNpzvCU1eG2X435auD7ftyVJ0gDMleK+E9gKvByYaJr+APBbC91gROwPPBf4JYDM3AXsiogTgecVs11I47FuY1VBH8XneEvy2i7L/bRofblvS5KkwelYQc/MrwJfjYhLMnN3D7d5OHAX8KGI+EkaXyJ+A/iRzLyz2PadEfHEdgtHxOnA6QBr1qzpYVjVG9XneEvjzmu7HPfT4vTxvi1JkgakTB/0Y4o+4d+MiO0R8R8RsX0R21wGPAN4f2b+Z+BBGunspWTm+Zm5ITM3rFq1ahFh1I/P8dYompicYtM125iYnKo6lMq0Xtsr9l0+9vukHT8De+b6iPhay8//i4j3RsScOzUijo2IWyJiW9HdrPX9vSPi8uL9ayNiXTF9ZURcExHfi4j3tSzz2WKdNxQ/bf8BL0mSyo3i/gEaqXETwJ4ebHMHsCMzry1eX0mjgv7tiHhS0Xr+JOA7PdjWUPE53ho1piw3NF/bK/Zdzjkfv3ns90k7fgb2zCdo3K8vKV6fDARwP/Bh4GXtFoqIpcAm4MU07tXXRcTmzPx602xvAqYy84iIOBk4D3gN8APgncDRxU+rUzJz6yLLJUnSyCtTQb8/Mz/Rqw1m5rci4vaIeGpm3gK8EPh68XMqcG7x+2O92uYw8TneGiWmLP/Q7LW96Zpt7pM5+BnYE8/OzGc3vb4xIr6Qmc+OiF+YY7ljgG2ZuR0gIi4DTqRxf551InB28feVwPsiIjLzQeDzEXFEz0ohSdIYKlNBvyYi/jfwd8DDsxMz8/pFbPfXgIuLEdy3A2+gkW5/RUS8CbgN8Jmtqg1Hll6Y2ZTl3dMzpiwX3CcagCdExDNnM9Ui4hjgCcV7c43mfghwe9PrHcAzO82TmdMRcT+wErh7npg+FBF7gI8A787MbJ1hlMeYkSSprDIV9Nmb84amaQm8YKEbzcwbWtY364ULXafUL6ZpL5wpy4/lPtEAnAZ8MCKeQCO1/bvAaRHxeOCP5lgu2kxrrUiXmafVKZl5R0TsR6OC/nrgrx+zkszzgfMBNmzYMN86JUkaSfNW0DPz+YMIRKor07QXx5Tlx3KfqJ8y8zrgJyLiACAy876mt6+YY9EdwKFNr1fTeHRbu3l2RMQy4ADg3nniuaP4/UBEXEIjlf4xFXRJklSigh4RPwL8IfDkzDwuIp4GPCszP9D36KQaMCVZ0jCJiL2BVwHrgGURjUbvzDxnnkWvA46MiMOAO2gMLve6lnk20xgn5kvAq4HPtEtXb4plGXBgZt4dEXsBJwD/1G2ZJEkaF2VS3D8MfAh4R/H6m8DlNEZ3l0aeKcmShszHaIzYPkHT2DHzKfqUvw34JLAU+GBm3hwR5wBbM3MzjXv/RRGxjUbL+cmzy0fErcD+wPKIeAXwEmAS+GRROV9Ko3L+V4svoiRJo6lMBf3gzLwiIs6ER27gvXjcmjQ0TEmWNERWZ+axC1kwM68Grm6ZdlbT3z+gwyCumbmuw2rXLyQWSZLG0ZIS8zwYESspBoGJiI00/jMvSZLq54sR8RNVByFJkrpXpgX97TT6nP1YRHwBWEWj35kkSaqf5wC/FBH/QSPFPYDMzKdXG5YkSZpPmVHcr4+InwGeSuMmf0tm7u57ZJI0hCYmpxyvQFU7ruoAJEnSwsyb4h4RJwH7ZObNwCuAyyPiGX2PTJKGzMTkFKdcsIU//tQtnHLBFiYmp6oOSWMoMydpPArtBcXfD1GuS5skSapYmRv2O4tnlz4H+FngQuD9/Q1LkobPlu33sGt6hpmE3dMzbNl+T9UhaQxFxLuA3wPOLCbtBfxNdRFJkqSyylTQZ0dsPx54f2Z+DFjev5AkaXhMTE6x6ZptTExOsfHwlSxftoSlAXstW8LGw1dWHZ7G088BLwceBMjMncB+lUYkSZJKKTNI3B0R8ZfAi4DzImJvTJWTpEdS2ndNz7B82RIuPm0jF5+20T7oqtquzMyImH36yuOrDkiSJJVTpoL+88CxwHsy876IeBLw3/obliTVX7uU9rc+/wgr5qraFcU/1g+MiF8G3gj8VcUxSZKkEsqM4v4Q8HdNr+8E7uxnUOOoHyM/O5q01F+zKe27p2fGOqXdz5p6ycz3RMSLge/SeALLWZn56YrDkiRJJZRpQVeftUuTXeyX3H6sU9KjrV+7YuxT2v2sqaeiQm6lXJKkIWMFvQbapcku9gtuP9Yp6bHWr10x1teWnzX1EREPANnuLSAzc/8BhyRJkrpkBb0G+pEm26t11iF1tV8x1KFsUr/1+zw3zb8+MtOR2iVJGnJW0GugH2myvVhnHVJX+xVDHcom9dsgznPT/CXNWnfGVaXmu/Xc4/sciSQNLyvoNdGPNNnFrrMOqav9iqEOZZP6bVDn+bin+UuSJPWKzzNXR7Opq0uDylJX+xVDHcqmziYmp9h0zTYmJqeqDmWoeZ5LkiQNF1vQ1VEdUlf7FUMdyqb27H7QO57nkiRJw8UKuuZUh9TVhcRQZmCs+dbrIHLVsPtBb1V1DXv9SJIkdc8KukZOL1pgbcWtjqOCDz+vH0mSpIWxD7pGTrsW2CrWoYWZTct++0ueasVuSHn9SJIkLYwt6CplmNJVe9ECW8dW3GE6BovVi7TsOu2vOsUyCHW8fgZl3I61JEnqLSvomtewpav2YmCsug2uNWzHoGp12l91imVQ6nb9DMo4HmtJktRbVtA1r2EctKsXLbB1GCBv1jAegyrVaX/VKZZBqtP1MyjjeqwlSVLv2Add8/JZytXzGHSnTvurTrGU4TPoF27YjrUkSaofW9A1r3FNV60Tj0F36rS/6hTLfEzRXpxhOtaSJKmerKCrlHFMV60bj0F36rS/6hTLXEzRXrxhOdaSJKmeTHHX2BuGlN5+xDgM5a6rUd13pmhLkiRVyxZ0jbVhSOntR4zDUO66GuV9Z4q2JElStayga6wNQ0pvP2IchnLX1ajvO1O0JfXbujOuqjoESaotU9wFjG7K7nyGIaW3HzHWudx1PxfrvO/GWd3PG0mSpDJsQddIp+zOZxhSevsRY13LPQznYl333TgbhvNGkiSpDCvoY25icoo/+advjnTK7nyGIaW3HzHWsdyd0scnJqf4yPU7COCVz1j9qLgnJqcGXllezL7rFG8V5eiFKuJu3WbZbgfDuo8lSdL4sII+xmZbnR7ePUMCS0zZVcVm08d3T888ci5OTE7x2vO/xK49CcDfTuzg0l/e+EjFfZhaTjvFO2zlmFVF3O222e68qUOskiRJ3bIP+hibbXVKGifCs4842C+tqtRs+vjbX/LUR87FLdvvYXdROYcftpBC+xb3OusU77CVY1YVcXdqLW89b+oQqyRJUrdsQe+hYUufbG11+s0XPWUkUm413GbTx2cH/Vqx73L2WhqPtKA3t5CWaTmtk07xDqocZa7pbq77KvZ/p202dzto1yVi2M4VSZI0niIz55+rpjZs2JBbt26tOgxgeNMn5+oPO4zl0WhoPf/OOuEobtp5f236oC9GVX3Qy1zTC7nu69AHvfW95i4Ry5cteVSXiGE6V2ZFxERmbqg6jkHq9f3dx3qNvlvPPb7qECSpK53u77ag98iwPhu502BXw1oejYbW82/qoV384c/9RNt56zjY3Vw6xdvvcpS5phdy3Vex/+faZqcuEbPLDNO5IkmSxo990AuLfYZu67ORV+y7fKifydvuWc8+Z1iD4rPGyyt7XZbZp83zLF0S7Lzv+wO53nv52bLx8JXstTQeee35I0mShokp7vQunXs2fXLFvss55+M3D316eHM6KGDKuwZqWNORB6nbz66yfdA/cv0OrpzYwfSe/l/v/ehOM9dj+YaRKe6LZ4r76DPFXdKwMcV9Dr1K555Nn9x0zbaRSA9vTgcdlTJpeJiOPL9uP7vK7NPZkfOn9wzmeu9HdxrPHUmSNKxMcaf36bRVpYn2kynH48cuDf3Xq641S4CIYMW+y3sS1yCvdz9bJEmSfsgU90Kv02kHnSY6CKYcjw9H8e+/Xu3jS669jbM+dhMzmT09VoO83v1smZsp7otnivvoM8Vd0rAxxX0evU6JHHSa6CCYNjo+HMW//3q1j6ce2sVMZs+P1SCvdz9bJEmSGkxx70K36ahVjexuavL4aXfMF3MemHbcf73ax71Yz7h/Zox7+ZtFxLERcUtEbIuIM9q8v3dEXF68f21ErCumr4yIayLiexHxvpZl1kfEjcUyfxoR0bpeSZLUUFkLekQsBbYCd2TmCRFxGHAZcBBwPfD6zNxVVXytFpKOun7tCi4+beNAR3Y3NXn8tDvmsLhR95vPXdOO+6NX+3ix6xn3z4xxL3+z4r68CXgxsAO4LiI2Z+bXm2Z7EzCVmUdExMnAecBrgB8A7wSOLn6avR84HdgCXA0cC3yin2WRJGlYVdmC/hvAN5penwe8NzOPBKZofAmojXbpqGWsX7uCtz7/CKYe2rWg5QcVp4ZXu2Pei/Ng9twd18rKIPRqHy9mPeP+mTHu5W9xDLAtM7cX/yC/DDixZZ4TgQuLv68EXhgRkZkPZubnaVTUHxERTwL2z8wvZWPQm78GXtHXUkiSNMQqqaBHxGrgeOCC4nUAL6Bxs4fGzb9WN/DFppEOKmXY1OSGfqes1ikltt0xX+h5UKdyDbsq9uVCtjnunxnjXv4WhwC3N73eUUxrO09mTgP3A3PttEOK9cy1TgAi4vSI2BoRW++6664uQ5ckaTRUleL+J8DvAvsVr1cC9xU3e5jjBl6VxaaRDipl2NTk/qes1i0lttMx7/Y8qFu5hlkV+3Kh2xz3z4xxL3+Ldn3DWx/1UmaeBc2fmecD50NjFPc51ilJ0sgaeAU9Ik4AvpOZExHxvNnJbWZte3OOiNNp9GVjzZo1fYmxk8WONDyokYrHfUTk5pTVXbtn+JN/+ia/+aKn9Gyf1HGE83bHvNvzYFDlGodHalVxjixmm+P+mTHu5W+yAzi06fVqYGeHeXZExDLgAODeeda5ep51SpKkQhUp7s8GXh4Rt9Lo3/YCGi3qBxY3e5jjBp6Z52fmhszcsGrVqkHEqyEzm7K6BJgBvrDtbk65YEvPUo1HNSV2EOWabeX940/d0tNjUjdVnCOjel5qoK4DjoyIwyJiOXAysLllns3AqcXfrwY+U/Qtbysz7wQeiIiNRXe2XwQ+1vvQJUkaDQNvQc/MM4EzAYoW9N/JzFMi4m9p3Owvo3Hz9wauBZlNWf2Tf/omX9h2d1+eDz2KKbGDKFcdsw/6oYpzZFTPSw1OZk5HxNuATwJLgQ9m5s0RcQ6wNTM3Ax8ALoqIbTRazk+eXb74x/v+wPKIeAXwkmIE+F8BPgzsQ2P0dkdwlySpg8oes9bG7wGXRcS7ga/Q+BKgii0kHbkOKczr167gN1/0FK679V52T8/0vEWx6pTYfu3jfpdrtpW3H8ekbqo4R6o+L4dZHT636iAzr6bxKLTmaWc1/f0D4KQOy67rMH0rj330miRJaqPSCnpmfhb4bPH3dhqPeFFNLGTQqToNNDaqLYp12sfdGtVjouE2zNeUJEkaLXVqQVfNLCQduW4pzKPYoli3fdytUTwmGm7Dfk1JkqTRUclz0LU4E5NT/P5Hb+QdH73xkUG2+vHM5YUMOjXogar6Ue46PL96rhia9/HSJcHO+76/4Fh97rmqUIdrrNlCP7e8fiRJUq/FHIOv1t6GDRty69atVYcxUBOTU7z2/C+xa0/juC1ftoSzX3YU53z85r6kZ9a5D3o/0lLr8Pzqs06Y/3hOTE7xket3cOXEDqb3LCxW03pVhTpcY52uqW4+twZZjoiYyMwNfVl5TfX6/r7ujKt6ti7V063nHl91CJLUlU73d1vQh8yW7fewe88P/6mye3qGT9x052PSM3tl/doVvPX5R3T1xXMhyyxEu7TUOq6z222WOZ7r167gkAP3YXrPwmOtoqxSHa6xTtdUN59bXj+SJKkfrKAPmY2Hr2SvpfHI672WLeG4o580ls8/7kc6fR2eX132eC421rmWnyt1t917pvoOl0Efr+btzZ53S4CIYMW+y/u+/VH5rJAkSaPPFPchNJveHMArn7Ga9WtXjO0jgvpR7ir2Zes2y8aw2FjbLT9X6m679wBT5YfIoFPM223vlm89wFkfu4mZzIGmuQ/rZ4Up7otnivvoM8Vd0rDpdH93FPch1G4U7HEdGbsf5a7D86vLxrDYWNstP9eI1p3Seh0Be3gMesTyTufMTOZAz5lR+ayQJEmjzQp6xeZrgRnXlvHF6MU+q8t+ryKO2dTd3dMzj0nd7fRep/n7YVDHt5/bqfL8muv4trPYWBdyztTl+pMkSRo0K+gVmi/V1FG2u9eLfVaX/V5VHOvXruDi0za2rSB1eq/T/L02qOPbz+1UfX7NdXzLlqEX2+sUQ9X7R5IkqUpW0Cs0X6rpoFNRR0Ev9lld9nuVccyVultlF4tBHd9+bqcO51fZ49WrWLs5Z+qwfyRJkqpiBb0Cs+mbK/ZdPmea52xq6K7dMwMb7XiYtBtY7Y77vs+ypUvYs2fu9N25Umi7TQHul/niaDdYYDsLHYBuPt2sZ655u1lPN8em03rnWkfZa7OMTtupy/lVRhWxljk+pr5LkqRR5SjuA9aavnnWCUcx9dCujl84L7n2toGPdjwM2u3Hcz5+M7umZ1i2JDhpw6EdK61lU5zrUBGYqw/za8//Erv2NK7f5cuWcOkvz5+q3byfFnM+dZOG3O2o8Iup7JeNr8zo9fNdm2XUsQ96t+rwVIPZaeOa+u4o7ovnKO6jz1HcJQ0bR3FfoF5/OW1N35x6aBdvff4RHeefemjXwEc7Hgat+/ETN935yOs9M8mTD9yn434qk0Jbl9GZ50oD3r3nh/9cay3HbOv6zXfc/6iyXn7dbTy8e4Zss0w3uklD7mZU+I9cv2Pe663MsVlImnS312aZz4ZOsdbl/GrWqTx1eKoBmPouSZLGgxX0OfSjxabblNFhSocdpNb9ctzRT+K6W+8ttZ9GYZ9uPHwley2NR1rQm8vR2roOsCRg6ZLg5ju/y+zUpUsXXvZu9mHZUeGXLgmunNjB9J7FX2/zpUm3u667TZ8fpdbcYSjPKFy3kiRJ87GCPod+tNh0M4LyQuYfF+32y1N/dL9S+2kU9un6tSu49PRnte2D3tq6DvDsIw5mzUH7cumXbwMggFev79xvvcz2y+7DsqPC77zv+1z65dt6cr3Ntc1O13U3ZRq11txhKM8oXLeSJEnzsYI+h3612HSbMlpFiukw9JFt3S/d7Kc6pRgvdF93KkNr6/ryZUv4zRc9BYCPXL/jkfP5Vc9Yvai4y+zD5rJ1ShefXc9sWn6vrre59k+vB5pbse9yNl2zjRX7Ln9Uv/W6XEfzxdGLASkHUdY6XbeSJEn94CBx86jLF+xBGoZ011HRr33daYT3QZ7P/RoArlexlRkkbq4B9ZpHfD/n4zc/0rd/SdDTAfl6UdYyx2ExA1L6mdFfDhK3eA4SN/ocJE7SsHGQuAUaxxabYUh3HRX92td1GJxsIWUbVHxlBiFrHniwXfyz69h0zTZ2Tc880re/7PKDUvY4LGZASj8zJEmSemNJ1QGos4nJKTZds42JyamBbnc23XVpMO+zxKuIr58GXaay+7rf+lHufpRtNs5Lrr2t7/Eed/STSsU/u9zsh+mSLpfvt7LHYTHHq8yyZc6xOnymLDSGOsQuSZKGnynuNVV1yuh8qcZVx9cPVZWp6m4U/Sx3L8s2G2drKnk/4y0bf3O6+zD2Qe92vm6XLXOO1eEzZaEx9Dt2U9wXzxT30WeKu6RhY4r7kKk6ZXS+VOOq4+uHqspUdTeKfpa7l2WbjbM1lbyf8ZaNvw5dCuay2HIsdhtlzrE6fKYsNIY6xC5JkkaDFfQOum1J6nVLWaeRpvvRIreQdY7iM4nblalMJkFVg5r1al3djODdaXC1srEtphzNcc7ww2e777zv+0xMTtWqxXpU9Gp/lvm8WMxnSqcMhl7GOde+WLHvcpZEADkyn4eSJKkapri30W26Yj9H4m5Nt+31dhazzlGsDDWXCZhz3wwqJbeX2+m0rjIjeLdbFubeR70uR3NF7Kad93PlxA6m95QbdV3d6fX5XebzYiGfKb3u+lBmhP/mdTdvf+mS4JwTj+Z1z1zT9XbnYor74pniPvpMcZc0bDrd3x0kro126Yq9nL+s9WtX8NbnH/HIF8F+bGcx62yNbxQ0l2m+fdOv496ql9vptK52I3iXWbab2HpRjtnj87pnruGQA/dhes/co65r4Xp9fpf5vFjIZ8pcXR96Fedc+6J5+5nJ1EO7FrRdSZIksILeVrejGQ9qJO5+bKcuo4jX0Xz7pt37dR8NvdO6ymyj3TzdxNbrc22ho64PkypHBh+Wz4bWUfQD5u2qsdBttLvWV+y7fCj2kyRJGg6muHdQdR/0QW5nFFPVe6WbPuhQPt2713H0Yl0LTUEeVB/0MusbpXO5LqOaD8P+nI3zge/v5oLP/8ecXTUWu4121/pZJxy1qL7v8zHFffFMcR99prhLGjaO4t6lbkcznmv+Xn7JXcwoy53iGNRI0/M9hqnqR4112jfAIymtrbE177tN12wbitHQW9fVXPa3Pv+ItsvMNU83sc03b7fnwUJHXR8GzWnVD++e4SPX7+jbGAed9nkd9meZc2I2zk3XbHtMV42FDO7Y7v25rvWph3Z1vHYkSZK6YQW9z+rQClaHOMoMsjTssQ3jyPZ1ej511edB3Ww8fCXLlgS79iQJXDmxg1c9Y3VP90nd93m38ZW5Budb5/9v797D7arrO4+/v+ecHCVqISIq98sDWklULhmI0jrgFSw1U4UCdma8M06xF0efKegz1KZ1pkx96milrVQdrcPNCaAZpoiIWLVjAiSCJGA0DbeY1AskqA3NSc75zh977bDOOmvtve6XvT+v58mTs/dZ67d+67sue6+zvr/vSrPMLh7rIiIi0g0ag16xugqJtb0faYosdb1vpx69EHHDEgAAIABJREFUhKvfuYL/9NoXtu5CJ0ma9etiMbxRcOrRSzh/+ZFY8Hp2tvyYtD3mWfuX5hgso/hjF491ERER6QbdQR8gz3jbpGdMp7nTkvcRQ2nGDf9w15NMTU4wOzu4H2n7EDf294YN2zDgjTF3+QY9a7vpu1GDlp+1b2WkBNeZ7l/186nL7su4eeMpR3DDhm2VxSRrzOseipJnnxg0hCPNOTntMtuQ/i8iIiKjR0XiEuR55nNSamTai+isqaZZ05OnJozzlx8ZewGdpQ/R6S4/dykfWrORmdnevjQ9NcG171o476Bnbbd1DHrdfWsi5biq51NX1ZdxU3VMsvxRrol0+CLrn/ec3Lb9UEXiilORuNGnInEi0jUqEpdRUprjoCJgcfP077IM+5KXNG/RecLTzM45hx10QGK7afsQne6WjTvYO/vUH3qS5o171nZcAaYmDFp+nX3Lsx8UlWb96opB0/tBG1Udk7TtN7FvZulfnLznZO2HIiIi0hSNQU+Q55nP4efxJj2HN+m5xsPajpsv77Ors6xz3LLjnj+9aNL2t5OnL2me91z1M6Gztr/+4Z184Kb7+OBN95XWp648e7qvyed0192HpOUMW34bYlSGru2b0M0+i4iIyHhTivsAecagD0vjHpYin/R86kFVxstMT44bW54mRXTYGPRhMW26knjW9tc/vJOLrvr20LT+vH1pU3ptkjZUAG+6wnwZFcG7pCv7ZlgX+xylFPfilOI++pTiLiJdoxT3HOLSHIelPg5K4x6WIprU9qD5yk5Pjk6bNkU07TLipsuaql9Fem3W9tdufSxVWn8eXUmvbSrluYk+JC1n2PLbEKMydWXfDOtin0VERGR8KcW9ZINSKvOmW+aZr6y02qzLzrPcJYunmTBjoqRU/Tyytr/iuINTpfXHxWNUU56XLJ6ufb2ybLcicU/aR9MOeym631YxnKJt6jpW8gxlGYXjVURERLpBKe4VqKIieNY09TLTaqus8tyfZ8/eOSYnjFUrl/Hm048q1I+8srY/LK0/z5MAuqYfsyWLp1l186ZG1quqpyRE503aR6uuCF7lcIq2qOtYyTOUpQ3Hq1Lci1OKu1RFqfUikpdS3GtURUXwLPOVnVZbZZXn/jwOuDs7d88U7kdeWdsfNn2eJwF0TT8GV96xpbH1SrPdihwTw/bRqiuCVzmcoi3qOlbyDGUZpeNVRERE2k8X6Dm1ofBQUh/6abV7983VWrm4v9yZvXOJVezD1j+8kx/uepKpyQlmZ+cWVI7PkjGQpkBdXnm3ddJ2aGLbQLX7bFP7XNig9UvqX5pCkHnWrUhhxqj+cIr+HfTwcII6zz/XrHuEWzbu4Jxlh/Lm048qZX/qH7s//fme2PNAOO5lrHPWbdmG/VpERETGi1Lcc2hD2mPeivBVG1TFPqn/UxPG+cuP3H9xnSW+Vaf/Ft3WeZ4EUIU69tkm/2iV9kkAw55QAPFp1VUNMUk7bfiPUEsPO7D24QTXrHuED9x03/7X737FcXz22w8V6kP02J2aNC4InQf605Q9hCLPUJam/xirFPfilOIuVVGKu4jkpRT3ErUh7TFvRfiqDapiHxbu/+ycc9hBB6Sudh9tp8r036LbOm47NLFt6thnm6yWnWb9ov3LklZd1RCTtNOGl9/EcIJbNu6Y9/rLm/6pcB+ix+7s7PzzAFQzhKLsoSwiIiIiZVIV9xyqrijelT7E6fdrAvanucdVQS6r2n3aaupF16dtcc6qyHrUVcW6SHXtPOsXN8+wdtL0cVAb0fnjjpc8/a7aOcsOnff67KXPL9yHLMduV47DUaj4bmZnm9lmM9tiZpfG/P5pZnZ98Pt1ZnZM6HeXBe9vNrPXhd5/yMzuM7N7zKx9lV1FRERaRCnuObUh7bENfYgTTnOfmjAwY9/swtTUsqrdt3UMetvkWY+6hnOUUV0byLV+aYcgZE1dj2s3bv60w0KGtV+1Ksegpzl2234cVnms1JXibmaTwPeB1wDbgLuAi9z9/tA0vw28xN3fbWYXAr/h7heY2YnAtcBpwGHAV4EXuPusmT0ELHf3n6bti1LcpSuU4i4ieSnFvWThdOzw67Cqv1C2NfVyXpr7rAOOszA1dVD/s6xb26q7t/VCIk+c6hrOUUZ17UvOOj7TmOIli6fZuXuGJYun5x3HSXHK0se4NpLmTzssZFj7SetZ1n745tOPmvd4uXAf8i5r0HpE28yz/9ZxLPaXsX3Xk40PfSrBacAWd98KYGbXASuB+0PTrAQ+FPy8GviEmVnw/nXuvgd40My2BO19u6a+i4iIjARdoOeUpkhb04XkmhKufDwZ3EGPVmceVaO23euqYl1Xde3wM80dMMCBCWPo9ioai6T5q4hxnfthFcsqo826CiOGi13GVaLvmMOBR0OvtwGnJ03j7vvM7Ang4OD9tZF5Dw9+duArZubAJ939qgr6LiIiMhJ0gZ7TsLtpbSgk15RTj17C1e9csf/OFWRPPe6qUdvu0W1Z1bpkXU7efoWfaQ7s/z/tXfEisUiav4oY17kfVrGsMtqsIwbRYpcXnHYkhx90QJfPdRbzXnQcXNI0g+Y9w923m9lzgdvM7Hvu/o0FCze7GLgY4Kijjor+WkREZCzoAj2lpGcjJz3ze1Sfn5s2ZTSajpp3THDSGNW2ppHHbfe29jWt/rbsF8Bqy5CNPCnP4eN2jvl30Ic9Hz3vMtP0uexhGmnPP2Xsm1Wc68pos8x+JcUpuow3VVADo2bbgCNDr48AtidMs83MpoADgccHzevu/f9/bGY30Ut9X3CBHtxZvwp6Y9BLWB8REZHOUZG4FPIWd+r6hVlU1Smj0fYvP3cpH1qzccEzziH+WdVtEd7u0O6+pjVKqftxY9B37p4Z+Hz0Lq7rsPNPmetZxbmurCJ0ZbQxbDhT1ef5GovETdErEvcq4If0isS92d03haa5BHhxqEjcG939N81sKXANTxWJux04AXg6MOHuPzezZwC3Aavc/cuD+qIicdIVKhInInm1pkicmR0J/C3wfGAOuMrdP2ZmzwauB44BHgJ+091b8ayavMWd2lrELa+qU0aj7d+ycUfsM84h/lnVbRHe7k08s7oKo5S6P+y4HJV1rXM9qzjXldFmGW0Mi9MoneeDMeXvAW4FJoHPuPsmM1sF3O3ua4BPA58PisA9DlwYzLvJzL5Ar6DcPuCSoIL784CbenXkmAKuGXZxLiIiMs6aSHHfB7zP3TeY2bOA9WZ2G/BW4HZ3/9Pg2auXAn/QQP8WqKO4U1xq97C7MnXfoa86bT/a/jnLDmXd1sf230EPL7Pq1NWy2ginVAPc++gurln3yP47tsC8u+11bfOs7SxZPM2E9RLCq9j2RderzGMhuh8uWTw9L7W/qcyYspc7qsNwighnV2zc/gQGLD3swHlFL+99dBcfvOm+Sh7p2Abu/nfA30Xeuzz0878A5yfM+2Hgw5H3tgIvLb+nIiIio6nxFHcz+xLwieDfme6+w8wOBb7u7i8cNG+dz0Ef9GzkslMoLz93Katu3jQw9bSpNNyqL07qHINeV6Xoa9Y9wn/54n2EkgGYMOY9I37Q8+LL7G+edsKVzycnjFUrl8173FZRRderqiri/Qu18LGY5tisQlXH+6gNwykiWuG/b3pqgg/9+lI2bn+CL9z9KPsiQ27qiFtdKe5tohR36QqluItIXq1JcQ8zs2OAk4F1wPPcfQdAcJH+3IR5GqnyWmVxp7jU7mGpp02l4VadzhlXXK6uuFdVKbo3FGL+fAueET/gefFl9jdPO+HK5+7Ozt0zmZdZZn/Knj9Of/+KDlFIc2xWoarjfZTSs4uKVvjv27tvjp27Zzj8oAOYjRlyo/iJiIhImSaaWrCZPRO4Afh9d/9Z2vnc/Sp3X+7uyw855JDqOlijfqrpZFBJ+pxlh857HZd6Gp0nXH36yju2sP7hVgzfX6BN/UuK4SDR/ofbmJwwtu96csG6rTjuYBZNzn8C0YTBokljUX/54Z8zbvM0/b1m3SP7+521nTzLzaJo+1X2L8+xWYV+P/on7Hsf3ZX7GKr6GGzTMT5MuK/RGPf1t3P0ONawABEREalCIynuZrYIuBm41d3/PHhvMy1Oca9aGWPQ2159uo39y5Lim9T/fhr+6vXbElPUw6n6Sw87sJYx6NGU3Qljf9/SLCvvcvNo0xj0YW03lRYeHSqRJ8W67icxtOEYTxLXV2DBGPTo0Jq4ITdVU4p7cUpxl6ooxV1E8mpNirv1Srl+Gnigf3EeWAO8BfjT4P8v1d23JqVN7R40T9urT7exf1lSfJP6f+rRS1i79TH2zear9Bydruz+9v8EF+7bJWcdnyn2dQ9tqHv+LG03lRYeHSrRluEAdbZfpri+DjsuNCRAREREqtZEivsZwL8DXmlm9wT/Xk/vwvw1ZvYD4DXB60YMS9HMm8JZdepn1anIRbW9f8MM6n/0d/3K302k+fb3syWLp+el7E50NO5FdDGdO6nNMlKs2zRcoelU+CKxaLrvIiIiMroar+JeRBUp7sNSNPOmcNaV+tn2qsxt798wg/qfVPm7zjTfuCcC7Nw9w5LF0/vT6rsY9zy6mM6d5vxTNMW6DcMV2pIKnycWTfRdKe7FKcVdqqIUdxHJqzUp7m03LEUzbwpnXamfbU/BbHv/hhmWqh5X+bvONN/ofrZz9wyXnHV8Lctumy6mcw9rs4zjpw3DFdqSCp8nFm3pu4iIiIwmXaBH9NMe9+6b25/2GL7LEvf7vO1KtcJ3tOOKslX1pXrFcQczNWHsnXUmJyz3ts5zd6/u/Syuj01nSYS3+/TUBDN75zAzliyeLnU5VcQ6S5tNx7mIrLELrytUfwzHLbe/LJ3LRUREpEpKcY8R/TKYVOk36xfELn+h7pq4CuZTEwZmiZXWy1z2RX+zdv8X+KyVtsP9z5NGW9d+llQFu8nU5Wif3vqyY/jUtx5kzr2yNPeyY92lFPEi0sYuvK51HcPR5UaXVfe5XCnuxSnFXaqiFHcRyUsp7imEv3T104Lj0pX7v1u79TFgeOXtvmg6ZRMX7F34I0Gacd7D7qTFVjCfdcBxqk1N7Vd0d2B2dv5y0sY/bxptnds3ro/A/vf27J3jhg3bat3Pon3atONnzLmnimOe2BVJFx+WfQC988+g/Tsuzm04xtP0IW3s5m3THMdw3ngMOga7PlRHRERE2ksX6IGkuyVJKe9F7141cQesC3fdht21Snsnrb/dZvbOMcf8O+izs9WmpialwGaJf5402rq3b1IfpyaMmdneRdTq9dt4U43Pi4726Zxlh3LXQ48PjWPdsRuWfZBm/46Lc7iNpo7xsmMZ3qaTGY/hIn1RKruIiIg0QRfogUHPuL76nSvm3YEpowhYE4WGulDcaFAfs9xJC2+3usegx+0zw9YtbRuD1L19k/p4/vIjuWbdI7EZBFWL69MLn/+soXGsO3bDsg/S7N9xcZ7XRkPHeNmxjG7T/jLSHBdF+pLnGBQREREpShfogUF3S6LpjMPurKRJqRzURt6UzGHzpb0jlGX5ZTz2KW0fs95JG5SGGjc8oczU4Lhlp4l/tA9Z+lHFHb9hMYnr4xtPOYIbNmwrfT9LO220T2nimGfbFJG0vCz7d1Kcm77rW8b5MSpum5bRl6zLHaQNQwtERESk+1QkLqSMC4YsKZVJY1CrfM76sHXM2v+Lrvo2M7O9fWg6Z0G0LH0sWs05af3a8Jz6soZOlHWRUGWhuqz72ShsmzTLy7p/t7GK/qA+NDW0p+p4VL1eKhJXnIrESVVUJE5E8lKRuBSy3C1JmjZrGvOg4k9VPGd92DpmWf7arY8Fqbg9ZaXUDupj3jtpfUnrV1eK87C7+kX7kPWu+yBF04PL3M9GYdukWV7W/TtNG00o4/xYdV/K1IXhQyIiItINukAvQdHnpDfxnPWku0pZ2llx3MEsmrT9d9DjUvXLSn8v6y5Y0vq1oSBUG/oAC58jnmUYRto7wFn3s6bjUmcf2nwHvKg2bMsqjOp6iUj3NZU9oswCkfyU4l5QUjXmLKnydT9nfVg6Zhlj0MtMfy87fTTNxeWoXRhlWX441pefu3R/gb1hwzCyPq+6qjHoVRmFVOk29KEN27IKVa6XUtyLU4q7VKXtF6K6QBdpL6W4VyQutfGSs45P/QWt6PxhaVM5h6VjlpXqX1b6exVVofOk/9eh6T5EY71z9wyXnHX8wGlihwmkeF51GftZncYlVbrqPrRhW1ZhVNdLRERE6jXRdAe6rp/aOGkLU7zrmD+PuGWuf3gnV96xhfUP7yxtGYsmbf/rIuvWRIwGSRurrDEtexvkkSbWSdPMe3/SWFTCNis7JlXGuIy2i+zrZa1b08dbXcdNG443ERERkSiluJegaGpjEymf0bHCVaS0tnEMelFZquVniWkbUpvDfclTPTz6PhR75nwVQxuqinGZbefZ1+saBlK1uo6bNh1vaSjFvTiluEtV2p7KrRR3kfZSinuFiqY2NpEaGV7mlXdsqSSltcz1akv6aNr036xpwm1Ibe5LE+u0wwSKrEPZMakyxmW2nWdfr2sYSNXqOm7adLyJiIiIhCnFvUPKSMmMa6PplNY6FY1h2lilmS7cl3HaBmmVHZMqY9z09osuf8ni6U6mb2eNY964rzjuYKYmJzDAJoztu57sXKxERERkNCnFvSPKSMkc1EZbUsirVFZaa5Zq+UnTlVm9f5SVvV9WuZ83fQyFH4+36uZNnUnfjsoax7xDAsJPmZgwWh0rpbgXpxR3qUraVO5x2wfLTnFPGz+l1kuXKMW94/KmZIa/vEbbuGHDtnlfbNv4xbRMN27Yxp69cwMriw8SjmW0snmcQTEts3p/0xeGScroV3++tVsfm/c6r6RtUlZfyxzTnvciNc25osj6Vr2/ZY1jnn1k7dbH2Df31B+n055X23qsiYiIyOjQBXpH9FM59+6bS53KGfdM634bkxPG6vXbBj6nepSsf3gn//vuR+l/JZ+czF4hu8yiUnm2Zx39KkuZ2QpVr1/bYli0UFr4OI/bt4qsb9tilbdP/eNvZu8cc/TuoA87Dtu47iIiIjJ6dIHeEacevYSr37ki092buGda99vYvutJrr3zkbEpkhS+Y2bAeadmqypfRRGurNuzjn6Vpax+1bF+bYth0UJp4eM8bt8qsr5ti1XePoWPvyWLp9m5e2bocdjGdRcREZHRowv0IcpKaYy2k6fdtKmf4bGo0Ttp/Tb6j0Aregc3rTzrn+ZRXmljt2TxNBNmgDM9NcGbTjkiU//LuuPdV1aKcbhfk6FiV01fOKSN17A4lB33In2tS9b+xE0fPs6vvGPLvPjmzcZJOqeknbfstPAsfYrrQ9ZU+rbtJyIiIjKaVCRugKrSdC8/d2llRZzilpV0d6iu8ZR51j8p9nm2SX+ePXt7F7GrVi7jzacflWs9yvpjTZkpxgA3bNjWuiELw+KV5ZnyVe+nbRtbXEahtLKKQmY5pwybt6z9Mut5rsln1JdBReKKG7cCXVIfFYmLpyJxIsOpSFwOVaXp3rJxR23PY965eyaxoFldheHyrH9S7PNsk/48Drg7O3fP5FqPsuJVdorxJWcd30vhn21X+u2weKWNQx37aduKJOYplJYllT1L+1nOKcPmLWu/zNKnpp9RLyIiIpKFnoM+QFnPNo62c86yQ0f2ecxp+pRm/ZPWI8/6tS0mRfpTZlya1sU+d0lV568y9teisrSr/UxERES6RCnuQ7RpDHreZbVB02PQ2xaTKh5z1bZ1TKOLfe6Sqs5fTfShSLtd38+U4l7cuKUXi3TFKA0RUHq9ZJX0+a4LdBERkRbTBXpxXfhyLzKOdIEu4yzp810p7iIiIiIiIiItoAt0ERERERERkRbQBbqIiIiIiIhIC+gCXURERERERKQFdIEuIiIiIiIi0gJTTXdARERERETGTxeqs5et7HUuu3p82v6NStX6Nq6v7qCLiIiIiIiItIAu0EVERERERERaQBfoIiIiIiIiIi2gC3QRERERERGRFtAFuoiIiABgZmeb2WYz22Jml8b8/mlmdn3w+3Vmdkzod5cF7282s9elbVNERESeogt0ERERwcwmgSuBc4ATgYvM7MTIZO8Adrr78cBHgSuCeU8ELgSWAmcDf2lmkynbFBERkYAu0EVERATgNGCLu2919xngOmBlZJqVwOeCn1cDrzIzC96/zt33uPuDwJagvTRtioiISKDTz0Ffv379T83s4RKbfA7w0xLbk6cottVRbKuhuFZHsc3m6JqWczjwaOj1NuD0pGncfZ+ZPQEcHLy/NjLv4cHPw9oEwMwuBi4OXv7CzDbnWIe2076/kGKykGKyUOtjYlfUvsjnAD9tYLlAI+s7TKX7SEXrG/v53ukLdHc/pMz2zOxud19eZpvSo9hWR7GthuJaHcW2tSzmPU85TdL7cZl60TZ7b7pfBVw1qINdp31/IcVkIcVkIcVkIcVkvlGKh1LcRUREBHp3t48MvT4C2J40jZlNAQcCjw+YN02bIiIiEtAFuoiIiADcBZxgZsea2TS9om9rItOsAd4S/Hwe8DV39+D9C4Mq78cCJwB3pmxTREREAp1Oca/ASKfWNUyxrY5iWw3FtTqKbQsFY8rfA9wKTAKfcfdNZrYKuNvd1wCfBj5vZlvo3Tm/MJh3k5l9Abgf2Adc4u6zAHFt1r1uLaJ9fyHFZCHFZCHFZCHFZL6RiYf1/vAtIiIiIiIiIk1SiruIiIiIiIhIC+gCXURERERERKQFdIEeMLOzzWyzmW0xs0ub7k/XmNlnzOzHZrYx9N6zzew2M/tB8P+S4H0zs48Hsf6umZ3SXM/bzcyONLM7zOwBM9tkZr8XvK/YFmRmTzezO83s3iC2fxS8f6yZrQtie31Q2Iqg+NX1QWzXmdkxTfa/7cxs0sy+Y2Y3B68VVxkrWc/f4yLtuWFcmNlBZrbazL4X7Csv0z5i7w2OmY1mdm3weT1W+4m+Vy+UEJM/C46d75rZTWZ2UOh3lwUx2Wxmr2um1/noAp3ehwVwJXAOcCJwkZmd2GyvOuezwNmR9y4Fbnf3E4Dbg9fQi/MJwb+Lgb+qqY9dtA94n7u/CFgBXBLsm4ptcXuAV7r7S4GTgLPNbAVwBfDRILY7gXcE078D2OnuxwMfDaaTZL8HPBB6rbjKuMl6/h4Xac8N4+JjwJfd/ZeBl9KLzdjuI2Z2OPC7wHJ3X0avuOSFjN9+8ln0vTrqsyyMyW3AMnd/CfB94DKA4Fx7IbA0mOcvg+u9TtAFes9pwBZ33+ruM8B1wMqG+9Qp7v4NehV9w1YCnwt+/hzwb0Lv/633rAUOMrND6+lpt7j7DnffEPz8c3of3Iej2BYWxOgXwctFwT8HXgmsDt6PxrYf89XAq8zMaupup5jZEcCvAZ8KXhuKq4yZHOfvkZfx3DDyzOyXgFfQezoC7j7j7rsY430kMAUcYGZTwGJgB2O2n+h79UJxMXH3r7j7vuDlWuCI4OeVwHXuvsfdHwS20Lve6wRdoPccDjwaer0teE+KeZ6774DeFxXgucH7incOQervycA6FNtSBKmW9wA/pvdX2H8EdoVO9uH47Y9t8PsngIPr7XFn/A/gPwNzweuDUVxljKU8f4+DLOeGcXAc8BPgfwZp/58ys2cwxvuIu/8Q+AjwCL0L8yeA9Yz3ftKn736DvR24Jfi50zHRBXpP3N0aPX+uOop3Rmb2TOAG4Pfd/WeDJo15T7FN4O6z7n4Svb+4nga8KG6y4H/FNgUzOxf4sbuvD78dM6niKmMhw/l7pOU4N4yDKeAU4K/c/WTgnxmjdPY4wbjqlcCxwGHAM+ilcEeN034yzLgfR5jZB+kNK7q6/1bMZJ2JiS7Qe7YBR4ZeHwFsb6gvo+RH/RSb4P8fB+8r3hmY2SJ6X+6udvcbg7cV2xIFKYVfpzdO9KAgrQ7mx29/bIPfH8jC9DOBM4A3mNlD9IYLvZLeXTPFVcZOxvP3qMt6bhgH24Bt7r4ueL2a3gX7uO4jAK8GHnT3n7j7XuBG4OWM937Sp+9+MczsLcC5wG+5e/8ivNMx0QV6z13ACUGFyGl6RQXWNNynUbAGeEvw81uAL4Xe//dB1ckVwBP9lB2ZLxif92ngAXf/89CvFNuCzOyQfrVPMzuA3peCB4A7gPOCyaKx7cf8POBroQ8CCbj7Ze5+hLsfQ+9c+jV3/y0UVxkzOc7fIy3HuWHkufs/AY+a2QuDt14F3M+Y7iOBR4AVZrY4OIb6MRnb/SRE3/0izOxs4A+AN7j77tCv1gAXWu9JMcfSK6B3ZxN9zMP0PajHzF5P7y+5k8Bn3P3DDXepU8zsWuBM4DnAj4A/BL4IfAE4it4J93x3fzw44X6CXlXF3cDb3P3uJvrddmb2K8A3gft4aszeB+iNY1RsCzCzl9ArsjJJ74+VX3D3VWZ2HL27O88GvgP8W3ffY2ZPBz5Pbxzp48CF7r61md53g5mdCbzf3c9VXGXcZD1/N9LJhqQ5NzTZvzqZ2Un0iuZNA1uBtxF8JjGm+4j1Hnt6Ab2U5e8A76Q3fnhs9hN9r14oISaXAU8DHgsmW+vu7w6m/yC9cen76A0xuiXaZlvpAl1ERERERESkBZTiLiIiIiIiItICukAXERERERERaQFdoIuIiIiIiIi0gC7QRURERERERFpAF+giIiIiIiIiLaALdJERZmYnBY8QbGLZh5nZ6ozzvNXMPlFVn0RERLqs4c/1Y8xsYwXtnmlmLw+9/qyZnTdoHpFRpgt0kdF2EtDIB7m7b3d3fcCKiIiUp7HP9QqdCbx82EQi40IX6CItZGbPMLP/a2b3mtlGM7sgeP9UM/t7M1tvZrea2aHB+183syvM7E4z+76Z/aqZTQOrgAvM7B4zuyBo9zNmdpeZfcfMVgbzv9XMbjSzL5vZD8zsv4f6craZbQj6cnuofwvaiazD/r8dzcrnAAAEAUlEQVS0D2n/bUGf/x44I/T+IWZ2Q7CMu8zsjOD9j5vZ5cHPrzOzb5iZzmUiItJao/C5HlmfSTP7s2D675rZfwjePzPo+2oz+56ZXW1mFvzu9cF73wo+y282s2OAdwPvDdbpV4NFvMLM/p+ZbdXddBk3U013QERinQ1sd/dfAzCzA81sEfAXwEp3/0nw4f5h4O3BPFPufpr1Ut/+0N1fHVzILnf39wTt/Ffga+7+djM7CLjTzL4azH8ScDKwB9hsZn8B/AvwN8Ar3P1BM3t2MO0H49px938esE5x7e8D/gg4FXgCuAP4TjD9x4CPuvu3zOwo4FbgRcClwF1m9k3g48Dr3X0ua4BFRERqNGqf6+8AnnD3f2VmTwP+wcy+EvzuZGApsB34B+AMM7sb+GRoudcCuPtDZvbXwC/c/SPBOr0DOBT4FeCXgTVApiFzIl2mC3SRdroP+IiZXQHc7O7fNLNlwDLgtuCP0ZPAjtA8Nwb/rweOSWj3tcAbzOz9weunA0cFP9/u7k8AmNn9wNHAEuAb7v4ggLs/PqSdBwasU1z7zwG+7u4/Cd6/HnhBMP2rgRODdQX4JTN7lrv/3MzeBXwDeK+7/+OAZYqIiLTBqH2uvxZ4Seju9oHACcAMcKe7bwuWe0/Q918AW/vLBa4FLk5oG+CLwR/f7zez5w2YTmTk6AJdpIXc/ftmdiq9cWb/Lfir9E3AJnd/WcJse4L/Z0k+tg14k7tvnvem2emh+cNtGOBp2xkirn0S2ofeEJyXufuTMb97MfAYcFiG5YuIiDRiBD/XDfgdd781stwzByw3i3AbWecV6TSN2xRpITM7DNjt7v8L+AhwCrAZOMTMXhZMs8jMlg5p6ufAs0KvbwV+JzQe7OQh838b+NdmdmwwfT8VLms7SdYBZ5rZwUGq3/mh330FeE//hZmdFPx/NPA+eil05wRfQkRERFprBD/XbwX+Y/DZjZm9wMyeMWD67wHHBWPOAS4YsE4iY00X6CLt9GJ647/uoTcu7E/cfQY4D7jCzO4F7mF41dM76KWJ3xOMbftjYBHwXesVcPvjQTMHqecXAzcGy7w++FWmdga0vwP4EL0vDF8FNoR+/bvA8qD4zP3Au4MvDp8G3u/u2+mNgfuUmT09z/JFRERqMmqf658C7gc2BNN/kgGZuUE23G8DXzazbwE/old7BuD/AL8RKRInMrbMPSm7VEREREREpDgze6a7/yL4Y/uVwA/c/aNN90ukbXQHXUREREREqvauIINgE72icp9suD8iraQ76CIiIiIiIiItoDvoIiIiIiIiIi2gC3QRERERERGRFtAFuoiIiIiIiEgL6AJdREREREREpAV0gS4iIiIiIiLSAv8f6FOsh0QlgaEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset visualization\n",
    "lengths = [ len(sentence)  for sentence in dataset ]\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14,6))\n",
    "axs.ravel()\n",
    "axs[0].set_title('Sentence length values')\n",
    "axs[0].set_xlabel('sentence index')\n",
    "axs[0].set_ylabel('sentence length')\n",
    "_ = axs[0].plot(lengths, '.')\n",
    "axs[1].set_title('Sentence length distribution')\n",
    "axs[1].set_xlabel('sentence length')\n",
    "axs[1].set_ylabel('length probability')\n",
    "_ = axs[1].hist(lengths, bins=40, density=True)\n",
    "\n",
    "print('Mean sentence length:', np.mean(lengths))\n",
    "print('Max sentence length:', np.max(lengths))\n",
    "print('Min sentence length:', np.min(lengths))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(images_path+f'/md_{min_len}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2986\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding dimension\n",
    "embedding_dim = 50\n",
    "\n",
    "# Copy dataset words\n",
    "words = set(dataset.words.copy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time \n",
    "\n",
    "# Load embeddings from glove\n",
    "glove = from_file(path='data/glove.6B.50d.txt',\n",
    "                  words=words\n",
    "                 )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Analyse glove embeddings\n",
    "print('Mean:', np.mean(list(glove.values())), ', STD: ', np.std(list(glove.values())) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define embeddings for unknown words by sampling from normal distirbution, using found parameters\n",
    "mean = np.mean(list(glove.values()))\n",
    "std = np.std(list(glove.values())) \n",
    "\n",
    "# Initialize randomly sampled embeddings\n",
    "embeddings = gaussian_sampling(mean, std, dim=embedding_dim, words=words)\n",
    "# Loop through each embedded word\n",
    "for word, vector in embeddings.items():\n",
    "    # Subsititute current entry with glove one, if available\n",
    "    embeddings[word] = glove.get(word, vector)\n",
    "\n",
    "# Get list of words\n",
    "words = [*embeddings.keys()]\n",
    "# Get vectors as float tensor\n",
    "vectors = torch.tensor([*embeddings.values()], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Analyse generated embeddings\n",
    "print('Mean:', np.mean(list(embeddings.values())), ', STD: ', np.std(list(embeddings.values())) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = vectors.shape[0] # obtained with all words in vocab and in glove + words in vocab not in glove\n",
    "b = len(set(glove.keys()) & set(dataset.words)) + len(set(dataset.words) - set(glove.keys())) \n",
    "# sanity check\n",
    "a == b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('./data/embeddings', 'wb') as f:\n",
    "    pickle.dump(embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "with open('./data/embeddings', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "# Get list of words\n",
    "words = [*embeddings.keys()]\n",
    "# Get vectors as float tensor\n",
    "vectors = torch.tensor([*embeddings.values()], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_len = 10\n",
    "\n",
    "# Define transformation \n",
    "dataset.transform = transforms.Compose([\n",
    "    RandomCrop(crop_len),\n",
    "    WordToIndex(words),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defined the transform, this call returns the word index (after random cropping)\n",
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZNAdwzKNBkr"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        # network\n",
    "        'hidden_units' : 2**9, # symmetric layers\n",
    "        'layers_num' : 2,\n",
    "        'dropout_prob' : 0.3,\n",
    "        'architecture' : 'GRU',\n",
    "\n",
    "        # training\n",
    "        'batch_size' : 80,\n",
    "        'num_epochs' : 1000,\n",
    "\n",
    "        # optimizer\n",
    "        'lr' : 1e-2,\n",
    "        'wd' : 1e-3,\n",
    "    \n",
    "        # dataset\n",
    "        'crop_len' : crop_len,\n",
    "        'min_len' : min_len\n",
    "        }\n",
    "\n",
    "# save\n",
    "out_dir = '{}_{}_{}_{}_{}'.format(params['architecture'], params['crop_len'], params['min_len'], params['lr'],params['wd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split initial dataset in train dataset and test dataset\n",
    "train_dataset, test_dataset = split_train_test(dataset, 0.9)\n",
    "# Make train dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "# Make test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oc8u0owgNBk2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (rnn): GRU(50, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (out): Linear(in_features=512, out_features=50, bias=True)\n",
       "  (embed): Embedding(2998, 50)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: hyperparameter optimization, this part must be included in the GridSearch iteration\n",
    "\n",
    "#%% Initialize network\n",
    "net = Network(vocab_size = vectors.shape[0],\n",
    "              embedding_dim = embedding_dim,\n",
    "              hidden_units = params['hidden_units'],\n",
    "              layers_num = params['layers_num'],\n",
    "              hidden_type = params['architecture'],\n",
    "              trained_embeddings = vectors,\n",
    "              freeze_embeddings = False,\n",
    "              dropout_prob = params['dropout_prob']\n",
    "             )\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=params['lr'], weight_decay=params['wd'])\n",
    "# Define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "colab_type": "code",
    "id": "1ChGVuPFNBk5",
    "outputId": "49d215e0-67b4-4022-f23b-69bbca522b94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " EPOCH 1/1000\n",
      "\t Training loss (epoch - mean):  2.9606871555248895\n",
      "\t Validation loss (epoch - mean): 0.9468319771137643\n",
      "\n",
      "\n",
      " EPOCH 2/1000\n",
      "\t Training loss (epoch - mean):  0.6670250544945399\n",
      "\t Validation loss (epoch - mean): 0.4282228582082911\n",
      "\n",
      "\n",
      " EPOCH 3/1000\n",
      "\t Training loss (epoch - mean):  0.3389662404855092\n",
      "\t Validation loss (epoch - mean): 0.31637389735972626\n",
      "\n",
      "\n",
      " EPOCH 4/1000\n",
      "\t Training loss (epoch - mean):  0.25268485645453137\n",
      "\t Validation loss (epoch - mean): 0.23679912962177965\n",
      "\n",
      "\n",
      " EPOCH 5/1000\n",
      "\t Training loss (epoch - mean):  0.20766486724217734\n",
      "\t Validation loss (epoch - mean): 0.21024132615074198\n",
      "\n",
      "\n",
      " EPOCH 6/1000\n",
      "\t Training loss (epoch - mean):  0.18313078582286835\n",
      "\t Validation loss (epoch - mean): 0.19108848931624534\n",
      "\n",
      "\n",
      " EPOCH 7/1000\n",
      "\t Training loss (epoch - mean):  0.16882625967264175\n",
      "\t Validation loss (epoch - mean): 0.16747795830064632\n",
      "\n",
      "\n",
      " EPOCH 8/1000\n",
      "\t Training loss (epoch - mean):  0.15600261837244034\n",
      "\t Validation loss (epoch - mean): 0.16039612104601048\n",
      "\n",
      "\n",
      " EPOCH 9/1000\n",
      "\t Training loss (epoch - mean):  0.13423623144626617\n",
      "\t Validation loss (epoch - mean): 0.14687357541728527\n",
      "\n",
      "\n",
      " EPOCH 10/1000\n",
      "\t Training loss (epoch - mean):  0.12744885310530663\n",
      "\t Validation loss (epoch - mean): 0.11084455410216718\n",
      "\n",
      "\n",
      " EPOCH 11/1000\n",
      "\t Training loss (epoch - mean):  0.11488851656516393\n",
      "\t Validation loss (epoch - mean): 0.10027995126995634\n",
      "\n",
      "\n",
      " EPOCH 12/1000\n",
      "\t Training loss (epoch - mean):  0.11682767296830814\n",
      "\t Validation loss (epoch - mean): 0.1020649902205518\n",
      "\n",
      "\n",
      " EPOCH 13/1000\n",
      "\t Training loss (epoch - mean):  0.10022744784752528\n",
      "\t Validation loss (epoch - mean): 0.10861988452837822\n",
      "\n",
      "\n",
      " EPOCH 14/1000\n",
      "\t Training loss (epoch - mean):  0.0947781354188919\n",
      "\t Validation loss (epoch - mean): 0.11368529633321661\n",
      "\n",
      "\n",
      " EPOCH 15/1000\n",
      "\t Training loss (epoch - mean):  0.09861346830924352\n",
      "\t Validation loss (epoch - mean): 0.08073167605920041\n",
      "\n",
      "\n",
      " EPOCH 16/1000\n",
      "\t Training loss (epoch - mean):  0.08416269719600677\n",
      "\t Validation loss (epoch - mean): 0.10784063567506506\n",
      "\n",
      "\n",
      " EPOCH 17/1000\n",
      "\t Training loss (epoch - mean):  0.08371509611606598\n",
      "\t Validation loss (epoch - mean): 0.07159309576642006\n",
      "\n",
      "\n",
      " EPOCH 18/1000\n",
      "\t Training loss (epoch - mean):  0.07524977003534634\n",
      "\t Validation loss (epoch - mean): 0.07251096574312195\n",
      "\n",
      "\n",
      " EPOCH 19/1000\n",
      "\t Training loss (epoch - mean):  0.06491114074985187\n",
      "\t Validation loss (epoch - mean): 0.07170222901758995\n",
      "\n",
      "\n",
      " EPOCH 20/1000\n",
      "\t Training loss (epoch - mean):  0.17187395070989928\n",
      "\t Validation loss (epoch - mean): 0.11323027288977136\n",
      "\n",
      "\n",
      " EPOCH 21/1000\n",
      "\t Training loss (epoch - mean):  0.10670241589347522\n",
      "\t Validation loss (epoch - mean): 0.09922606727861344\n",
      "\n",
      "\n",
      " EPOCH 22/1000\n",
      "\t Training loss (epoch - mean):  0.08214026192824046\n",
      "\t Validation loss (epoch - mean): 0.06889152289070982\n",
      "\n",
      "\n",
      " EPOCH 23/1000\n",
      "\t Training loss (epoch - mean):  0.06781076453626156\n",
      "\t Validation loss (epoch - mean): 0.07217054219639048\n",
      "\n",
      "\n",
      " EPOCH 24/1000\n",
      "\t Training loss (epoch - mean):  0.05837987425426642\n",
      "\t Validation loss (epoch - mean): 0.05700946797398811\n",
      "\n",
      "\n",
      " EPOCH 25/1000\n",
      "\t Training loss (epoch - mean):  0.06203132557372252\n",
      "\t Validation loss (epoch - mean): 0.08146975340044245\n",
      "\n",
      "\n",
      " EPOCH 26/1000\n",
      "\t Training loss (epoch - mean):  0.07592357136309147\n",
      "\t Validation loss (epoch - mean): 0.07671227789622673\n",
      "\n",
      "\n",
      " EPOCH 27/1000\n",
      "\t Training loss (epoch - mean):  0.056369077414274216\n",
      "\t Validation loss (epoch - mean): 0.05100482789442894\n",
      "\n",
      "\n",
      " EPOCH 28/1000\n",
      "\t Training loss (epoch - mean):  0.054920426880319916\n",
      "\t Validation loss (epoch - mean): 0.058214137845851006\n",
      "\n",
      "\n",
      " EPOCH 29/1000\n",
      "\t Training loss (epoch - mean):  0.049949015801151596\n",
      "\t Validation loss (epoch - mean): 0.039289890758772476\n",
      "\n",
      "\n",
      " EPOCH 30/1000\n",
      "\t Training loss (epoch - mean):  0.045640273640553154\n",
      "\t Validation loss (epoch - mean): 0.04062898472902623\n",
      "\n",
      "\n",
      " EPOCH 31/1000\n",
      "\t Training loss (epoch - mean):  0.04506135111053785\n",
      "\t Validation loss (epoch - mean): 0.042535791094315815\n",
      "\n",
      "\n",
      " EPOCH 32/1000\n",
      "\t Training loss (epoch - mean):  0.04047417640686035\n",
      "\t Validation loss (epoch - mean): 0.050552567348201224\n",
      "\n",
      "\n",
      " EPOCH 33/1000\n",
      "\t Training loss (epoch - mean):  0.04859941949446996\n",
      "\t Validation loss (epoch - mean): 0.04308011656270382\n",
      "\n",
      "\n",
      " EPOCH 34/1000\n",
      "\t Training loss (epoch - mean):  0.04096350011726221\n",
      "\t Validation loss (epoch - mean): 0.040646948594045135\n",
      "\n",
      "\n",
      " EPOCH 35/1000\n",
      "\t Training loss (epoch - mean):  0.03954570492108663\n",
      "\t Validation loss (epoch - mean): 0.100415355268311\n",
      "\n",
      "\n",
      " EPOCH 36/1000\n",
      "\t Training loss (epoch - mean):  0.08439802130063374\n",
      "\t Validation loss (epoch - mean): 0.08235424891748327\n",
      "\n",
      "\n",
      " EPOCH 37/1000\n",
      "\t Training loss (epoch - mean):  0.05408151944478353\n",
      "\t Validation loss (epoch - mean): 0.059243748083393624\n",
      "\n",
      "\n",
      " EPOCH 38/1000\n",
      "\t Training loss (epoch - mean):  0.043214588736494385\n",
      "\t Validation loss (epoch - mean): 0.04218207480029223\n",
      "\n",
      "\n",
      " EPOCH 39/1000\n",
      "\t Training loss (epoch - mean):  0.039203417797883354\n",
      "\t Validation loss (epoch - mean): 0.04428338035544817\n",
      "\n",
      "\n",
      " EPOCH 40/1000\n",
      "\t Training loss (epoch - mean):  0.11413676602145036\n",
      "\t Validation loss (epoch - mean): 0.7794903427996533\n",
      "\n",
      "\n",
      " EPOCH 41/1000\n",
      "\t Training loss (epoch - mean):  0.3061381845424573\n",
      "\t Validation loss (epoch - mean): 0.1797544537706578\n",
      "\n",
      "\n",
      " EPOCH 42/1000\n",
      "\t Training loss (epoch - mean):  0.1325544702510039\n",
      "\t Validation loss (epoch - mean): 0.09154122560582262\n",
      "\n",
      "\n",
      " EPOCH 43/1000\n",
      "\t Training loss (epoch - mean):  0.07354515045881271\n",
      "\t Validation loss (epoch - mean): 0.06112028658390045\n",
      "\n",
      "\n",
      " EPOCH 44/1000\n",
      "\t Training loss (epoch - mean):  0.05027175756792227\n",
      "\t Validation loss (epoch - mean): 0.03869938553172223\n",
      "\n",
      "\n",
      " EPOCH 45/1000\n",
      "\t Training loss (epoch - mean):  0.20274246235688528\n",
      "\t Validation loss (epoch - mean): 0.8198403576587109\n",
      "\n",
      "\n",
      " EPOCH 46/1000\n",
      "\t Training loss (epoch - mean):  0.3969602460662524\n",
      "\t Validation loss (epoch - mean): 0.16501629289160383\n",
      "\n",
      "\n",
      " EPOCH 47/1000\n",
      "\t Training loss (epoch - mean):  0.19284074008464813\n",
      "\t Validation loss (epoch - mean): 0.08745775847358907\n",
      "\n",
      "\n",
      " EPOCH 48/1000\n",
      "\t Training loss (epoch - mean):  0.1035268430908521\n",
      "\t Validation loss (epoch - mean): 0.07936514137273139\n",
      "\n",
      "\n",
      " EPOCH 49/1000\n",
      "\t Training loss (epoch - mean):  0.06323714554309845\n",
      "\t Validation loss (epoch - mean): 0.03741855982770311\n",
      "\n",
      "\n",
      " EPOCH 50/1000\n",
      "\t Training loss (epoch - mean):  0.040440824503699936\n",
      "\t Validation loss (epoch - mean): 0.03465676283899774\n",
      "\n",
      "\n",
      " EPOCH 51/1000\n",
      "\t Training loss (epoch - mean):  0.04817750584334135\n",
      "\t Validation loss (epoch - mean): 0.04421076146846122\n",
      "\n",
      "\n",
      " EPOCH 52/1000\n",
      "\t Training loss (epoch - mean):  0.03389897538969914\n",
      "\t Validation loss (epoch - mean): 0.03586694207835071\n",
      "\n",
      "\n",
      " EPOCH 53/1000\n",
      "\t Training loss (epoch - mean):  0.02788136185457309\n",
      "\t Validation loss (epoch - mean): 0.029689067112717855\n",
      "\n",
      "\n",
      " EPOCH 54/1000\n",
      "\t Training loss (epoch - mean):  0.026843936803440254\n",
      "\t Validation loss (epoch - mean): 0.08491589382607886\n",
      "\n",
      "\n",
      " EPOCH 55/1000\n",
      "\t Training loss (epoch - mean):  0.040118029030660786\n",
      "\t Validation loss (epoch - mean): 0.03339351446149831\n",
      "\n",
      "\n",
      " EPOCH 56/1000\n",
      "\t Training loss (epoch - mean):  0.030795312176148098\n",
      "\t Validation loss (epoch - mean): 0.03235059733344044\n",
      "\n",
      "\n",
      " EPOCH 57/1000\n",
      "\t Training loss (epoch - mean):  0.029507176019251347\n",
      "\t Validation loss (epoch - mean): 0.02229180827023501\n",
      "\n",
      "\n",
      " EPOCH 58/1000\n",
      "\t Training loss (epoch - mean):  0.019372299313545227\n",
      "\t Validation loss (epoch - mean): 0.017730144060593336\n",
      "\n",
      "\n",
      " EPOCH 59/1000\n",
      "\t Training loss (epoch - mean):  0.02984706995387872\n",
      "\t Validation loss (epoch - mean): 0.0280530965351995\n",
      "\n",
      "\n",
      " EPOCH 60/1000\n",
      "\t Training loss (epoch - mean):  0.022255782037973404\n",
      "\t Validation loss (epoch - mean): 0.023130844872286345\n",
      "\n",
      "\n",
      " EPOCH 61/1000\n",
      "\t Training loss (epoch - mean):  0.018735559346775215\n",
      "\t Validation loss (epoch - mean): 0.027314959450605067\n",
      "\n",
      "\n",
      " EPOCH 62/1000\n",
      "\t Training loss (epoch - mean):  0.023981221330662567\n",
      "\t Validation loss (epoch - mean): 0.021481166137976848\n",
      "\n",
      "\n",
      " EPOCH 63/1000\n",
      "\t Training loss (epoch - mean):  0.020119980288048584\n",
      "\t Validation loss (epoch - mean): 0.0196419549769068\n",
      "\n",
      "\n",
      " EPOCH 64/1000\n",
      "\t Training loss (epoch - mean):  0.013456313172355294\n",
      "\t Validation loss (epoch - mean): 0.01657484042299713\n",
      "\n",
      "\n",
      " EPOCH 65/1000\n",
      "\t Training loss (epoch - mean):  0.03660751755038897\n",
      "\t Validation loss (epoch - mean): 0.04832982049977526\n",
      "\n",
      "\n",
      " EPOCH 66/1000\n",
      "\t Training loss (epoch - mean):  0.027786691207438707\n",
      "\t Validation loss (epoch - mean): 0.016216127598222267\n",
      "\n",
      "\n",
      " EPOCH 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (epoch - mean):  0.0193220735527575\n",
      "\t Validation loss (epoch - mean): 0.01663304848517192\n",
      "\n",
      "\n",
      " EPOCH 68/1000\n",
      "\t Training loss (epoch - mean):  0.015862147323787212\n",
      "\t Validation loss (epoch - mean): 0.018795486301817793\n",
      "\n",
      "\n",
      " EPOCH 69/1000\n",
      "\t Training loss (epoch - mean):  0.01524507331972321\n",
      "\t Validation loss (epoch - mean): 0.011632358348195223\n",
      "\n",
      "\n",
      " EPOCH 70/1000\n",
      "\t Training loss (epoch - mean):  0.012548403659214577\n",
      "\t Validation loss (epoch - mean): 0.02141856875746174\n",
      "\n",
      "\n",
      " EPOCH 71/1000\n",
      "\t Training loss (epoch - mean):  0.08024212097128232\n",
      "\t Validation loss (epoch - mean): 0.06928133385929655\n",
      "\n",
      "\n",
      " EPOCH 72/1000\n",
      "\t Training loss (epoch - mean):  0.038266551991303764\n",
      "\t Validation loss (epoch - mean): 0.018319872128678128\n",
      "\n",
      "\n",
      " EPOCH 73/1000\n",
      "\t Training loss (epoch - mean):  0.021373160183429718\n",
      "\t Validation loss (epoch - mean): 0.018907380944236794\n",
      "\n",
      "\n",
      " EPOCH 74/1000\n",
      "\t Training loss (epoch - mean):  0.014932372607290745\n",
      "\t Validation loss (epoch - mean): 0.01565089177815838\n",
      "\n",
      "\n",
      " EPOCH 75/1000\n",
      "\t Training loss (epoch - mean):  0.015201552460590998\n",
      "\t Validation loss (epoch - mean): 0.02070413233633054\n",
      "\n",
      "\n",
      " EPOCH 76/1000\n",
      "\t Training loss (epoch - mean):  0.019460507047673065\n",
      "\t Validation loss (epoch - mean): 0.011208746418792834\n",
      "\n",
      "\n",
      " EPOCH 77/1000\n",
      "\t Training loss (epoch - mean):  0.011027550480018059\n",
      "\t Validation loss (epoch - mean): 0.015457962104614744\n",
      "\n",
      "\n",
      " EPOCH 78/1000\n",
      "\t Training loss (epoch - mean):  0.012694084085524082\n",
      "\t Validation loss (epoch - mean): 0.009170735938514167\n",
      "\n",
      "\n",
      " EPOCH 79/1000\n",
      "\t Training loss (epoch - mean):  0.04723071454403301\n",
      "\t Validation loss (epoch - mean): 0.03259516495497937\n",
      "\n",
      "\n",
      " EPOCH 80/1000\n",
      "\t Training loss (epoch - mean):  0.0347252848247687\n",
      "\t Validation loss (epoch - mean): 0.022923991024652694\n",
      "\n",
      "\n",
      " EPOCH 81/1000\n",
      "\t Training loss (epoch - mean):  0.019073754238585632\n",
      "\t Validation loss (epoch - mean): 0.01628372930188445\n",
      "\n",
      "\n",
      " EPOCH 82/1000\n",
      "\t Training loss (epoch - mean):  0.013310054472337166\n",
      "\t Validation loss (epoch - mean): 0.010766950237782711\n",
      "\n",
      "\n",
      " EPOCH 83/1000\n",
      "\t Training loss (epoch - mean):  0.009362473152577877\n",
      "\t Validation loss (epoch - mean): 0.008169168190277637\n",
      "\n",
      "\n",
      " EPOCH 84/1000\n",
      "\t Training loss (epoch - mean):  0.007968907399723927\n",
      "\t Validation loss (epoch - mean): 0.009166002067975066\n",
      "\n",
      "\n",
      " EPOCH 85/1000\n",
      "\t Training loss (epoch - mean):  0.06925606556857626\n",
      "\t Validation loss (epoch - mean): 0.04965963048186708\n",
      "\n",
      "\n",
      " EPOCH 86/1000\n",
      "\t Training loss (epoch - mean):  0.04344765841960907\n",
      "\t Validation loss (epoch - mean): 0.02451307326555252\n",
      "\n",
      "\n",
      " EPOCH 87/1000\n",
      "\t Training loss (epoch - mean):  0.020836625558634598\n",
      "\t Validation loss (epoch - mean): 0.016111102440652062\n",
      "\n",
      "\n",
      " EPOCH 88/1000\n",
      "\t Training loss (epoch - mean):  0.013069490902125835\n",
      "\t Validation loss (epoch - mean): 0.009950322168383827\n",
      "\n",
      "\n",
      " EPOCH 89/1000\n",
      "\t Training loss (epoch - mean):  0.009891302945713202\n",
      "\t Validation loss (epoch - mean): 0.011390671391594918\n",
      "\n",
      "\n",
      " EPOCH 90/1000\n",
      "\t Training loss (epoch - mean):  0.04262312160183986\n",
      "\t Validation loss (epoch - mean): 0.04754444124533775\n",
      "\n",
      "\n",
      " EPOCH 91/1000\n",
      "\t Training loss (epoch - mean):  0.025091875189294417\n",
      "\t Validation loss (epoch - mean): 0.008214464222893436\n",
      "\n",
      "\n",
      " EPOCH 92/1000\n",
      "\t Training loss (epoch - mean):  0.013309376314282417\n",
      "\t Validation loss (epoch - mean): 0.013653405902392052\n",
      "\n",
      "\n",
      " EPOCH 93/1000\n",
      "\t Training loss (epoch - mean):  0.010072585195302963\n",
      "\t Validation loss (epoch - mean): 0.01003688127991367\n",
      "\n",
      "\n",
      " EPOCH 94/1000\n",
      "\t Training loss (epoch - mean):  0.007413587610547741\n",
      "\t Validation loss (epoch - mean): 0.0060273853398146145\n",
      "\n",
      "\n",
      " EPOCH 95/1000\n",
      "\t Training loss (epoch - mean):  0.0072240995553632574\n",
      "\t Validation loss (epoch - mean): 0.013519578088233446\n",
      "\n",
      "\n",
      " EPOCH 96/1000\n",
      "\t Training loss (epoch - mean):  0.008034519540766874\n",
      "\t Validation loss (epoch - mean): 0.006545837220557509\n",
      "\n",
      "\n",
      " EPOCH 97/1000\n",
      "\t Training loss (epoch - mean):  0.005641231390958031\n",
      "\t Validation loss (epoch - mean): 0.005652960425767889\n",
      "\n",
      "\n",
      " EPOCH 98/1000\n",
      "\t Training loss (epoch - mean):  0.011431283239896098\n",
      "\t Validation loss (epoch - mean): 0.005861407341534629\n",
      "\n",
      "\n",
      " EPOCH 99/1000\n",
      "\t Training loss (epoch - mean):  0.005216004443354905\n",
      "\t Validation loss (epoch - mean): 0.0031346381614678243\n",
      "\n",
      "\n",
      " EPOCH 100/1000\n",
      "\t Training loss (epoch - mean):  0.004254829293737809\n",
      "\t Validation loss (epoch - mean): 0.005046405142762679\n",
      "\n",
      "\n",
      " EPOCH 101/1000\n",
      "\t Training loss (epoch - mean):  0.003437567657480637\n",
      "\t Validation loss (epoch - mean): 0.002953756787935569\n",
      "\n",
      "\n",
      " EPOCH 102/1000\n",
      "\t Training loss (epoch - mean):  0.003989818738773465\n",
      "\t Validation loss (epoch - mean): 0.003892127010522985\n",
      "\n",
      "\n",
      " EPOCH 103/1000\n",
      "\t Training loss (epoch - mean):  0.0031959232098112502\n",
      "\t Validation loss (epoch - mean): 0.0022730041514187954\n",
      "\n",
      "\n",
      " EPOCH 104/1000\n",
      "\t Training loss (epoch - mean):  0.0029976717196404934\n",
      "\t Validation loss (epoch - mean): 0.003478050036256776\n",
      "\n",
      "\n",
      " EPOCH 105/1000\n",
      "\t Training loss (epoch - mean):  0.002844322744446496\n",
      "\t Validation loss (epoch - mean): 0.0024191941821358185\n",
      "\n",
      "\n",
      " EPOCH 106/1000\n",
      "\t Training loss (epoch - mean):  0.0022854540729895234\n",
      "\t Validation loss (epoch - mean): 0.0027461656799788853\n",
      "\n",
      "\n",
      " EPOCH 107/1000\n",
      "\t Training loss (epoch - mean):  0.0023507173367155096\n",
      "\t Validation loss (epoch - mean): 0.002989691993052281\n",
      "\n",
      "\n",
      " EPOCH 108/1000\n",
      "\t Training loss (epoch - mean):  0.0023328084498643875\n",
      "\t Validation loss (epoch - mean): 0.002499799867361249\n",
      "\n",
      "\n",
      " EPOCH 109/1000\n",
      "\t Training loss (epoch - mean):  0.002538239195321997\n",
      "\t Validation loss (epoch - mean): 0.002631849237931942\n",
      "\n",
      "\n",
      " EPOCH 110/1000\n",
      "\t Training loss (epoch - mean):  0.0018310340819880366\n",
      "\t Validation loss (epoch - mean): 0.002198475974279411\n",
      "\n",
      "\n",
      " EPOCH 111/1000\n",
      "\t Training loss (epoch - mean):  0.002262594255929192\n",
      "\t Validation loss (epoch - mean): 0.0013618496180997487\n",
      "\n",
      "\n",
      " EPOCH 112/1000\n",
      "\t Training loss (epoch - mean):  0.0024605931248515844\n",
      "\t Validation loss (epoch - mean): 0.0016482140297042899\n",
      "\n",
      "\n",
      " EPOCH 113/1000\n",
      "\t Training loss (epoch - mean):  0.0018514272718069453\n",
      "\t Validation loss (epoch - mean): 0.001299957561020571\n",
      "\n",
      "\n",
      " EPOCH 114/1000\n",
      "\t Training loss (epoch - mean):  0.001804321480449289\n",
      "\t Validation loss (epoch - mean): 0.0016544508576152247\n",
      "\n",
      "\n",
      " EPOCH 115/1000\n",
      "\t Training loss (epoch - mean):  0.001912275853101164\n",
      "\t Validation loss (epoch - mean): 0.0011135634582736182\n",
      "\n",
      "\n",
      " EPOCH 116/1000\n",
      "\t Training loss (epoch - mean):  0.001886683360983928\n",
      "\t Validation loss (epoch - mean): 0.0023380379626611646\n",
      "\n",
      "\n",
      " EPOCH 117/1000\n",
      "\t Training loss (epoch - mean):  0.0019630748817386725\n",
      "\t Validation loss (epoch - mean): 0.002329537617736199\n",
      "\n",
      "\n",
      " EPOCH 118/1000\n",
      "\t Training loss (epoch - mean):  0.0015859588553818564\n",
      "\t Validation loss (epoch - mean): 0.0012568205363114682\n",
      "\n",
      "\n",
      " EPOCH 119/1000\n",
      "\t Training loss (epoch - mean):  0.0013539175076099734\n",
      "\t Validation loss (epoch - mean): 0.0012094803898938327\n",
      "\n",
      "\n",
      " EPOCH 120/1000\n",
      "\t Training loss (epoch - mean):  0.001429411214000235\n",
      "\t Validation loss (epoch - mean): 0.0013371783251522116\n",
      "\n",
      "\n",
      " EPOCH 121/1000\n",
      "\t Training loss (epoch - mean):  0.0012630333367269486\n",
      "\t Validation loss (epoch - mean): 0.0014075548408962934\n",
      "\n",
      "\n",
      " EPOCH 122/1000\n",
      "\t Training loss (epoch - mean):  0.0011602941473635535\n",
      "\t Validation loss (epoch - mean): 0.0006874171093009051\n",
      "\n",
      "\n",
      " EPOCH 123/1000\n",
      "\t Training loss (epoch - mean):  0.0016066453147990007\n",
      "\t Validation loss (epoch - mean): 0.0018268881867697928\n",
      "\n",
      "\n",
      " EPOCH 124/1000\n",
      "\t Training loss (epoch - mean):  0.0012036178183431427\n",
      "\t Validation loss (epoch - mean): 0.0015091745292417553\n",
      "\n",
      "\n",
      " EPOCH 125/1000\n",
      "\t Training loss (epoch - mean):  0.0011397788184694946\n",
      "\t Validation loss (epoch - mean): 0.0011032190482086314\n",
      "\n",
      "\n",
      " EPOCH 126/1000\n",
      "\t Training loss (epoch - mean):  0.0012108088509800534\n",
      "\t Validation loss (epoch - mean): 0.0011617325843427022\n",
      "\n",
      "\n",
      " EPOCH 127/1000\n",
      "\t Training loss (epoch - mean):  0.0012023387340983998\n",
      "\t Validation loss (epoch - mean): 0.0010476181383985911\n",
      "\n",
      "\n",
      " EPOCH 128/1000\n",
      "\t Training loss (epoch - mean):  0.001200998774341618\n",
      "\t Validation loss (epoch - mean): 0.0011464502358649042\n",
      "\n",
      "\n",
      " EPOCH 129/1000\n",
      "\t Training loss (epoch - mean):  0.0008079581214891126\n",
      "\t Validation loss (epoch - mean): 0.0010073195007088485\n",
      "\n",
      "\n",
      " EPOCH 130/1000\n",
      "\t Training loss (epoch - mean):  0.0009812971984501928\n",
      "\t Validation loss (epoch - mean): 0.0008829194746679005\n",
      "\n",
      "\n",
      " EPOCH 131/1000\n",
      "\t Training loss (epoch - mean):  0.0009506870701443404\n",
      "\t Validation loss (epoch - mean): 0.0008890219502218509\n",
      "\n",
      "\n",
      " EPOCH 132/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (epoch - mean):  0.0009179162152577192\n",
      "\t Validation loss (epoch - mean): 0.0012367240377453197\n",
      "\n",
      "\n",
      " EPOCH 133/1000\n",
      "\t Training loss (epoch - mean):  0.0008366680218993375\n",
      "\t Validation loss (epoch - mean): 0.0005841651636341765\n",
      "\n",
      "\n",
      " EPOCH 134/1000\n",
      "\t Training loss (epoch - mean):  0.0008173397412368407\n",
      "\t Validation loss (epoch - mean): 0.0005386359910546085\n",
      "\n",
      "\n",
      " EPOCH 135/1000\n",
      "\t Training loss (epoch - mean):  0.0007596941334971538\n",
      "\t Validation loss (epoch - mean): 0.0004438534982029446\n",
      "\n",
      "\n",
      " EPOCH 136/1000\n",
      "\t Training loss (epoch - mean):  0.0006809856665010253\n",
      "\t Validation loss (epoch - mean): 0.0007682421019319705\n",
      "\n",
      "\n",
      " EPOCH 137/1000\n",
      "\t Training loss (epoch - mean):  0.000742320844437927\n",
      "\t Validation loss (epoch - mean): 0.000841323606550759\n",
      "\n",
      "\n",
      " EPOCH 138/1000\n",
      "\t Training loss (epoch - mean):  0.0005645648003943885\n",
      "\t Validation loss (epoch - mean): 0.0005485700899140632\n",
      "\n",
      "\n",
      " EPOCH 139/1000\n",
      "\t Training loss (epoch - mean):  0.0006681706581730396\n",
      "\t Validation loss (epoch - mean): 0.0005444438713427789\n",
      "\n",
      "\n",
      " EPOCH 140/1000\n",
      "\t Training loss (epoch - mean):  0.0005337330367183313\n",
      "\t Validation loss (epoch - mean): 0.0005018200204273293\n",
      "\n",
      "\n",
      " EPOCH 141/1000\n",
      "\t Training loss (epoch - mean):  0.0005612489185295999\n",
      "\t Validation loss (epoch - mean): 0.0006093896850832104\n",
      "\n",
      "\n",
      " EPOCH 142/1000\n",
      "\t Training loss (epoch - mean):  0.0005040599110846719\n",
      "\t Validation loss (epoch - mean): 0.0006014828959257315\n",
      "\n",
      "\n",
      " EPOCH 143/1000\n",
      "\t Training loss (epoch - mean):  0.0005185033660382032\n",
      "\t Validation loss (epoch - mean): 0.0005326994894415564\n",
      "\n",
      "\n",
      " EPOCH 144/1000\n",
      "\t Training loss (epoch - mean):  0.0004837910576801126\n",
      "\t Validation loss (epoch - mean): 0.0008633746818382048\n",
      "\n",
      "\n",
      " EPOCH 145/1000\n",
      "\t Training loss (epoch - mean):  0.00043660099133073044\n",
      "\t Validation loss (epoch - mean): 0.0004502960275029646\n",
      "\n",
      "\n",
      " EPOCH 146/1000\n",
      "\t Training loss (epoch - mean):  0.00043276353972032666\n",
      "\t Validation loss (epoch - mean): 0.00027694558393027654\n",
      "\n",
      "\n",
      " EPOCH 147/1000\n",
      "\t Training loss (epoch - mean):  0.0004991705015224094\n",
      "\t Validation loss (epoch - mean): 0.0002279371518891415\n",
      "\n",
      "\n",
      " EPOCH 148/1000\n",
      "\t Training loss (epoch - mean):  0.000436632568986776\n",
      "\t Validation loss (epoch - mean): 0.0004170989557344891\n",
      "\n",
      "\n",
      " EPOCH 149/1000\n",
      "\t Training loss (epoch - mean):  0.0003664201867650263\n",
      "\t Validation loss (epoch - mean): 0.0002606700570946158\n",
      "\n",
      "\n",
      " EPOCH 150/1000\n",
      "\t Training loss (epoch - mean):  0.00040631866674326983\n",
      "\t Validation loss (epoch - mean): 0.0002611903947034931\n",
      "\n",
      "\n",
      " EPOCH 151/1000\n",
      "\t Training loss (epoch - mean):  0.0003899720274300004\n",
      "\t Validation loss (epoch - mean): 0.0004461903689181123\n",
      "\n",
      "\n",
      " EPOCH 152/1000\n",
      "\t Training loss (epoch - mean):  0.0002841377936420031\n",
      "\t Validation loss (epoch - mean): 0.000249354565698197\n",
      "\n",
      "\n",
      " EPOCH 153/1000\n",
      "\t Training loss (epoch - mean):  0.00031020505654547986\n",
      "\t Validation loss (epoch - mean): 0.00038818375498686557\n",
      "\n",
      "\n",
      " EPOCH 154/1000\n",
      "\t Training loss (epoch - mean):  0.0003012805003284787\n",
      "\t Validation loss (epoch - mean): 0.0002882565632847779\n",
      "\n",
      "\n",
      " EPOCH 155/1000\n",
      "\t Training loss (epoch - mean):  0.00036134618130745366\n",
      "\t Validation loss (epoch - mean): 0.0002266332996412607\n",
      "\n",
      "\n",
      " EPOCH 156/1000\n",
      "\t Training loss (epoch - mean):  0.00028327774634817615\n",
      "\t Validation loss (epoch - mean): 0.00023734150387641188\n",
      "\n",
      "\n",
      " EPOCH 157/1000\n",
      "\t Training loss (epoch - mean):  0.00023101491630465412\n",
      "\t Validation loss (epoch - mean): 0.0002724354537223646\n",
      "\n",
      "\n",
      " EPOCH 158/1000\n",
      "\t Training loss (epoch - mean):  0.00024773506690204766\n",
      "\t Validation loss (epoch - mean): 0.00018095909753926807\n",
      "\n",
      "\n",
      " EPOCH 159/1000\n",
      "\t Training loss (epoch - mean):  0.00021901843623102954\n",
      "\t Validation loss (epoch - mean): 0.00018800570909845306\n",
      "\n",
      "\n",
      " EPOCH 160/1000\n",
      "\t Training loss (epoch - mean):  0.00023066789678220326\n",
      "\t Validation loss (epoch - mean): 0.0002447713152196713\n",
      "\n",
      "\n",
      " EPOCH 161/1000\n",
      "\t Training loss (epoch - mean):  0.0002380656927319554\n",
      "\t Validation loss (epoch - mean): 0.0001378282241999996\n",
      "\n",
      "\n",
      " EPOCH 162/1000\n",
      "\t Training loss (epoch - mean):  0.00021009474221500568\n",
      "\t Validation loss (epoch - mean): 0.00021129964725671326\n",
      "\n",
      "\n",
      " EPOCH 163/1000\n",
      "\t Training loss (epoch - mean):  0.00020884980525200567\n",
      "\t Validation loss (epoch - mean): 0.00024409339772909277\n",
      "\n",
      "\n",
      " EPOCH 164/1000\n",
      "\t Training loss (epoch - mean):  0.0001656838636942363\n",
      "\t Validation loss (epoch - mean): 0.00012843884748322802\n",
      "\n",
      "\n",
      " EPOCH 165/1000\n",
      "\t Training loss (epoch - mean):  0.00015962249744916335\n",
      "\t Validation loss (epoch - mean): 0.00023042896605437953\n",
      "\n",
      "\n",
      " EPOCH 166/1000\n",
      "\t Training loss (epoch - mean):  0.00020031414169352502\n",
      "\t Validation loss (epoch - mean): 0.0001309552411443137\n",
      "\n",
      "\n",
      " EPOCH 167/1000\n",
      "\t Training loss (epoch - mean):  0.00015199932143635428\n",
      "\t Validation loss (epoch - mean): 0.00013465885187320344\n",
      "\n",
      "\n",
      " EPOCH 168/1000\n",
      "\t Training loss (epoch - mean):  0.0001410641828745914\n",
      "\t Validation loss (epoch - mean): 8.759236348631002e-05\n",
      "\n",
      "\n",
      " EPOCH 169/1000\n",
      "\t Training loss (epoch - mean):  0.0001521017196258375\n",
      "\t Validation loss (epoch - mean): 0.000147453264568088\n",
      "\n",
      "\n",
      " EPOCH 170/1000\n",
      "\t Training loss (epoch - mean):  0.00011432436076574959\n",
      "\t Validation loss (epoch - mean): 5.1997550933284654e-05\n",
      "\n",
      "\n",
      " EPOCH 171/1000\n",
      "\t Training loss (epoch - mean):  0.00011299913967377506\n",
      "\t Validation loss (epoch - mean): 8.113716413533361e-05\n",
      "\n",
      "\n",
      " EPOCH 172/1000\n",
      "\t Training loss (epoch - mean):  9.308993200344655e-05\n",
      "\t Validation loss (epoch - mean): 9.770209061253142e-05\n",
      "\n",
      "\n",
      " EPOCH 173/1000\n",
      "\t Training loss (epoch - mean):  9.383016610324073e-05\n",
      "\t Validation loss (epoch - mean): 9.029996142102962e-05\n",
      "\n",
      "\n",
      " EPOCH 174/1000\n",
      "\t Training loss (epoch - mean):  0.00010466099896196586\n",
      "\t Validation loss (epoch - mean): 0.00010101603609708501\n",
      "\n",
      "\n",
      " EPOCH 175/1000\n",
      "\t Training loss (epoch - mean):  9.587217573425733e-05\n",
      "\t Validation loss (epoch - mean): 5.7357299641459594e-05\n",
      "\n",
      "\n",
      " EPOCH 176/1000\n",
      "\t Training loss (epoch - mean):  8.971881955706824e-05\n",
      "\t Validation loss (epoch - mean): 7.36369005175162e-05\n",
      "\n",
      "\n",
      " EPOCH 177/1000\n",
      "\t Training loss (epoch - mean):  7.849937173887156e-05\n",
      "\t Validation loss (epoch - mean): 6.869987515877237e-05\n",
      "\n",
      "\n",
      " EPOCH 178/1000\n",
      "\t Training loss (epoch - mean):  0.00010435039742636339\n",
      "\t Validation loss (epoch - mean): 9.706647423423961e-05\n",
      "\n",
      "\n",
      " EPOCH 179/1000\n",
      "\t Training loss (epoch - mean):  8.281002131601174e-05\n",
      "\t Validation loss (epoch - mean): 7.146075365333314e-05\n",
      "\n",
      "\n",
      " EPOCH 180/1000\n",
      "\t Training loss (epoch - mean):  7.598768206662498e-05\n",
      "\t Validation loss (epoch - mean): 8.986422427231209e-05\n",
      "\n",
      "\n",
      " EPOCH 181/1000\n",
      "\t Training loss (epoch - mean):  7.325306614802685e-05\n",
      "\t Validation loss (epoch - mean): 7.105167956559456e-05\n",
      "\n",
      "\n",
      " EPOCH 182/1000\n",
      "\t Training loss (epoch - mean):  6.513268393367373e-05\n",
      "\t Validation loss (epoch - mean): 8.757913898555312e-05\n",
      "\n",
      "\n",
      " EPOCH 183/1000\n",
      "\t Training loss (epoch - mean):  5.3643184704318024e-05\n",
      "\t Validation loss (epoch - mean): 8.840610456189174e-05\n",
      "\n",
      "\n",
      " EPOCH 184/1000\n",
      "\t Training loss (epoch - mean):  6.993514216446783e-05\n",
      "\t Validation loss (epoch - mean): 6.488361413916054e-05\n",
      "\n",
      "\n",
      " EPOCH 185/1000\n",
      "\t Training loss (epoch - mean):  6.0622771343332715e-05\n",
      "\t Validation loss (epoch - mean): 5.73569905980021e-05\n",
      "\n",
      "\n",
      " EPOCH 186/1000\n",
      "\t Training loss (epoch - mean):  5.532324212254025e-05\n",
      "\t Validation loss (epoch - mean): 6.105698775144824e-05\n",
      "\n",
      "\n",
      " EPOCH 187/1000\n",
      "\t Training loss (epoch - mean):  4.856590991645741e-05\n",
      "\t Validation loss (epoch - mean): 2.8946230804407643e-05\n",
      "\n",
      "\n",
      " EPOCH 188/1000\n",
      "\t Training loss (epoch - mean):  5.350453890666055e-05\n",
      "\t Validation loss (epoch - mean): 4.094460771701758e-05\n",
      "\n",
      "\n",
      " EPOCH 189/1000\n",
      "\t Training loss (epoch - mean):  4.07639912130738e-05\n",
      "\t Validation loss (epoch - mean): 3.09261033316278e-05\n",
      "\n",
      "\n",
      " EPOCH 190/1000\n",
      "\t Training loss (epoch - mean):  3.649262392476279e-05\n",
      "\t Validation loss (epoch - mean): 4.747826518236339e-05\n",
      "\n",
      "\n",
      " EPOCH 191/1000\n",
      "\t Training loss (epoch - mean):  4.378804775721316e-05\n",
      "\t Validation loss (epoch - mean): 2.1229243959575624e-05\n",
      "\n",
      "\n",
      " EPOCH 192/1000\n",
      "\t Training loss (epoch - mean):  4.326178653476139e-05\n",
      "\t Validation loss (epoch - mean): 3.354229680889767e-05\n",
      "\n",
      "\n",
      " EPOCH 193/1000\n",
      "\t Training loss (epoch - mean):  3.021174870809773e-05\n",
      "\t Validation loss (epoch - mean): 2.8153846360655347e-05\n",
      "\n",
      "\n",
      " EPOCH 194/1000\n",
      "\t Training loss (epoch - mean):  3.3514960099031064e-05\n",
      "\t Validation loss (epoch - mean): 3.814425610109896e-05\n",
      "\n",
      "\n",
      " EPOCH 195/1000\n",
      "\t Training loss (epoch - mean):  3.589334513283878e-05\n",
      "\t Validation loss (epoch - mean): 2.6573472183902094e-05\n",
      "\n",
      "\n",
      " EPOCH 196/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (epoch - mean):  2.750665438118934e-05\n",
      "\t Validation loss (epoch - mean): 1.4307419221758418e-05\n",
      "\n",
      "\n",
      " EPOCH 197/1000\n",
      "\t Training loss (epoch - mean):  2.6516609674824092e-05\n",
      "\t Validation loss (epoch - mean): 3.2084140679836175e-05\n",
      "\n",
      "\n",
      " EPOCH 198/1000\n",
      "\t Training loss (epoch - mean):  2.583067407613271e-05\n",
      "\t Validation loss (epoch - mean): 2.7829167364304938e-05\n",
      "\n",
      "\n",
      " EPOCH 199/1000\n",
      "\t Training loss (epoch - mean):  2.418024208357868e-05\n",
      "\t Validation loss (epoch - mean): 2.792768204406631e-05\n",
      "\n",
      "\n",
      " EPOCH 200/1000\n",
      "\t Training loss (epoch - mean):  2.2394652660295833e-05\n",
      "\t Validation loss (epoch - mean): 2.0001885916480824e-05\n",
      "\n",
      "\n",
      " EPOCH 201/1000\n",
      "\t Training loss (epoch - mean):  2.0549467232437262e-05\n",
      "\t Validation loss (epoch - mean): 2.8814642394414195e-05\n",
      "\n",
      "\n",
      " EPOCH 202/1000\n",
      "\t Training loss (epoch - mean):  2.2176334975180605e-05\n",
      "\t Validation loss (epoch - mean): 2.3152628203392657e-05\n",
      "\n",
      "\n",
      " EPOCH 203/1000\n",
      "\t Training loss (epoch - mean):  1.965618654746019e-05\n",
      "\t Validation loss (epoch - mean): 1.131914409763353e-05\n",
      "\n",
      "\n",
      " EPOCH 204/1000\n",
      "\t Training loss (epoch - mean):  1.4834844629755631e-05\n",
      "\t Validation loss (epoch - mean): 1.5576596699539075e-05\n",
      "\n",
      "\n",
      " EPOCH 205/1000\n",
      "\t Training loss (epoch - mean):  1.6407982457167236e-05\n",
      "\t Validation loss (epoch - mean): 1.1747003456280665e-05\n",
      "\n",
      "\n",
      " EPOCH 206/1000\n",
      "\t Training loss (epoch - mean):  1.7422014328379493e-05\n",
      "\t Validation loss (epoch - mean): 1.4143412671347006e-05\n",
      "\n",
      "\n",
      " EPOCH 207/1000\n",
      "\t Training loss (epoch - mean):  1.4243118585000047e-05\n",
      "\t Validation loss (epoch - mean): 2.5067523351938984e-05\n",
      "\n",
      "\n",
      " EPOCH 208/1000\n",
      "\t Training loss (epoch - mean):  1.4415169819888737e-05\n",
      "\t Validation loss (epoch - mean): 5.588939356138607e-06\n",
      "\n",
      "\n",
      " EPOCH 209/1000\n",
      "\t Training loss (epoch - mean):  1.2185385003249394e-05\n",
      "\t Validation loss (epoch - mean): 1.133243169734437e-05\n",
      "\n",
      "\n",
      " EPOCH 210/1000\n",
      "\t Training loss (epoch - mean):  1.2034494375257054e-05\n",
      "\t Validation loss (epoch - mean): 1.3124417021350367e-05\n",
      "\n",
      "\n",
      " EPOCH 211/1000\n",
      "\t Training loss (epoch - mean):  9.242739603602482e-06\n",
      "\t Validation loss (epoch - mean): 1.239548019298659e-05\n",
      "\n",
      "\n",
      " EPOCH 212/1000\n",
      "\t Training loss (epoch - mean):  9.662829749383187e-06\n",
      "\t Validation loss (epoch - mean): 7.382650938203088e-06\n",
      "\n",
      "\n",
      " EPOCH 213/1000\n",
      "\t Training loss (epoch - mean):  7.411022086974602e-06\n",
      "\t Validation loss (epoch - mean): 7.1041228959275706e-06\n",
      "\n",
      "\n",
      " EPOCH 214/1000\n",
      "\t Training loss (epoch - mean):  1.0383322660345584e-05\n",
      "\t Validation loss (epoch - mean): 4.0870775079765325e-06\n",
      "\n",
      "\n",
      " EPOCH 215/1000\n",
      "\t Training loss (epoch - mean):  1.0516783277125796e-05\n",
      "\t Validation loss (epoch - mean): 3.735031151881293e-06\n",
      "\n",
      "\n",
      " EPOCH 216/1000\n",
      "\t Training loss (epoch - mean):  8.457880236771112e-06\n",
      "\t Validation loss (epoch - mean): 5.771602984994837e-06\n",
      "\n",
      "\n",
      " EPOCH 217/1000\n",
      "\t Training loss (epoch - mean):  6.896706887952557e-06\n",
      "\t Validation loss (epoch - mean): 7.222100906758708e-06\n",
      "\n",
      "\n",
      " EPOCH 218/1000\n",
      "\t Training loss (epoch - mean):  6.2799253252402805e-06\n",
      "\t Validation loss (epoch - mean): 5.121037502365645e-06\n",
      "\n",
      "\n",
      " EPOCH 219/1000\n",
      "\t Training loss (epoch - mean):  5.793617674498819e-06\n",
      "\t Validation loss (epoch - mean): 7.068416059404568e-06\n",
      "\n",
      "\n",
      " EPOCH 220/1000\n",
      "\t Training loss (epoch - mean):  6.899667596371728e-06\n",
      "\t Validation loss (epoch - mean): 8.442703179669096e-06\n",
      "\n",
      "\n",
      " EPOCH 221/1000\n",
      "\t Training loss (epoch - mean):  5.35558092451538e-06\n",
      "\t Validation loss (epoch - mean): 4.602875248306578e-06\n",
      "\n",
      "\n",
      " EPOCH 222/1000\n",
      "\t Training loss (epoch - mean):  4.840286010221462e-06\n",
      "\t Validation loss (epoch - mean): 4.324979894145725e-06\n",
      "\n",
      "\n",
      " EPOCH 223/1000\n",
      "\t Training loss (epoch - mean):  5.307532584690004e-06\n",
      "\t Validation loss (epoch - mean): 4.960460553937256e-06\n",
      "\n",
      "\n",
      " EPOCH 224/1000\n",
      "\t Training loss (epoch - mean):  4.146346327615902e-06\n",
      "\t Validation loss (epoch - mean): 2.1467780810707653e-06\n",
      "\n",
      "\n",
      " EPOCH 225/1000\n",
      "\t Training loss (epoch - mean):  5.14299040332844e-06\n",
      "\t Validation loss (epoch - mean): 5.347162475598687e-06\n",
      "\n",
      "\n",
      " EPOCH 226/1000\n",
      "\t Training loss (epoch - mean):  4.058832890526294e-06\n",
      "\t Validation loss (epoch - mean): 5.205803926288276e-06\n",
      "\n",
      "\n",
      " EPOCH 227/1000\n",
      "\t Training loss (epoch - mean):  4.178900705179937e-06\n",
      "\t Validation loss (epoch - mean): 3.120458137340785e-06\n",
      "\n",
      "\n",
      " EPOCH 228/1000\n",
      "\t Training loss (epoch - mean):  3.6916819681209745e-06\n",
      "\t Validation loss (epoch - mean): 3.591619291867385e-06\n",
      "\n",
      "\n",
      " EPOCH 229/1000\n",
      "\t Training loss (epoch - mean):  3.42259450765899e-06\n",
      "\t Validation loss (epoch - mean): 3.2750045194155577e-06\n",
      "\n",
      "\n",
      " EPOCH 230/1000\n",
      "\t Training loss (epoch - mean):  3.0536758686139365e-06\n",
      "\t Validation loss (epoch - mean): 3.3481096716452322e-06\n",
      "\n",
      "\n",
      " EPOCH 231/1000\n",
      "\t Training loss (epoch - mean):  2.508586864754155e-06\n",
      "\t Validation loss (epoch - mean): 1.776643202635975e-06\n",
      "\n",
      "\n",
      " EPOCH 232/1000\n",
      "\t Training loss (epoch - mean):  2.430721489569502e-06\n",
      "\t Validation loss (epoch - mean): 1.8210619787731367e-06\n",
      "\n",
      "\n",
      " EPOCH 233/1000\n",
      "\t Training loss (epoch - mean):  2.2766958333401512e-06\n",
      "\t Validation loss (epoch - mean): 1.2143073901487687e-06\n",
      "\n",
      "\n",
      " EPOCH 234/1000\n",
      "\t Training loss (epoch - mean):  2.795885147103642e-06\n",
      "\t Validation loss (epoch - mean): 1.227320123999039e-06\n",
      "\n",
      "\n",
      " EPOCH 235/1000\n",
      "\t Training loss (epoch - mean):  2.3473527145749054e-06\n",
      "\t Validation loss (epoch - mean): 1.7510943319478808e-06\n",
      "\n",
      "\n",
      " EPOCH 236/1000\n",
      "\t Training loss (epoch - mean):  1.976852975834239e-06\n",
      "\t Validation loss (epoch - mean): 1.725993972369835e-06\n",
      "\n",
      "\n",
      " EPOCH 237/1000\n",
      "\t Training loss (epoch - mean):  1.610452822357426e-06\n",
      "\t Validation loss (epoch - mean): 1.0732535128045008e-06\n",
      "\n",
      "\n",
      " EPOCH 238/1000\n",
      "\t Training loss (epoch - mean):  1.906608967298477e-06\n",
      "\t Validation loss (epoch - mean): 1.4115240272266745e-06\n",
      "\n",
      "\n",
      " EPOCH 239/1000\n",
      "\t Training loss (epoch - mean):  1.3924605373934658e-06\n",
      "\t Validation loss (epoch - mean): 1.879100689480533e-06\n",
      "\n",
      "\n",
      " EPOCH 240/1000\n",
      "\t Training loss (epoch - mean):  1.4536863517378151e-06\n",
      "\t Validation loss (epoch - mean): 7.806219388909203e-07\n",
      "\n",
      "\n",
      " EPOCH 241/1000\n",
      "\t Training loss (epoch - mean):  1.6761355823291524e-06\n",
      "\t Validation loss (epoch - mean): 1.108504553529236e-06\n",
      "\n",
      "\n",
      " EPOCH 242/1000\n",
      "\t Training loss (epoch - mean):  1.5799759391181094e-06\n",
      "\t Validation loss (epoch - mean): 1.455852779947219e-06\n",
      "\n",
      "\n",
      " EPOCH 243/1000\n",
      "\t Training loss (epoch - mean):  1.270093823298642e-06\n",
      "\t Validation loss (epoch - mean): 7.251645483647701e-07\n",
      "\n",
      "\n",
      " EPOCH 244/1000\n",
      "\t Training loss (epoch - mean):  1.2184605301020686e-06\n",
      "\t Validation loss (epoch - mean): 9.692621689693493e-07\n",
      "\n",
      "\n",
      " EPOCH 245/1000\n",
      "\t Training loss (epoch - mean):  1.0613662908554033e-06\n",
      "\t Validation loss (epoch - mean): 6.059474463375546e-07\n",
      "\n",
      "\n",
      " EPOCH 246/1000\n",
      "\t Training loss (epoch - mean):  1.07080474739026e-06\n",
      "\t Validation loss (epoch - mean): 1.08580611994735e-06\n",
      "\n",
      "\n",
      " EPOCH 247/1000\n",
      "\t Training loss (epoch - mean):  7.522696373977548e-07\n",
      "\t Validation loss (epoch - mean): 1.116347187825358e-06\n",
      "\n",
      "\n",
      " EPOCH 248/1000\n",
      "\t Training loss (epoch - mean):  9.542483875672285e-07\n",
      "\t Validation loss (epoch - mean): 4.898025525026761e-07\n",
      "\n",
      "\n",
      " EPOCH 249/1000\n",
      "\t Training loss (epoch - mean):  9.073711121724651e-07\n",
      "\t Validation loss (epoch - mean): 4.6311763583861453e-07\n",
      "\n",
      "\n",
      " EPOCH 250/1000\n",
      "\t Training loss (epoch - mean):  7.225377392690765e-07\n",
      "\t Validation loss (epoch - mean): 6.517478210076949e-07\n",
      "\n",
      "\n",
      " EPOCH 251/1000\n",
      "\t Training loss (epoch - mean):  6.112215184354378e-07\n",
      "\t Validation loss (epoch - mean): 6.657552166718715e-07\n",
      "\n",
      "\n",
      " EPOCH 252/1000\n",
      "\t Training loss (epoch - mean):  6.789444739752071e-07\n",
      "\t Validation loss (epoch - mean): 7.437141663411707e-07\n",
      "\n",
      "\n",
      " EPOCH 253/1000\n",
      "\t Training loss (epoch - mean):  5.642601621502763e-07\n",
      "\t Validation loss (epoch - mean): 2.381461627992805e-07\n",
      "\n",
      "\n",
      " EPOCH 254/1000\n",
      "\t Training loss (epoch - mean):  5.723553092214692e-07\n",
      "\t Validation loss (epoch - mean): 1.5846704322275655e-07\n",
      "\n",
      "\n",
      " EPOCH 255/1000\n",
      "\t Training loss (epoch - mean):  4.3709352344952396e-07\n",
      "\t Validation loss (epoch - mean): 3.9246379482441797e-07\n",
      "\n",
      "\n",
      " EPOCH 256/1000\n",
      "\t Training loss (epoch - mean):  4.6235234899919914e-07\n",
      "\t Validation loss (epoch - mean): 2.137122784933685e-07\n",
      "\n",
      "\n",
      " EPOCH 257/1000\n",
      "\t Training loss (epoch - mean):  3.7028880465565334e-07\n",
      "\t Validation loss (epoch - mean): 2.9094056460464157e-07\n",
      "\n",
      "\n",
      " EPOCH 258/1000\n",
      "\t Training loss (epoch - mean):  3.8982392188321075e-07\n",
      "\t Validation loss (epoch - mean): 3.0116987469865576e-07\n",
      "\n",
      "\n",
      " EPOCH 259/1000\n",
      "\t Training loss (epoch - mean):  3.201620136413415e-07\n",
      "\t Validation loss (epoch - mean): 4.818056861633419e-07\n",
      "\n",
      "\n",
      " EPOCH 260/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (epoch - mean):  3.1225988313584213e-07\n",
      "\t Validation loss (epoch - mean): 1.2158572034997086e-07\n",
      "\n",
      "\n",
      " EPOCH 261/1000\n",
      "\t Training loss (epoch - mean):  3.134013108289461e-07\n",
      "\t Validation loss (epoch - mean): 1.4664430960559132e-07\n",
      "\n",
      "\n",
      " EPOCH 262/1000\n",
      "\t Training loss (epoch - mean):  2.626732099978805e-07\n",
      "\t Validation loss (epoch - mean): 1.201161978580986e-07\n",
      "\n",
      "\n",
      " EPOCH 263/1000\n",
      "\t Training loss (epoch - mean):  2.2988339765106502e-07\n",
      "\t Validation loss (epoch - mean): 3.422684847977352e-07\n",
      "\n",
      "\n",
      " EPOCH 264/1000\n",
      "\t Training loss (epoch - mean):  2.4598672787590203e-07\n",
      "\t Validation loss (epoch - mean): 1.8237902588029284e-07\n",
      "\n",
      "\n",
      " EPOCH 265/1000\n",
      "\t Training loss (epoch - mean):  1.8219168633019459e-07\n",
      "\t Validation loss (epoch - mean): 4.0097091317933286e-07\n",
      "\n",
      "\n",
      " EPOCH 266/1000\n",
      "\t Training loss (epoch - mean):  1.871260527271564e-07\n",
      "\t Validation loss (epoch - mean): 1.1654429703747002e-07\n",
      "\n",
      "\n",
      " EPOCH 267/1000\n",
      "\t Training loss (epoch - mean):  1.7325558706223396e-07\n",
      "\t Validation loss (epoch - mean): 2.3154470346088726e-07\n",
      "\n",
      "\n",
      " EPOCH 268/1000\n",
      "\t Training loss (epoch - mean):  1.8403094159680222e-07\n",
      "\t Validation loss (epoch - mean): 9.809134123637832e-08\n",
      "\n",
      "\n",
      " EPOCH 269/1000\n",
      "\t Training loss (epoch - mean):  1.7042804382564705e-07\n",
      "\t Validation loss (epoch - mean): 1.0321276349901488e-07\n",
      "\n",
      "\n",
      " EPOCH 270/1000\n",
      "\t Training loss (epoch - mean):  1.4023127666481136e-07\n",
      "\t Validation loss (epoch - mean): 2.0687994175798743e-07\n",
      "\n",
      "\n",
      " EPOCH 271/1000\n",
      "\t Training loss (epoch - mean):  1.9587217299961898e-07\n",
      "\t Validation loss (epoch - mean): 7.02982437763708e-08\n",
      "\n",
      "\n",
      " EPOCH 272/1000\n",
      "\t Training loss (epoch - mean):  1.4904102011807177e-07\n",
      "\t Validation loss (epoch - mean): 1.1477866800146715e-07\n",
      "\n",
      "\n",
      " EPOCH 273/1000\n",
      "\t Training loss (epoch - mean):  1.3922268730463352e-07\n",
      "\t Validation loss (epoch - mean): 6.378047544024419e-08\n",
      "\n",
      "\n",
      " EPOCH 274/1000\n",
      "\t Training loss (epoch - mean):  1.2938465824656e-07\n",
      "\t Validation loss (epoch - mean): 1.2259808992053296e-07\n",
      "\n",
      "\n",
      " EPOCH 275/1000\n",
      "\t Training loss (epoch - mean):  9.952486834663432e-08\n",
      "\t Validation loss (epoch - mean): 7.546965240792259e-08\n",
      "\n",
      "\n",
      " EPOCH 276/1000\n",
      "\t Training loss (epoch - mean):  9.974891905054999e-08\n",
      "\t Validation loss (epoch - mean): 5.0043361751798346e-08\n",
      "\n",
      "\n",
      " EPOCH 277/1000\n",
      "\t Training loss (epoch - mean):  7.764793726513138e-08\n",
      "\t Validation loss (epoch - mean): 1.398156832227142e-07\n",
      "\n",
      "\n",
      " EPOCH 278/1000\n",
      "\t Training loss (epoch - mean):  8.456198585804486e-08\n",
      "\t Validation loss (epoch - mean): 5.727662703640479e-08\n",
      "\n",
      "\n",
      " EPOCH 279/1000\n",
      "\t Training loss (epoch - mean):  7.67857777361769e-08\n",
      "\t Validation loss (epoch - mean): 5.806687860950867e-08\n",
      "\n",
      "\n",
      " EPOCH 280/1000\n",
      "\t Training loss (epoch - mean):  6.462722475693757e-08\n",
      "\t Validation loss (epoch - mean): 4.760941731691081e-08\n",
      "\n",
      "\n",
      " EPOCH 281/1000\n",
      "\t Training loss (epoch - mean):  5.619546309768945e-08\n",
      "\t Validation loss (epoch - mean): 1.725856120279572e-08\n",
      "\n",
      "\n",
      " EPOCH 282/1000\n",
      "\t Training loss (epoch - mean):  5.4318132214348225e-08\n",
      "\t Validation loss (epoch - mean): 3.247804065994004e-08\n",
      "\n",
      "\n",
      " EPOCH 283/1000\n",
      "\t Training loss (epoch - mean):  5.229668313215067e-08\n",
      "\t Validation loss (epoch - mean): 5.3555658283133134e-08\n",
      "\n",
      "\n",
      " EPOCH 284/1000\n",
      "\t Training loss (epoch - mean):  4.316820116182877e-08\n",
      "\t Validation loss (epoch - mean): 3.094883227295259e-08\n",
      "\n",
      "\n",
      " EPOCH 285/1000\n",
      "\t Training loss (epoch - mean):  4.4887913238274755e-08\n",
      "\t Validation loss (epoch - mean): 1.3639847197042112e-08\n",
      "\n",
      "\n",
      " EPOCH 286/1000\n",
      "\t Training loss (epoch - mean):  3.221476448563484e-08\n",
      "\t Validation loss (epoch - mean): 1.8162827802043278e-08\n",
      "\n",
      "\n",
      " EPOCH 287/1000\n",
      "\t Training loss (epoch - mean):  3.6901924479100977e-08\n",
      "\t Validation loss (epoch - mean): 2.2273399145457188e-08\n",
      "\n",
      "\n",
      " EPOCH 288/1000\n",
      "\t Training loss (epoch - mean):  3.6038277251293493e-08\n",
      "\t Validation loss (epoch - mean): 2.032848756437984e-08\n",
      "\n",
      "\n",
      " EPOCH 289/1000\n",
      "\t Training loss (epoch - mean):  3.3717192001366435e-08\n",
      "\t Validation loss (epoch - mean): 2.2201750701261263e-08\n",
      "\n",
      "\n",
      " EPOCH 290/1000\n",
      "\t Training loss (epoch - mean):  2.7101094405660813e-08\n",
      "\t Validation loss (epoch - mean): 1.908518272235788e-08\n",
      "\n",
      "\n",
      " EPOCH 291/1000\n",
      "\t Training loss (epoch - mean):  3.9151284584685676e-08\n",
      "\t Validation loss (epoch - mean): 1.4693230588948867e-08\n",
      "\n",
      "\n",
      " EPOCH 292/1000\n",
      "\t Training loss (epoch - mean):  2.8591913133387454e-08\n",
      "\t Validation loss (epoch - mean): 2.0293350478356868e-08\n",
      "\n",
      "\n",
      " EPOCH 293/1000\n",
      "\t Training loss (epoch - mean):  1.8417243878114203e-08\n",
      "\t Validation loss (epoch - mean): 1.28345802885536e-08\n",
      "\n",
      "\n",
      " EPOCH 294/1000\n",
      "\t Training loss (epoch - mean):  2.7618362855956018e-08\n",
      "\t Validation loss (epoch - mean): 1.62047479772837e-08\n",
      "\n",
      "\n",
      " EPOCH 295/1000\n",
      "\t Training loss (epoch - mean):  1.499937181866547e-08\n",
      "\t Validation loss (epoch - mean): 1.9485393450796703e-08\n",
      "\n",
      "\n",
      " EPOCH 296/1000\n",
      "\t Training loss (epoch - mean):  2.0930959484151874e-08\n",
      "\t Validation loss (epoch - mean): 1.5367063240104815e-08\n",
      "\n",
      "\n",
      " EPOCH 297/1000\n",
      "\t Training loss (epoch - mean):  1.2107055062197484e-08\n",
      "\t Validation loss (epoch - mean): 1.1978429494898234e-08\n",
      "\n",
      "\n",
      " EPOCH 298/1000\n",
      "\t Training loss (epoch - mean):  1.676518464795625e-08\n",
      "\t Validation loss (epoch - mean): 3.7791927173378046e-09\n",
      "\n",
      "\n",
      " EPOCH 299/1000\n",
      "\t Training loss (epoch - mean):  1.666172622094564e-08\n",
      "\t Validation loss (epoch - mean): 1.2561085437156935e-08\n",
      "\n",
      "\n",
      " EPOCH 300/1000\n",
      "\t Training loss (epoch - mean):  1.6844820279378798e-08\n",
      "\t Validation loss (epoch - mean): 7.086455849430427e-09\n",
      "\n",
      "\n",
      " EPOCH 301/1000\n",
      "\t Training loss (epoch - mean):  9.978651312655037e-09\n",
      "\t Validation loss (epoch - mean): 1.0659859019057755e-08\n",
      "\n",
      "\n",
      " EPOCH 302/1000\n",
      "\t Training loss (epoch - mean):  1.0180559062433758e-08\n",
      "\t Validation loss (epoch - mean): 1.0753197960243813e-08\n",
      "\n",
      "\n",
      " EPOCH 303/1000\n",
      "\t Training loss (epoch - mean):  8.310290529796779e-09\n",
      "\t Validation loss (epoch - mean): 2.4351214804865264e-08\n",
      "\n",
      "\n",
      " EPOCH 304/1000\n",
      "\t Training loss (epoch - mean):  6.619254995972786e-09\n",
      "\t Validation loss (epoch - mean): 2.4989276011203337e-09\n",
      "\n",
      "\n",
      " EPOCH 305/1000\n",
      "\t Training loss (epoch - mean):  1.2099361438681436e-08\n",
      "\t Validation loss (epoch - mean): 5.377957399593673e-09\n",
      "\n",
      "\n",
      " EPOCH 306/1000\n",
      "\t Training loss (epoch - mean):  6.0956287054973286e-09\n",
      "\t Validation loss (epoch - mean): 1.5996827228624643e-08\n",
      "\n",
      "\n",
      " EPOCH 307/1000\n",
      "\t Training loss (epoch - mean):  6.408026627842863e-09\n",
      "\t Validation loss (epoch - mean): 2.508955801391109e-09\n",
      "\n",
      "\n",
      " EPOCH 308/1000\n",
      "\t Training loss (epoch - mean):  5.505532104284801e-09\n",
      "\t Validation loss (epoch - mean): 5.549868085902373e-09\n",
      "\n",
      "\n",
      " EPOCH 309/1000\n",
      "\t Training loss (epoch - mean):  6.220565433127471e-09\n",
      "\t Validation loss (epoch - mean): 1.63915654797975e-08\n",
      "\n",
      "\n",
      " EPOCH 310/1000\n",
      "\t Training loss (epoch - mean):  4.7801051635450636e-09\n",
      "\t Validation loss (epoch - mean): 1.5323583296823894e-08\n",
      "\n",
      "\n",
      " EPOCH 311/1000\n",
      "\t Training loss (epoch - mean):  7.478769825652876e-09\n",
      "\t Validation loss (epoch - mean): 5.333469173971785e-09\n",
      "\n",
      "\n",
      " EPOCH 312/1000\n",
      "\t Training loss (epoch - mean):  5.390096979862591e-09\n",
      "\t Validation loss (epoch - mean): 3.5181092597546917e-09\n",
      "\n",
      "\n",
      " EPOCH 313/1000\n",
      "\t Training loss (epoch - mean):  2.3367076421957713e-09\n",
      "\t Validation loss (epoch - mean): 1.9779230561505083e-09\n",
      "\n",
      "\n",
      " EPOCH 314/1000\n",
      "\t Training loss (epoch - mean):  2.022460079039353e-09\n",
      "\t Validation loss (epoch - mean): 2.8734858543464277e-09\n",
      "\n",
      "\n",
      " EPOCH 315/1000\n",
      "\t Training loss (epoch - mean):  2.89603430125851e-09\n",
      "\t Validation loss (epoch - mean): 1.0117891679625912e-09\n",
      "\n",
      "\n",
      " EPOCH 316/1000\n",
      "\t Training loss (epoch - mean):  5.4311530735725215e-09\n",
      "\t Validation loss (epoch - mean): 1.2752341574857087e-09\n",
      "\n",
      "\n",
      " EPOCH 317/1000\n",
      "\t Training loss (epoch - mean):  5.420169609434329e-09\n",
      "\t Validation loss (epoch - mean): 2.0774943144701225e-09\n",
      "\n",
      "\n",
      " EPOCH 318/1000\n",
      "\t Training loss (epoch - mean):  2.520785894836545e-09\n",
      "\t Validation loss (epoch - mean): 2.3875866807789884e-09\n",
      "\n",
      "\n",
      " EPOCH 319/1000\n",
      "\t Training loss (epoch - mean):  2.922400765825728e-09\n",
      "\t Validation loss (epoch - mean): 1.1002317699413377e-09\n",
      "\n",
      "\n",
      " EPOCH 320/1000\n",
      "\t Training loss (epoch - mean):  5.678912321875155e-09\n",
      "\t Validation loss (epoch - mean): 1.311939007748624e-09\n",
      "\n",
      "\n",
      " EPOCH 321/1000\n",
      "\t Training loss (epoch - mean):  2.350057629977679e-09\n",
      "\t Validation loss (epoch - mean): 6.233988690077273e-10\n",
      "\n",
      "\n",
      " EPOCH 322/1000\n",
      "\t Training loss (epoch - mean):  8.967292417875564e-10\n",
      "\t Validation loss (epoch - mean): 5.772868121219047e-10\n",
      "\n",
      "\n",
      " EPOCH 323/1000\n",
      "\t Training loss (epoch - mean):  1.6588270016863287e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Validation loss (epoch - mean): 9.01310639108885e-10\n",
      "\n",
      "\n",
      " EPOCH 324/1000\n",
      "\t Training loss (epoch - mean):  1.748397251051988e-09\n",
      "\t Validation loss (epoch - mean): 9.15108256722007e-10\n",
      "\n",
      "\n",
      " EPOCH 325/1000\n",
      "\t Training loss (epoch - mean):  2.0684932655612442e-09\n",
      "\t Validation loss (epoch - mean): 3.4614038010228535e-09\n",
      "\n",
      "\n",
      " EPOCH 326/1000\n",
      "\t Training loss (epoch - mean):  8.530761351951041e-10\n",
      "\t Validation loss (epoch - mean): 5.049472265018194e-09\n",
      "\n",
      "\n",
      " EPOCH 327/1000\n",
      "\t Training loss (epoch - mean):  1.4992932959959997e-09\n",
      "\t Validation loss (epoch - mean): 8.697549008093711e-10\n",
      "\n",
      "\n",
      " EPOCH 328/1000\n",
      "\t Training loss (epoch - mean):  2.872333001076773e-09\n",
      "\t Validation loss (epoch - mean): 1.283796048560671e-10\n",
      "\n",
      "\n",
      " EPOCH 329/1000\n",
      "\t Training loss (epoch - mean):  5.54578979961029e-10\n",
      "\t Validation loss (epoch - mean): 3.736215431969591e-09\n",
      "\n",
      "\n",
      " EPOCH 330/1000\n",
      "\t Training loss (epoch - mean):  1.6652703105357602e-09\n",
      "\t Validation loss (epoch - mean): 2.520846021397978e-10\n",
      "\n",
      "\n",
      " EPOCH 331/1000\n",
      "\t Training loss (epoch - mean):  1.0823040658136662e-09\n",
      "\t Validation loss (epoch - mean): 1.364843172544644e-10\n",
      "\n",
      "\n",
      " EPOCH 332/1000\n",
      "\t Training loss (epoch - mean):  6.570682169656136e-10\n",
      "\t Validation loss (epoch - mean): 4.152294835724272e-10\n",
      "\n",
      "\n",
      " EPOCH 333/1000\n",
      "\t Training loss (epoch - mean):  6.326970752867922e-10\n",
      "\t Validation loss (epoch - mean): 3.908182707428674e-10\n",
      "\n",
      "\n",
      " EPOCH 334/1000\n",
      "\t Training loss (epoch - mean):  8.73184575570877e-10\n",
      "\t Validation loss (epoch - mean): 2.7859102258101957e-09\n",
      "\n",
      "\n",
      " EPOCH 335/1000\n",
      "\t Training loss (epoch - mean):  1.0108323021211045e-09\n",
      "\t Validation loss (epoch - mean): 2.53633491774074e-09\n",
      "\n",
      "\n",
      " EPOCH 336/1000\n",
      "\t Training loss (epoch - mean):  4.4187414838248174e-10\n",
      "\t Validation loss (epoch - mean): 8.410118693592034e-11\n",
      "\n",
      "\n",
      " EPOCH 337/1000\n",
      "\t Training loss (epoch - mean):  5.921377242701364e-10\n",
      "\t Validation loss (epoch - mean): 1.0909389063291476e-10\n",
      "\n",
      "\n",
      " EPOCH 338/1000\n",
      "\t Training loss (epoch - mean):  9.593147757650893e-10\n",
      "\t Validation loss (epoch - mean): 1.4521439972432134e-10\n",
      "\n",
      "\n",
      " EPOCH 339/1000\n",
      "\t Training loss (epoch - mean):  3.411679463244388e-10\n",
      "\t Validation loss (epoch - mean): 2.848485570957443e-10\n",
      "\n",
      "\n",
      " EPOCH 340/1000\n",
      "\t Training loss (epoch - mean):  3.1456853325876705e-10\n",
      "\t Validation loss (epoch - mean): 1.6507671203080454e-10\n",
      "\n",
      "\n",
      " EPOCH 341/1000\n",
      "\t Training loss (epoch - mean):  4.7831035081732e-10\n",
      "\t Validation loss (epoch - mean): 1.207152608102318e-10\n",
      "\n",
      "\n",
      " EPOCH 342/1000\n",
      "\t Training loss (epoch - mean):  1.0961382160742013e-09\n",
      "\t Validation loss (epoch - mean): 5.944097842113297e-11\n",
      "\n",
      "\n",
      " EPOCH 343/1000\n",
      "\t Training loss (epoch - mean):  2.479853070576817e-10\n",
      "\t Validation loss (epoch - mean): 9.882179607996832e-11\n",
      "\n",
      "\n",
      " EPOCH 344/1000\n",
      "\t Training loss (epoch - mean):  3.5780697964689817e-10\n",
      "\t Validation loss (epoch - mean): 1.053707996484698e-10\n",
      "\n",
      "\n",
      " EPOCH 345/1000\n",
      "\t Training loss (epoch - mean):  1.9777998336301117e-10\n",
      "\t Validation loss (epoch - mean): 6.038800106022388e-11\n",
      "\n",
      "\n",
      " EPOCH 346/1000\n",
      "\t Training loss (epoch - mean):  3.1154926150485834e-10\n",
      "\t Validation loss (epoch - mean): 1.2963411973304868e-09\n",
      "\n",
      "\n",
      " EPOCH 347/1000\n",
      "\t Training loss (epoch - mean):  1.585452894548959e-10\n",
      "\t Validation loss (epoch - mean): 8.753896593065649e-11\n",
      "\n",
      "\n",
      " EPOCH 348/1000\n",
      "\t Training loss (epoch - mean):  4.339250226498281e-11\n",
      "\t Validation loss (epoch - mean): 5.393062618578326e-11\n",
      "\n",
      "\n",
      " EPOCH 349/1000\n",
      "\t Training loss (epoch - mean):  4.938500626957184e-10\n",
      "\t Validation loss (epoch - mean): 5.422402163596617e-11\n",
      "\n",
      "\n",
      " EPOCH 350/1000\n",
      "\t Training loss (epoch - mean):  1.3812200428536814e-10\n",
      "\t Validation loss (epoch - mean): 5.490898114625948e-11\n",
      "\n",
      "\n",
      " EPOCH 351/1000\n",
      "\t Training loss (epoch - mean):  4.398420837617434e-10\n",
      "\t Validation loss (epoch - mean): 2.1042805461230148e-11\n",
      "\n",
      "\n",
      " EPOCH 352/1000\n",
      "\t Training loss (epoch - mean):  1.246797699805402e-10\n",
      "\t Validation loss (epoch - mean): 3.8140943158253886e-11\n",
      "\n",
      "\n",
      " EPOCH 353/1000\n",
      "\t Training loss (epoch - mean):  3.6615752559606324e-10\n",
      "\t Validation loss (epoch - mean): 4.0019940058898755e-11\n",
      "\n",
      "\n",
      " EPOCH 354/1000\n",
      "\t Training loss (epoch - mean):  1.1113761271229021e-10\n",
      "\t Validation loss (epoch - mean): 2.8536665069126327e-11\n",
      "\n",
      "\n",
      " EPOCH 355/1000\n",
      "\t Training loss (epoch - mean):  2.565808772045375e-10\n",
      "\t Validation loss (epoch - mean): 1.5035189318156714e-09\n",
      "\n",
      "\n",
      " EPOCH 356/1000\n",
      "\t Training loss (epoch - mean):  2.879156577502891e-11\n",
      "\t Validation loss (epoch - mean): 4.291127380322129e-11\n",
      "\n",
      "\n",
      " EPOCH 357/1000\n",
      "\t Training loss (epoch - mean):  2.2152637479030032e-10\n",
      "\t Validation loss (epoch - mean): 2.044499701480951e-11\n",
      "\n",
      "\n",
      " EPOCH 358/1000\n",
      "\t Training loss (epoch - mean):  1.457897773858574e-10\n",
      "\t Validation loss (epoch - mean): 6.5960306373251915e-09\n",
      "\n",
      "\n",
      " EPOCH 359/1000\n",
      "\t Training loss (epoch - mean):  3.7520622416312155e-10\n",
      "\t Validation loss (epoch - mean): 1.4253062691359282e-11\n",
      "\n",
      "\n",
      " EPOCH 360/1000\n",
      "\t Training loss (epoch - mean):  1.8002081039689607e-10\n",
      "\t Validation loss (epoch - mean): 3.4651965357223145e-11\n",
      "\n",
      "\n",
      " EPOCH 361/1000\n",
      "\t Training loss (epoch - mean):  1.4997323695420395e-11\n",
      "\t Validation loss (epoch - mean): 1.3091263617195596e-11\n",
      "\n",
      "\n",
      " EPOCH 362/1000\n",
      "\t Training loss (epoch - mean):  1.5775595442237464e-10\n",
      "\t Validation loss (epoch - mean): 3.111777622525338e-11\n",
      "\n",
      "\n",
      " EPOCH 363/1000\n",
      "\t Training loss (epoch - mean):  1.0089223828497566e-10\n",
      "\t Validation loss (epoch - mean): 1.8910571219249274e-11\n",
      "\n",
      "\n",
      " EPOCH 364/1000\n",
      "\t Training loss (epoch - mean):  1.3856729840523255e-10\n",
      "\t Validation loss (epoch - mean): 5.30631085392341e-12\n",
      "\n",
      "\n",
      " EPOCH 365/1000\n",
      "\t Training loss (epoch - mean):  2.1231631672687565e-10\n",
      "\t Validation loss (epoch - mean): 6.854511386253868e-12\n",
      "\n",
      "\n",
      " EPOCH 366/1000\n",
      "\t Training loss (epoch - mean):  4.8144032452808925e-11\n",
      "\t Validation loss (epoch - mean): 4.917607442502258e-12\n",
      "\n",
      "\n",
      " EPOCH 367/1000\n",
      "\t Training loss (epoch - mean):  1.8305424190345754e-10\n",
      "\t Validation loss (epoch - mean): 2.7156998557913276e-09\n",
      "\n",
      "\n",
      " EPOCH 368/1000\n",
      "\t Training loss (epoch - mean):  7.0796660923735e-11\n",
      "\t Validation loss (epoch - mean): 3.247089681691884e-10\n",
      "\n",
      "\n",
      " EPOCH 369/1000\n",
      "\t Training loss (epoch - mean):  6.688804154650341e-11\n",
      "\t Validation loss (epoch - mean): 6.7261429696741956e-12\n",
      "\n",
      "\n",
      " EPOCH 370/1000\n",
      "\t Training loss (epoch - mean):  9.11719253334405e-11\n",
      "\t Validation loss (epoch - mean): 2.301442427228016e-12\n",
      "\n",
      "\n",
      " EPOCH 371/1000\n",
      "\t Training loss (epoch - mean):  8.424044496907826e-11\n",
      "\t Validation loss (epoch - mean): 2.1012793833111596e-12\n",
      "\n",
      "\n",
      " EPOCH 372/1000\n",
      "\t Training loss (epoch - mean):  7.833513510889974e-11\n",
      "\t Validation loss (epoch - mean): 2.7502336902315796e-12\n",
      "\n",
      "\n",
      " EPOCH 373/1000\n",
      "\t Training loss (epoch - mean):  7.258163518270254e-11\n",
      "\t Validation loss (epoch - mean): 3.1328441116552266e-12\n",
      "\n",
      "\n",
      " EPOCH 374/1000\n",
      "\t Training loss (epoch - mean):  6.895150397549556e-11\n",
      "\t Validation loss (epoch - mean): 2.0816329847747485e-12\n",
      "\n",
      "\n",
      " EPOCH 375/1000\n",
      "\t Training loss (epoch - mean):  2.2496954729522612e-11\n",
      "\t Validation loss (epoch - mean): 6.616411677380216e-13\n",
      "\n",
      "\n",
      " EPOCH 376/1000\n",
      "\t Training loss (epoch - mean):  2.2895090951863723e-11\n",
      "\t Validation loss (epoch - mean): 6.779066792303012e-13\n",
      "\n",
      "\n",
      " EPOCH 377/1000\n",
      "\t Training loss (epoch - mean):  1.8479399699908652e-10\n",
      "\t Validation loss (epoch - mean): 1.2708579596250152e-12\n",
      "\n",
      "\n",
      " EPOCH 378/1000\n",
      "\t Training loss (epoch - mean):  3.544667753040363e-11\n",
      "\t Validation loss (epoch - mean): 2.3544178667110113e-12\n",
      "\n",
      "\n",
      " EPOCH 379/1000\n",
      "\t Training loss (epoch - mean):  7.95649557885104e-11\n",
      "\t Validation loss (epoch - mean): 1.4567643549397742e-10\n",
      "\n",
      "\n",
      " EPOCH 380/1000\n",
      "\t Training loss (epoch - mean):  8.782818741306815e-11\n",
      "\t Validation loss (epoch - mean): 1.4307303707184947e-10\n",
      "\n",
      "\n",
      " EPOCH 381/1000\n",
      "\t Training loss (epoch - mean):  5.3344022034531917e-11\n",
      "\t Validation loss (epoch - mean): 1.8906431698445713e-12\n",
      "\n",
      "\n",
      " EPOCH 382/1000\n",
      "\t Training loss (epoch - mean):  5.070435544592249e-11\n",
      "\t Validation loss (epoch - mean): 1.1755218156722634e-12\n",
      "\n",
      "\n",
      " EPOCH 383/1000\n",
      "\t Training loss (epoch - mean):  2.3617938207729183e-11\n",
      "\t Validation loss (epoch - mean): 1.4407360322323385e-12\n",
      "\n",
      "\n",
      " EPOCH 384/1000\n",
      "\t Training loss (epoch - mean):  1.1061058780058655e-11\n",
      "\t Validation loss (epoch - mean): 2.7657448052062544e-09\n",
      "\n",
      "\n",
      " EPOCH 385/1000\n",
      "\t Training loss (epoch - mean):  2.0450727279707794e-11\n",
      "\t Validation loss (epoch - mean): 9.43271043445519e-10\n",
      "\n",
      "\n",
      " EPOCH 386/1000\n",
      "\t Training loss (epoch - mean):  9.97571096276005e-12\n",
      "\t Validation loss (epoch - mean): 5.193263302239504e-13\n",
      "\n",
      "\n",
      " EPOCH 387/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (epoch - mean):  3.522654610411466e-11\n",
      "\t Validation loss (epoch - mean): 7.511870360252132e-10\n",
      "\n",
      "\n",
      " EPOCH 388/1000\n",
      "\t Training loss (epoch - mean):  4.0323168555444294e-11\n",
      "\t Validation loss (epoch - mean): 8.656957988047458e-11\n",
      "\n",
      "\n",
      " EPOCH 389/1000\n",
      "\t Training loss (epoch - mean):  1.5424387348777758e-11\n",
      "\t Validation loss (epoch - mean): 2.8557465091898656e-10\n",
      "\n",
      "\n",
      " EPOCH 390/1000\n",
      "\t Training loss (epoch - mean):  4.931087439868255e-11\n",
      "\t Validation loss (epoch - mean): 4.727893321438557e-13\n",
      "\n",
      "\n",
      " EPOCH 391/1000\n",
      "\t Training loss (epoch - mean):  5.514822358903641e-13\n",
      "\t Validation loss (epoch - mean): 4.857399493851057e-13\n",
      "\n",
      "\n",
      " EPOCH 392/1000\n",
      "\t Training loss (epoch - mean):  1.8235277852967035e-11\n",
      "\t Validation loss (epoch - mean): 2.3563642626391885e-13\n",
      "\n",
      "\n",
      " EPOCH 393/1000\n",
      "\t Training loss (epoch - mean):  2.399274084529529e-13\n",
      "\t Validation loss (epoch - mean): 1.89396939970888e-13\n",
      "\n",
      "\n",
      " EPOCH 394/1000\n",
      "\t Training loss (epoch - mean):  4.243801662668741e-11\n",
      "\t Validation loss (epoch - mean): 8.447345918388378e-10\n",
      "\n",
      "\n",
      " EPOCH 395/1000\n",
      "\t Training loss (epoch - mean):  1.0072557954268584e-11\n",
      "\t Validation loss (epoch - mean): 3.4188014809252235e-13\n",
      "\n",
      "\n",
      " EPOCH 396/1000\n",
      "\t Training loss (epoch - mean):  1.365790172292459e-11\n",
      "\t Validation loss (epoch - mean): 3.313152622838334e-13\n",
      "\n",
      "\n",
      " EPOCH 397/1000\n",
      "\t Training loss (epoch - mean):  2.3992782180503117e-13\n",
      "\t Validation loss (epoch - mean): 1.3825313295310137e-10\n",
      "\n",
      "\n",
      " EPOCH 398/1000\n",
      "\t Training loss (epoch - mean):  1.1821545442489608e-11\n",
      "\t Validation loss (epoch - mean): 2.1242294895847076e-09\n",
      "\n",
      "\n",
      " EPOCH 399/1000\n",
      "\t Training loss (epoch - mean):  2.1898109160126047e-11\n",
      "\t Validation loss (epoch - mean): 1.2157342963795445e-13\n",
      "\n",
      "\n",
      " EPOCH 400/1000\n",
      "\t Training loss (epoch - mean):  3.446720364541142e-12\n",
      "\t Validation loss (epoch - mean): 6.290735389348859e-10\n",
      "\n",
      "\n",
      " EPOCH 401/1000\n",
      "\t Training loss (epoch - mean):  9.402778465642794e-12\n",
      "\t Validation loss (epoch - mean): 3.046873497843873e-11\n",
      "\n",
      "\n",
      " EPOCH 402/1000\n",
      "\t Training loss (epoch - mean):  3.0493758616370746e-12\n",
      "\t Validation loss (epoch - mean): 3.204089153201353e-11\n",
      "\n",
      "\n",
      " EPOCH 403/1000\n",
      "\t Training loss (epoch - mean):  1.3632519416171933e-11\n",
      "\t Validation loss (epoch - mean): 5.29337452966592e-11\n",
      "\n",
      "\n",
      " EPOCH 404/1000\n",
      "\t Training loss (epoch - mean):  7.563325332238073e-12\n",
      "\t Validation loss (epoch - mean): 6.810003013400415e-13\n",
      "\n",
      "\n",
      " EPOCH 405/1000\n",
      "\t Training loss (epoch - mean):  7.025928808156529e-12\n",
      "\t Validation loss (epoch - mean): 9.183356604776959e-13\n",
      "\n",
      "\n",
      " EPOCH 406/1000\n",
      "\t Training loss (epoch - mean):  2.27136591147171e-12\n",
      "\t Validation loss (epoch - mean): 1.1957106721852704e-09\n",
      "\n",
      "\n",
      " EPOCH 407/1000\n",
      "\t Training loss (epoch - mean):  8.318953740638293e-12\n",
      "\t Validation loss (epoch - mean): 8.152509682645583e-14\n",
      "\n",
      "\n",
      " EPOCH 408/1000\n",
      "\t Training loss (epoch - mean):  1.8832950985333686e-12\n",
      "\t Validation loss (epoch - mean): 4.70744532832638e-11\n",
      "\n",
      "\n",
      " EPOCH 409/1000\n",
      "\t Training loss (epoch - mean):  3.475287062012849e-12\n",
      "\t Validation loss (epoch - mean): 4.1292215917596563e-10\n",
      "\n",
      "\n",
      " EPOCH 410/1000\n",
      "\t Training loss (epoch - mean):  8.274600346051111e-12\n",
      "\t Validation loss (epoch - mean): 1.2031954640805746e-13\n",
      "\n",
      "\n",
      " EPOCH 411/1000\n",
      "\t Training loss (epoch - mean):  5.999328525102254e-12\n",
      "\t Validation loss (epoch - mean): 1.4933734919447644e-11\n",
      "\n",
      "\n",
      " EPOCH 412/1000\n",
      "\t Training loss (epoch - mean):  8.366693735578924e-12\n",
      "\t Validation loss (epoch - mean): 9.984269636927965e-14\n",
      "\n",
      "\n",
      " EPOCH 413/1000\n",
      "\t Training loss (epoch - mean):  6.3709131613996226e-12\n",
      "\t Validation loss (epoch - mean): 9.599013663154665e-14\n",
      "\n",
      "\n",
      " EPOCH 414/1000\n",
      "\t Training loss (epoch - mean):  4.850411446412712e-12\n",
      "\t Validation loss (epoch - mean): 9.025309939847652e-14\n",
      "\n",
      "\n",
      " EPOCH 415/1000\n",
      "\t Training loss (epoch - mean):  2.1722250751209137e-12\n",
      "\t Validation loss (epoch - mean): 4.856247442120541e-14\n",
      "\n",
      "\n",
      " EPOCH 416/1000\n",
      "\t Training loss (epoch - mean):  2.0370710397486268e-12\n",
      "\t Validation loss (epoch - mean): 2.6698808545195093e-14\n",
      "\n",
      "\n",
      " EPOCH 417/1000\n",
      "\t Training loss (epoch - mean):  9.45722651739807e-13\n",
      "\t Validation loss (epoch - mean): 1.1110241223262047e-09\n",
      "\n",
      "\n",
      " EPOCH 418/1000\n",
      "\t Training loss (epoch - mean):  8.991854241025198e-13\n",
      "\t Validation loss (epoch - mean): 6.744167223487166e-14\n",
      "\n",
      "\n",
      " EPOCH 419/1000\n",
      "\t Training loss (epoch - mean):  1.6546436477723543e-12\n",
      "\t Validation loss (epoch - mean): 8.185367744073385e-15\n",
      "\n",
      "\n",
      " EPOCH 420/1000\n",
      "\t Training loss (epoch - mean):  7.827237824669057e-13\n",
      "\t Validation loss (epoch - mean): 4.4199419962817496e-11\n",
      "\n",
      "\n",
      " EPOCH 421/1000\n",
      "\t Training loss (epoch - mean):  2.14143323324937e-12\n",
      "\t Validation loss (epoch - mean): 1.8958697939353812e-11\n",
      "\n",
      "\n",
      " EPOCH 422/1000\n",
      "\t Training loss (epoch - mean):  9.223566649899572e-15\n",
      "\t Validation loss (epoch - mean): 7.9232424991914e-15\n",
      "\n",
      "\n",
      " EPOCH 423/1000\n",
      "\t Training loss (epoch - mean):  6.19688776474427e-13\n",
      "\t Validation loss (epoch - mean): 7.95294259985231e-12\n",
      "\n",
      "\n",
      " EPOCH 424/1000\n",
      "\t Training loss (epoch - mean):  1.537842570187452e-10\n",
      "\t Validation loss (epoch - mean): 4.601168446924946e-11\n",
      "\n",
      "\n",
      " EPOCH 425/1000\n",
      "\t Training loss (epoch - mean):  1.126122021086434e-12\n",
      "\t Validation loss (epoch - mean): 5.171179557858735e-10\n",
      "\n",
      "\n",
      " EPOCH 426/1000\n",
      "\t Training loss (epoch - mean):  1.5914095111553518e-12\n",
      "\t Validation loss (epoch - mean): 1.8736983212041985e-13\n",
      "\n",
      "\n",
      " EPOCH 427/1000\n",
      "\t Training loss (epoch - mean):  2.3739285714763784e-12\n",
      "\t Validation loss (epoch - mean): 3.976777602181294e-10\n",
      "\n",
      "\n",
      " EPOCH 428/1000\n",
      "\t Training loss (epoch - mean):  2.607545276858155e-12\n",
      "\t Validation loss (epoch - mean): 4.835219467139028e-14\n",
      "\n",
      "\n",
      " EPOCH 429/1000\n",
      "\t Training loss (epoch - mean):  7.992884271813212e-13\n",
      "\t Validation loss (epoch - mean): 3.411637457918881e-14\n",
      "\n",
      "\n",
      " EPOCH 430/1000\n",
      "\t Training loss (epoch - mean):  3.960390500788724e-13\n",
      "\t Validation loss (epoch - mean): 2.0347831665306944e-14\n",
      "\n",
      "\n",
      " EPOCH 431/1000\n",
      "\t Training loss (epoch - mean):  6.656725163262684e-13\n",
      "\t Validation loss (epoch - mean): 1.1686421066940535e-09\n",
      "\n",
      "\n",
      " EPOCH 432/1000\n",
      "\t Training loss (epoch - mean):  9.213433901801013e-13\n",
      "\t Validation loss (epoch - mean): 2.4403702879748562e-15\n",
      "\n",
      "\n",
      " EPOCH 433/1000\n",
      "\t Training loss (epoch - mean):  1.213292012797141e-12\n",
      "\t Validation loss (epoch - mean): 2.738015534893721e-11\n",
      "\n",
      "\n",
      " EPOCH 434/1000\n",
      "\t Training loss (epoch - mean):  6.023382436859996e-15\n",
      "\t Validation loss (epoch - mean): 4.294596709526591e-15\n",
      "\n",
      "\n",
      " EPOCH 435/1000\n",
      "\t Training loss (epoch - mean):  2.501225161052947e-13\n",
      "\t Validation loss (epoch - mean): 2.123086703294825e-15\n",
      "\n",
      "\n",
      " EPOCH 436/1000\n",
      "\t Training loss (epoch - mean):  1.133897890444845e-12\n",
      "\t Validation loss (epoch - mean): 2.5074877507551553e-15\n",
      "\n",
      "\n",
      " EPOCH 437/1000\n",
      "\t Training loss (epoch - mean):  4.311249527933343e-13\n",
      "\t Validation loss (epoch - mean): 2.1130650514920015e-10\n",
      "\n",
      "\n",
      " EPOCH 438/1000\n",
      "\t Training loss (epoch - mean):  3.9963769150755287e-13\n",
      "\t Validation loss (epoch - mean): 6.090358043623462e-13\n",
      "\n",
      "\n",
      " EPOCH 439/1000\n",
      "\t Training loss (epoch - mean):  3.7034380036876017e-13\n",
      "\t Validation loss (epoch - mean): 1.7673903277289038e-10\n",
      "\n",
      "\n",
      " EPOCH 440/1000\n",
      "\t Training loss (epoch - mean):  3.413342356381085e-13\n",
      "\t Validation loss (epoch - mean): 4.896217036070922e-12\n",
      "\n",
      "\n",
      " EPOCH 441/1000\n",
      "\t Training loss (epoch - mean):  4.803289757853834e-13\n",
      "\t Validation loss (epoch - mean): 3.0149466802106665e-15\n",
      "\n",
      "\n",
      " EPOCH 442/1000\n",
      "\t Training loss (epoch - mean):  4.436972300012485e-13\n",
      "\t Validation loss (epoch - mean): 1.4745371246144084e-11\n",
      "\n",
      "\n",
      " EPOCH 443/1000\n",
      "\t Training loss (epoch - mean):  5.432849881106569e-13\n",
      "\t Validation loss (epoch - mean): 4.518360592416971e-14\n",
      "\n",
      "\n",
      " EPOCH 444/1000\n",
      "\t Training loss (epoch - mean):  6.123655540761973e-13\n",
      "\t Validation loss (epoch - mean): 4.806461505074278e-15\n",
      "\n",
      "\n",
      " EPOCH 445/1000\n",
      "\t Training loss (epoch - mean):  6.2712429532443e-12\n",
      "\t Validation loss (epoch - mean): 1.9014342242543527e-14\n",
      "\n",
      "\n",
      " EPOCH 446/1000\n",
      "\t Training loss (epoch - mean):  1.0927854813301346e-13\n",
      "\t Validation loss (epoch - mean): 1.0066341124314628e-11\n",
      "\n",
      "\n",
      " EPOCH 447/1000\n",
      "\t Training loss (epoch - mean):  9.858627805687776e-14\n",
      "\t Validation loss (epoch - mean): 4.3310110281588083e-11\n",
      "\n",
      "\n",
      " EPOCH 448/1000\n",
      "\t Training loss (epoch - mean):  1.8279413863218687e-13\n",
      "\t Validation loss (epoch - mean): 4.869899719176753e-15\n",
      "\n",
      "\n",
      " EPOCH 449/1000\n",
      "\t Training loss (epoch - mean):  3.3571657275385355e-13\n",
      "\t Validation loss (epoch - mean): 1.7847795332097574e-14\n",
      "\n",
      "\n",
      " EPOCH 450/1000\n",
      "\t Training loss (epoch - mean):  1.5201765700610412e-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Validation loss (epoch - mean): 4.1040652365870466e-11\n",
      "\n",
      "\n",
      " EPOCH 451/1000\n",
      "\t Training loss (epoch - mean):  1.430385048542153e-13\n",
      "\t Validation loss (epoch - mean): 1.3637906590541634e-12\n",
      "\n",
      "\n",
      " EPOCH 452/1000\n",
      "\t Training loss (epoch - mean):  6.45446193797126e-14\n",
      "\t Validation loss (epoch - mean): 4.0566431615506515e-14\n",
      "\n",
      "\n",
      " EPOCH 453/1000\n",
      "\t Training loss (epoch - mean):  2.397234568864744e-13\n",
      "\t Validation loss (epoch - mean): 1.4914633391420271e-12\n",
      "\n",
      "\n",
      " EPOCH 454/1000\n",
      "\t Training loss (epoch - mean):  1.12602489330068e-13\n",
      "\t Validation loss (epoch - mean): 5.672690718614532e-10\n",
      "\n",
      "\n",
      " EPOCH 455/1000\n",
      "\t Training loss (epoch - mean):  1.0370860369687496e-13\n",
      "\t Validation loss (epoch - mean): 1.2848130755621229e-15\n",
      "\n",
      "\n",
      " EPOCH 456/1000\n",
      "\t Training loss (epoch - mean):  4.843585497964281e-14\n",
      "\t Validation loss (epoch - mean): 2.8444926632548687e-13\n",
      "\n",
      "\n",
      " EPOCH 457/1000\n",
      "\t Training loss (epoch - mean):  4.4059894441645227e-14\n",
      "\t Validation loss (epoch - mean): 4.509756246988989e-11\n",
      "\n",
      "\n",
      " EPOCH 458/1000\n",
      "\t Training loss (epoch - mean):  4.181723255122067e-14\n",
      "\t Validation loss (epoch - mean): 1.2337563191392772e-09\n",
      "\n",
      "\n",
      " EPOCH 459/1000\n",
      "\t Training loss (epoch - mean):  4.50657396482499e-13\n",
      "\t Validation loss (epoch - mean): 2.1860199804944792e-09\n",
      "\n",
      "\n",
      " EPOCH 460/1000\n",
      "\t Training loss (epoch - mean):  2.441762068683131e-13\n",
      "\t Validation loss (epoch - mean): 4.36761685923293e-11\n",
      "\n",
      "\n",
      " EPOCH 461/1000\n",
      "\t Training loss (epoch - mean):  3.402157984055983e-14\n",
      "\t Validation loss (epoch - mean): 3.024046231002857e-13\n",
      "\n",
      "\n",
      " EPOCH 462/1000\n",
      "\t Training loss (epoch - mean):  1.1763732772509345e-13\n",
      "\t Validation loss (epoch - mean): 8.758104889587031e-16\n",
      "\n",
      "\n",
      " EPOCH 463/1000\n",
      "\t Training loss (epoch - mean):  2.7208176444180204e-14\n",
      "\t Validation loss (epoch - mean): 5.4104823744823245e-11\n",
      "\n",
      "\n",
      " EPOCH 464/1000\n",
      "\t Training loss (epoch - mean):  1.2217047357883459e-13\n",
      "\t Validation loss (epoch - mean): 2.971271563860998e-15\n",
      "\n",
      "\n",
      " EPOCH 465/1000\n",
      "\t Training loss (epoch - mean):  2.3448828998487678e-14\n",
      "\t Validation loss (epoch - mean): 3.699656788758607e-11\n",
      "\n",
      "\n",
      " EPOCH 466/1000\n",
      "\t Training loss (epoch - mean):  1.4345358198565854e-13\n",
      "\t Validation loss (epoch - mean): 1.4265973733844821e-09\n",
      "\n",
      "\n",
      " EPOCH 467/1000\n",
      "\t Training loss (epoch - mean):  3.8284275587248444e-14\n",
      "\t Validation loss (epoch - mean): 1.0302614642730132e-10\n",
      "\n",
      "\n",
      " EPOCH 468/1000\n",
      "\t Training loss (epoch - mean):  8.407668074158653e-14\n",
      "\t Validation loss (epoch - mean): 1.6151062649814707e-13\n",
      "\n",
      "\n",
      " EPOCH 469/1000\n",
      "\t Training loss (epoch - mean):  3.101281520037193e-14\n",
      "\t Validation loss (epoch - mean): 4.769962120770308e-16\n",
      "\n",
      "\n",
      " EPOCH 470/1000\n",
      "\t Training loss (epoch - mean):  2.8176615082573977e-14\n",
      "\t Validation loss (epoch - mean): 3.010256938342367e-16\n",
      "\n",
      "\n",
      " EPOCH 471/1000\n",
      "\t Training loss (epoch - mean):  1.2732359142520986e-14\n",
      "\t Validation loss (epoch - mean): 2.127518117388207e-15\n",
      "\n",
      "\n",
      " EPOCH 472/1000\n",
      "\t Training loss (epoch - mean):  6.027903752983215e-14\n",
      "\t Validation loss (epoch - mean): 1.0733334239966815e-09\n",
      "\n",
      "\n",
      " EPOCH 473/1000\n",
      "\t Training loss (epoch - mean):  3.281292007972704e-14\n",
      "\t Validation loss (epoch - mean): 3.7820440676110373e-16\n",
      "\n",
      "\n",
      " EPOCH 474/1000\n",
      "\t Training loss (epoch - mean):  1.0214365991953902e-14\n",
      "\t Validation loss (epoch - mean): 3.4068682746717274e-16\n",
      "\n",
      "\n",
      " EPOCH 475/1000\n",
      "\t Training loss (epoch - mean):  2.6879830766091755e-14\n",
      "\t Validation loss (epoch - mean): 7.198493557119994e-15\n",
      "\n",
      "\n",
      " EPOCH 476/1000\n",
      "\t Training loss (epoch - mean):  8.229593891188001e-15\n",
      "\t Validation loss (epoch - mean): 1.3524136843118383e-12\n",
      "\n",
      "\n",
      " EPOCH 477/1000\n",
      "\t Training loss (epoch - mean):  7.536916087129736e-15\n",
      "\t Validation loss (epoch - mean): 4.0823623519765267e-17\n",
      "\n",
      "\n",
      " EPOCH 478/1000\n",
      "\t Training loss (epoch - mean):  3.578857373795844e-14\n",
      "\t Validation loss (epoch - mean): 6.1988036099926e-17\n",
      "\n",
      "\n",
      " EPOCH 479/1000\n",
      "\t Training loss (epoch - mean):  6.57740013791864e-15\n",
      "\t Validation loss (epoch - mean): 2.1832685074329888e-16\n",
      "\n",
      "\n",
      " EPOCH 480/1000\n",
      "\t Training loss (epoch - mean):  2.3977520478943505e-14\n",
      "\t Validation loss (epoch - mean): 2.2783590715809534e-11\n",
      "\n",
      "\n",
      " EPOCH 481/1000\n",
      "\t Training loss (epoch - mean):  1.217646202479635e-16\n",
      "\t Validation loss (epoch - mean): 1.1524553742399704e-16\n",
      "\n",
      "\n",
      " EPOCH 482/1000\n",
      "\t Training loss (epoch - mean):  1.486033622950227e-14\n",
      "\t Validation loss (epoch - mean): 9.140124357623105e-14\n",
      "\n",
      "\n",
      " EPOCH 483/1000\n",
      "\t Training loss (epoch - mean):  9.060662328720645e-15\n",
      "\t Validation loss (epoch - mean): 1.3002985387539594e-13\n",
      "\n",
      "\n",
      " EPOCH 484/1000\n",
      "\t Training loss (epoch - mean):  1.6777604510478235e-14\n",
      "\t Validation loss (epoch - mean): 1.2257973381745514e-10\n",
      "\n",
      "\n",
      " EPOCH 485/1000\n",
      "\t Training loss (epoch - mean):  1.1175576395282942e-14\n",
      "\t Validation loss (epoch - mean): 8.488483552647942e-17\n",
      "\n",
      "\n",
      " EPOCH 486/1000\n",
      "\t Training loss (epoch - mean):  2.06889540456995e-14\n",
      "\t Validation loss (epoch - mean): 4.3024434090909625e-16\n",
      "\n",
      "\n",
      " EPOCH 487/1000\n",
      "\t Training loss (epoch - mean):  1.9563422240585316e-14\n",
      "\t Validation loss (epoch - mean): 5.451176600568658e-12\n",
      "\n",
      "\n",
      " EPOCH 488/1000\n",
      "\t Training loss (epoch - mean):  2.756307304056809e-16\n",
      "\t Validation loss (epoch - mean): 8.150185456873979e-10\n",
      "\n",
      "\n",
      " EPOCH 489/1000\n",
      "\t Training loss (epoch - mean):  2.5947936648420312e-15\n",
      "\t Validation loss (epoch - mean): 7.693165802607493e-10\n",
      "\n",
      "\n",
      " EPOCH 490/1000\n",
      "\t Training loss (epoch - mean):  2.3628647936760195e-15\n",
      "\t Validation loss (epoch - mean): 1.0617270097908763e-08\n",
      "\n",
      "\n",
      " EPOCH 491/1000\n",
      "\t Training loss (epoch - mean):  2.1456511709931643e-15\n",
      "\t Validation loss (epoch - mean): 1.1329913302165716e-17\n",
      "\n",
      "\n",
      " EPOCH 492/1000\n",
      "\t Training loss (epoch - mean):  6.025412001834282e-15\n",
      "\t Validation loss (epoch - mean): 3.477675649876232e-10\n",
      "\n",
      "\n",
      " EPOCH 493/1000\n",
      "\t Training loss (epoch - mean):  1.799370559692481e-15\n",
      "\t Validation loss (epoch - mean): 5.931949643957349e-13\n",
      "\n",
      "\n",
      " EPOCH 494/1000\n",
      "\t Training loss (epoch - mean):  4.223845578935312e-13\n",
      "\t Validation loss (epoch - mean): 4.0152022031563256e-11\n",
      "\n",
      "\n",
      " EPOCH 495/1000\n",
      "\t Training loss (epoch - mean):  1.881409566876243e-15\n",
      "\t Validation loss (epoch - mean): 1.4823853785202638e-10\n",
      "\n",
      "\n",
      " EPOCH 496/1000\n",
      "\t Training loss (epoch - mean):  1.8306195721521887e-16\n",
      "\t Validation loss (epoch - mean): 1.045522783965445e-10\n",
      "\n",
      "\n",
      " EPOCH 497/1000\n",
      "\t Training loss (epoch - mean):  3.860990321822431e-15\n",
      "\t Validation loss (epoch - mean): 7.090155590632146e-13\n",
      "\n",
      "\n",
      " EPOCH 498/1000\n",
      "\t Training loss (epoch - mean):  4.671828279588227e-15\n",
      "\t Validation loss (epoch - mean): 1.791002557061685e-09\n",
      "\n",
      "\n",
      " EPOCH 499/1000\n",
      "\t Training loss (epoch - mean):  4.387843782693201e-15\n",
      "\t Validation loss (epoch - mean): 1.0056868412009187e-14\n",
      "\n",
      "\n",
      " EPOCH 500/1000\n",
      "\t Training loss (epoch - mean):  5.7821355426326045e-15\n",
      "\t Validation loss (epoch - mean): 1.268270967762521e-14\n",
      "\n",
      "\n",
      " EPOCH 501/1000\n",
      "\t Training loss (epoch - mean):  2.7484721698853677e-15\n",
      "\t Validation loss (epoch - mean): 3.0448798229389095e-15\n",
      "\n",
      "\n",
      " EPOCH 502/1000\n",
      "\t Training loss (epoch - mean):  2.6704379977831137e-14\n",
      "\t Validation loss (epoch - mean): 5.892460450599042e-17\n",
      "\n",
      "\n",
      " EPOCH 503/1000\n",
      "\t Training loss (epoch - mean):  9.539046886827242e-15\n",
      "\t Validation loss (epoch - mean): 5.2244748344360414e-17\n",
      "\n",
      "\n",
      " EPOCH 504/1000\n",
      "\t Training loss (epoch - mean):  1.901046883697635e-15\n",
      "\t Validation loss (epoch - mean): 3.5727935884728927e-13\n",
      "\n",
      "\n",
      " EPOCH 505/1000\n",
      "\t Training loss (epoch - mean):  5.901007742510407e-16\n",
      "\t Validation loss (epoch - mean): 3.189535320778289e-16\n",
      "\n",
      "\n",
      " EPOCH 506/1000\n",
      "\t Training loss (epoch - mean):  1.5794845884550253e-15\n",
      "\t Validation loss (epoch - mean): 1.5267781506362694e-17\n",
      "\n",
      "\n",
      " EPOCH 507/1000\n",
      "\t Training loss (epoch - mean):  4.849821913420334e-16\n",
      "\t Validation loss (epoch - mean): 5.579015126545891e-11\n",
      "\n",
      "\n",
      " EPOCH 508/1000\n",
      "\t Training loss (epoch - mean):  4.589547418855425e-18\n",
      "\t Validation loss (epoch - mean): 1.0538204802923472e-09\n",
      "\n",
      "\n",
      " EPOCH 509/1000\n",
      "\t Training loss (epoch - mean):  1.2087058889387715e-15\n",
      "\t Validation loss (epoch - mean): 2.5572722469658687e-18\n",
      "\n",
      "\n",
      " EPOCH 510/1000\n",
      "\t Training loss (epoch - mean):  2.6307234475427153e-18\n",
      "\t Validation loss (epoch - mean): 4.827765849095879e-10\n",
      "\n",
      "\n",
      " EPOCH 511/1000\n",
      "\t Training loss (epoch - mean):  3.345740983268947e-16\n",
      "\t Validation loss (epoch - mean): 1.0080367542395881e-13\n",
      "\n",
      "\n",
      " EPOCH 512/1000\n",
      "\t Training loss (epoch - mean):  6.223900021925878e-14\n",
      "\t Validation loss (epoch - mean): 8.296994954354879e-18\n",
      "\n",
      "\n",
      " EPOCH 513/1000\n",
      "\t Training loss (epoch - mean):  3.805528674840617e-12\n",
      "\t Validation loss (epoch - mean): 6.270825033848748e-13\n",
      "\n",
      "\n",
      " EPOCH 514/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (epoch - mean):  3.221364589523025e-15\n",
      "\t Validation loss (epoch - mean): 2.2629984836893835e-15\n",
      "\n",
      "\n",
      " EPOCH 515/1000\n",
      "\t Training loss (epoch - mean):  1.2440671387205217e-15\n",
      "\t Validation loss (epoch - mean): 1.671922131364354e-13\n",
      "\n",
      "\n",
      " EPOCH 516/1000\n",
      "\t Training loss (epoch - mean):  1.5927341246157903e-12\n",
      "\t Validation loss (epoch - mean): 8.836086664304852e-13\n",
      "\n",
      "\n",
      " EPOCH 517/1000\n",
      "\t Training loss (epoch - mean):  2.409875279964403e-15\n",
      "\t Validation loss (epoch - mean): 3.524718843560303e-09\n",
      "\n",
      "\n",
      " EPOCH 518/1000\n",
      "\t Training loss (epoch - mean):  1.2253099073293006e-15\n",
      "\t Validation loss (epoch - mean): 3.580338444473969e-09\n",
      "\n",
      "\n",
      " EPOCH 519/1000\n",
      "\t Training loss (epoch - mean):  9.24353229290406e-16\n",
      "\t Validation loss (epoch - mean): 4.0730866554411e-11\n",
      "\n",
      "\n",
      " EPOCH 520/1000\n",
      "\t Training loss (epoch - mean):  4.831739035233233e-16\n",
      "\t Validation loss (epoch - mean): 1.3332601266784549e-15\n",
      "\n",
      "\n",
      " EPOCH 521/1000\n",
      "\t Training loss (epoch - mean):  4.9934474352261436e-17\n",
      "\t Validation loss (epoch - mean): 4.1272707311461584e-10\n",
      "\n",
      "\n",
      " EPOCH 522/1000\n",
      "\t Training loss (epoch - mean):  1.447469584106613e-16\n",
      "\t Validation loss (epoch - mean): 2.336199538967696e-13\n",
      "\n",
      "\n",
      " EPOCH 523/1000\n",
      "\t Training loss (epoch - mean):  1.146517402261539e-11\n",
      "\t Validation loss (epoch - mean): 3.1531570155113688e-09\n",
      "\n",
      "\n",
      " EPOCH 524/1000\n",
      "\t Training loss (epoch - mean):  9.584155351809306e-15\n",
      "\t Validation loss (epoch - mean): 1.4493869890365287e-10\n",
      "\n",
      "\n",
      " EPOCH 525/1000\n",
      "\t Training loss (epoch - mean):  8.617103341643229e-15\n",
      "\t Validation loss (epoch - mean): 2.9934846910906306e-10\n",
      "\n",
      "\n",
      " EPOCH 526/1000\n",
      "\t Training loss (epoch - mean):  8.639195902024791e-16\n",
      "\t Validation loss (epoch - mean): 2.2889555620157572e-10\n",
      "\n",
      "\n",
      " EPOCH 527/1000\n",
      "\t Training loss (epoch - mean):  8.693365688351473e-16\n",
      "\t Validation loss (epoch - mean): 9.471558233267707e-14\n",
      "\n",
      "\n",
      " EPOCH 528/1000\n",
      "\t Training loss (epoch - mean):  1.0172325573877465e-12\n",
      "\t Validation loss (epoch - mean): 1.6217811473107095e-11\n",
      "\n",
      "\n",
      " EPOCH 529/1000\n",
      "\t Training loss (epoch - mean):  7.973615587987689e-16\n",
      "\t Validation loss (epoch - mean): 2.6710613505208727e-12\n",
      "\n",
      "\n",
      " EPOCH 530/1000\n",
      "\t Training loss (epoch - mean):  5.626959909060811e-16\n",
      "\t Validation loss (epoch - mean): 1.0996441735180725e-12\n",
      "\n",
      "\n",
      " EPOCH 531/1000\n",
      "\t Training loss (epoch - mean):  2.6365934244689497e-16\n",
      "\t Validation loss (epoch - mean): 2.586977108736769e-09\n",
      "\n",
      "\n",
      " EPOCH 532/1000\n",
      "\t Training loss (epoch - mean):  7.929381586364898e-17\n",
      "\t Validation loss (epoch - mean): 5.320134958893372e-12\n",
      "\n",
      "\n",
      " EPOCH 533/1000\n",
      "\t Training loss (epoch - mean):  1.4557557611166518e-16\n",
      "\t Validation loss (epoch - mean): 2.8221001405744608e-11\n",
      "\n",
      "\n",
      " EPOCH 534/1000\n",
      "\t Training loss (epoch - mean):  1.0947225409484105e-14\n",
      "\t Validation loss (epoch - mean): 1.237151099713065e-10\n",
      "\n",
      "\n",
      " EPOCH 535/1000\n",
      "\t Training loss (epoch - mean):  1.1140555261110434e-16\n",
      "\t Validation loss (epoch - mean): 7.84657725878553e-14\n",
      "\n",
      "\n",
      " EPOCH 536/1000\n",
      "\t Training loss (epoch - mean):  3.864974542570457e-17\n",
      "\t Validation loss (epoch - mean): 3.09954143515774e-12\n",
      "\n",
      "\n",
      " EPOCH 537/1000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#%% Train network\n",
    "\n",
    "# Define losses containers\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    print('\\n\\n EPOCH %d/%d' % (epoch + 1, params['num_epochs']))\n",
    "    \n",
    "    # Set training mode\n",
    "    net.train()\n",
    "    # Define losses container for one epoch\n",
    "    train_loss_log = []\n",
    "    \n",
    "    # Iterate batches\n",
    "    for batch_sample in train_dataloader:\n",
    "        # Extract batch\n",
    "        batch = batch_sample.long().to(device)\n",
    "        # Update network\n",
    "        batch_loss = net.train_batch(batch, loss_fn, optimizer)\n",
    "        train_loss_log.append(batch_loss)\n",
    "        # print('\\t Training loss (single batch):', batch_loss)\n",
    "        \n",
    "    print('\\t Training loss (epoch - mean): ', np.mean(train_loss_log) )\n",
    "    train_loss_epochs.append(np.mean(train_loss_log))\n",
    "    \n",
    "    # Set evaluation mode\n",
    "    net.eval()\n",
    "    val_loss_log = []\n",
    "    with torch.no_grad():\n",
    "        for batch_sample in test_dataloader:\n",
    "            batch = batch_sample.long().to(device)\n",
    "            val_loss = net.test_batch(batch, loss_fn)\n",
    "            val_loss_log.append(val_loss)\n",
    "            \n",
    "        print('\\t Validation loss (epoch - mean):', np.mean(val_loss_log))\n",
    "        val_loss_epochs.append(np.mean(val_loss_log))\n",
    "        \n",
    "    # Early stopping\n",
    "    # if epoch>200 and abs(val_loss_epochs[-1] - val_loss_epochs[-200]) < 1e-10:\n",
    "    #    break\n",
    "    \n",
    "losses = { 'train': train_loss_epochs, 'val':  val_loss_epochs, 'params' : params }\n",
    "\n",
    "# Save losses dictionary\n",
    "with open(res_path + \"/\" + out_dir, 'wb') as f:\n",
    "    pickle.dump(losses, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Save model \n",
    "torch.save(net.state_dict(), res_path + \"/state_\" + out_dir + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MSNBDkx39Mg"
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.semilogy(train_loss_epochs, label='Train loss')\n",
    "plt.semilogy(val_loss_epochs, label='Validation loss', linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(images_path + \"/model_losses_\"  + out_dir +\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 'When you are alone in the middle of the sea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "with open('./data/embeddings', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "# Get list of words\n",
    "words = [*embeddings.keys()]\n",
    "# Get vectors as float tensor\n",
    "vectors = torch.tensor([*embeddings.values()], dtype=torch.float)\n",
    "\n",
    "# Load hyperparameters\n",
    "with open(res_path + \"/\" + out_dir, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    params = params['params']\n",
    "    \n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load model\n",
    "state = torch.load(res_path + \"/state_\" + out_dir + '.pth', map_location = device)\n",
    "\n",
    "net = Network(vocab_size = len(embeddings.keys()),\n",
    "              embedding_dim = 50,\n",
    "              hidden_units = params['hidden_units'],\n",
    "              layers_num = params['layers_num'],\n",
    "              hidden_type = params['architecture'],\n",
    "              trained_embeddings = vectors,\n",
    "              dropout_prob = params['dropout_prob']\n",
    "             )\n",
    "\n",
    "net.load_state_dict(state)\n",
    "\n",
    "# Embedding matrix - updated\n",
    "X = net.embed.weight.detach().numpy()\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WordToVector\n",
    "w2i = WordToIndex(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# print(seed, end=' ', flush=True)\n",
    "\n",
    "# Find initial state of the RNN\n",
    "with torch.no_grad():\n",
    "    # Transform words in the corresponding indices\n",
    "    seed_encoded = torch.tensor(w2i(seed.lower()))\n",
    "    # Reshape: batch-like shape\n",
    "    seed_encoded = torch.reshape(seed_encoded, (1, -1))\n",
    "    # Move to the selected device\n",
    "    seed_encoded = seed_encoded.to(device)\n",
    "    # Forward step\n",
    "    net_out, net_state = net(seed_encoded)\n",
    "    # Retrieve the index of the embedding closest to the net output\n",
    "    distances = np.linalg.norm((X - net_out[:, -1, :].to('cpu').numpy()[0]), axis = 1)\n",
    "    #print(distances)\n",
    "    closest_index = np.argmin(distances)\n",
    "    # Retrieve the chosen word text\n",
    "    closest_word = words[closest_index]\n",
    "    # Add to the seed sentence\n",
    "    seed += ' ' + closest_word\n",
    "\n",
    "    # Generate n words\n",
    "    for i in range(15):\n",
    "        # Transform words in the corresponding indices\n",
    "        seed_encoded = torch.tensor(w2i(seed.lower()))\n",
    "        # Reshape: batch-like shape\n",
    "        seed_encoded = torch.reshape(seed_encoded, (1, -1))\n",
    "        # Move to the selected device\n",
    "        seed_encoded = seed_encoded.to(device)\n",
    "        # Forward step\n",
    "        net_out, net_state = net(seed_encoded, net_state)\n",
    "        # Retrieve the index of the embedding closest to the net output\n",
    "        distances = np.linalg.norm((X - net_out[:, -1, :].to('cpu').numpy()[0]), axis = 1)\n",
    "        #print(distances)\n",
    "        closest_index = np.argmin(distances)\n",
    "        # Retrieve the chosen word text\n",
    "        closest_word = words[closest_index]\n",
    "        # Add to the seed sentence\n",
    "        seed += ' ' + closest_word\n",
    "        # Print the current result\n",
    "        # print(closest_word, end=' ', flush=True) \n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
